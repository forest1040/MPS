{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensornetwork as tn\n",
    "tn.set_default_backend(\"jax\")\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from jax import grad, jit, vmap, random\n",
    "\n",
    "import copy\n",
    "from jax.scipy.special import logsumexp\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.config import config\n",
    "config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling(data):\n",
    "    xlen = np.int(data.shape[1]/2)\n",
    "    ylen = np.int(data.shape[2]/2)\n",
    "    arr = np.zeros((data.shape[0], xlen, ylen))\n",
    "    for i in range(0, data.shape[1], 2):\n",
    "        for j in range(0, data.shape[2], 2):\n",
    "            for k in range(0, data.shape[0]):\n",
    "                arr[k][np.int(i/2)][np.int(j/2)] = \\\n",
    "                (data[k][i][j]+data[k][i+1][j]+data[k][i][j+1]+data[k][i+1][j+1])/4\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pool = pooling(x_train)\n",
    "x_test_pool = pooling(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryunagai/virtualenvs/jax_cpu/lib/python3.8/site-packages/jax/lib/xla_bridge.py:130: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    }
   ],
   "source": [
    "num_labels = 10\n",
    "\n",
    "x_train = jnp.array(x_train_pool, dtype=jnp.float64)\n",
    "x_test = jnp.array(x_test_pool, dtype=jnp.float64)\n",
    "\n",
    "num_pixels = x_train.shape[1] * x_train.shape[2]\n",
    "x_train_1d = jnp.reshape(x_train, (x_train.shape[0], num_pixels))\n",
    "x_test_1d = jnp.reshape(x_test, (x_test.shape[0], num_pixels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x, k, dtype=jnp.float64):\n",
    "    \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
    "    return jnp.array(x[:, None] == jnp.arange(k), dtype)\n",
    "\n",
    "y_train_1h = one_hot(y_train, num_labels)\n",
    "y_test_1h = one_hot(y_test, num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Tensor Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_map(p):\n",
    "    phi = jnp.array([1-p, p])\n",
    "    return phi\n",
    "\n",
    "def data_tensorize(vec):\n",
    "    data_tensor = [tn.Node(feature_map(p)) for p in vec]\n",
    "    return data_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block(*dimensions):\n",
    "    '''Construct a new matrix for the MPS with random numbers from 0 to 1'''\n",
    "    seed = np.random.randint(100)\n",
    "    key = random.PRNGKey(seed)\n",
    "    size = tuple([x for x in dimensions])\n",
    "    scale = 0.35\n",
    "    return scale * random.normal(key, size, dtype=jnp.float64)\n",
    "\n",
    "def create_blocks(rank, dim, bond_dim, label_dim):\n",
    "    half = jnp.array([(rank - 2) / 2], dtype = jnp.int32)[0]\n",
    "    blocks = [\n",
    "        block(dim, bond_dim) ] + \\\n",
    "        [ block(bond_dim, dim, bond_dim) for _ in range(half)] + \\\n",
    "        [ block(bond_dim, label_dim, bond_dim) ] + \\\n",
    "        [ block(bond_dim, dim, bond_dim) for _ in range(half, rank-2)] + \\\n",
    "        [ block(bond_dim, dim) \n",
    "        ]\n",
    "    return blocks\n",
    "\n",
    "def create_MPS_labeled(blocks, rank, dim, bond_dim):\n",
    "    '''Build the MPS tensor'''\n",
    "    #half = jnp.array([(rank - 2) / 2], dtype = jnp.int32)[0]\n",
    "    mps = []\n",
    "    for b in blocks:\n",
    "        mps.append(tn.Node(b))\n",
    "\n",
    "    #connect edges to build mps\n",
    "    connected_edges=[]\n",
    "    conn=mps[0][1]^mps[1][0]\n",
    "    connected_edges.append(conn)\n",
    "    for k in range(1,rank):\n",
    "        conn=mps[k][2]^mps[k+1][0]\n",
    "        connected_edges.append(conn)\n",
    "\n",
    "    return mps, connected_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_len = 1\n",
    "label_dim = 10\n",
    "data_len = x_train_1d.shape[1]\n",
    "rank = data_len\n",
    "dim = 2\n",
    "bond_dim = 10\n",
    "\n",
    "blocks = create_blocks(rank, dim, bond_dim, label_dim)\n",
    "\n",
    "@jit\n",
    "def predict_2(data, blocks):\n",
    "    data = [jnp.array([[1-p], [p]]) for p in data]\n",
    "    half = math.floor(len(data)/2)\n",
    "    mps = []\n",
    "    dims = []\n",
    "    mps += [data[0], blocks[0], blocks[1]]\n",
    "    dims += [(1, -1), (1, 2), (2, 3, 4)]\n",
    "    closed = 3\n",
    "    opened = -2\n",
    "    for i in range(1, half):\n",
    "        mps += [data[i], blocks[i+1]]\n",
    "        if i==half-1:\n",
    "            dims += [(closed, opened), (closed+1, opened-1, closed+3)]\n",
    "            opened -= 1\n",
    "        else:\n",
    "            dims += [(closed, opened), (closed+1, closed+2, closed+3)]\n",
    "        closed += 2\n",
    "        opened -= 1\n",
    "    for j in range(half, len(data)-1):\n",
    "        mps += [blocks[j+1], data[j]]\n",
    "        dims += [(closed+1, closed+2, closed+3), (closed+2, opened)]\n",
    "        closed += 2\n",
    "        opened -= 1\n",
    "    mps += [blocks[len(data)], data[-1]]\n",
    "    dims += [(closed+1, closed+2), (closed+2, opened)]\n",
    "    res = tn.ncon(mps, dims)\n",
    "    res = jnp.squeeze(res)\n",
    "    res = res - logsumexp(res)\n",
    "    return res\n",
    "\n",
    "def data_list(vec):\n",
    "    data_list = [jnp.array([[1-p], [p]]) for p in vec]\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def batched_predict_2(batch_data, blocks):\n",
    "    return vmap(predict_2, in_axes=(0, None))(batch_data, blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = (rank, dim, bond_dim, label_len, data_len)\n",
    "\n",
    "def accuracy(blocks, data, targets, params):\n",
    "    #mps, edges = create_MPS_labeled(blocks, params[0], params[1], params[2])\n",
    "    batched_preds = batched_predict_2(data, blocks)\n",
    "    target_class = jnp.argmax(targets, axis=1)\n",
    "    predicted_class = jnp.argmax(batched_preds, axis=1)\n",
    "    return jnp.mean(predicted_class == target_class)\n",
    "\n",
    "def loss(blocks, data, targets, params):\n",
    "    #mps, edges = create_MPS_labeled(blocks, params[0], params[1], params[2])\n",
    "    batched_preds = batched_predict_2(data, blocks)\n",
    "    return -jnp.mean(vmap(jnp.dot, in_axes=(0, 0))(batched_preds, targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def update(blocks, data, targets, params, lr = 1e-3):\n",
    "    grads = grad(loss)(blocks, data, targets, params)\n",
    "    return [x - lr * dx for x, dx in zip(blocks, grads)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class batch_gen(object):\n",
    "    def __init__(self, train, target, batch_size):\n",
    "        self._train = train\n",
    "        self._target = target\n",
    "        self._batch_size = batch_size\n",
    "        self._i = 0\n",
    "    def __iter__(self):\n",
    "        # __next__()はselfが実装してるのでそのままselfを返す\n",
    "        return self\n",
    "    def __next__(self):  # Python2だと next(self) で定義\n",
    "        if self._i >= len(self._train):\n",
    "            raise StopIteration()\n",
    "        train_batch = self._train[self._i:self._i + self._batch_size]\n",
    "        target_batch = self._target[self._i:self._i + self._batch_size]\n",
    "        self._i += self._batch_size\n",
    "        return (train_batch, target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 in 20.88 sec\n",
      "Training set accuracy 0.16003333333333333\n",
      "Training set loss 2.231128610088902\n",
      "Epoch 1 in 20.86 sec\n",
      "Training set accuracy 0.18928333333333333\n",
      "Training set loss 2.1168306715463587\n",
      "Epoch 2 in 20.89 sec\n",
      "Training set accuracy 0.19333333333333333\n",
      "Training set loss 2.0822781356800006\n",
      "Epoch 3 in 20.90 sec\n",
      "Training set accuracy 0.19333333333333333\n",
      "Training set loss 2.072642126645317\n",
      "Epoch 4 in 20.93 sec\n",
      "Training set accuracy 0.19268333333333335\n",
      "Training set loss 2.068078929665813\n",
      "Epoch 5 in 20.84 sec\n",
      "Training set accuracy 0.19275\n",
      "Training set loss 2.0656064259935936\n",
      "Epoch 6 in 20.77 sec\n",
      "Training set accuracy 0.19228333333333333\n",
      "Training set loss 2.0639993563167875\n",
      "Epoch 7 in 20.99 sec\n",
      "Training set accuracy 0.19285\n",
      "Training set loss 2.062779680845404\n",
      "Epoch 8 in 20.98 sec\n",
      "Training set accuracy 0.19163333333333332\n",
      "Training set loss 2.0618010695316187\n",
      "Epoch 9 in 20.87 sec\n",
      "Training set accuracy 0.19203333333333333\n",
      "Training set loss 2.060988645944274\n",
      "Epoch 10 in 20.91 sec\n",
      "Training set accuracy 0.19241666666666668\n",
      "Training set loss 2.060277810149153\n",
      "Epoch 11 in 20.92 sec\n",
      "Training set accuracy 0.19265\n",
      "Training set loss 2.0596432117072063\n",
      "Epoch 12 in 20.82 sec\n",
      "Training set accuracy 0.19281666666666666\n",
      "Training set loss 2.0590621764608077\n",
      "Epoch 13 in 20.79 sec\n",
      "Training set accuracy 0.19253333333333333\n",
      "Training set loss 2.0585157062139015\n",
      "Epoch 14 in 20.92 sec\n",
      "Training set accuracy 0.19303333333333333\n",
      "Training set loss 2.057985038799683\n",
      "Epoch 15 in 21.04 sec\n",
      "Training set accuracy 0.19473333333333334\n",
      "Training set loss 2.057418677276129\n",
      "Epoch 16 in 21.00 sec\n",
      "Training set accuracy 0.19525\n",
      "Training set loss 2.056847596670995\n",
      "Epoch 17 in 21.03 sec\n",
      "Training set accuracy 0.19853333333333334\n",
      "Training set loss 2.055824778920327\n",
      "Epoch 18 in 20.84 sec\n",
      "Training set accuracy 0.20201666666666668\n",
      "Training set loss 2.0541109216335487\n",
      "Epoch 19 in 20.90 sec\n",
      "Training set accuracy 0.2078\n",
      "Training set loss 2.0510165601074792\n",
      "Epoch 20 in 20.97 sec\n",
      "Training set accuracy 0.21468333333333334\n",
      "Training set loss 2.0446583379520336\n",
      "Epoch 21 in 21.02 sec\n",
      "Training set accuracy 0.23035\n",
      "Training set loss 2.024448051814576\n",
      "Epoch 22 in 20.99 sec\n",
      "Training set accuracy 0.25078333333333336\n",
      "Training set loss 1.983050186824614\n",
      "Epoch 23 in 20.96 sec\n",
      "Training set accuracy 0.26255\n",
      "Training set loss 1.943502593273557\n",
      "Epoch 24 in 20.91 sec\n",
      "Training set accuracy 0.27476666666666666\n",
      "Training set loss 1.899034552586054\n",
      "Epoch 25 in 20.99 sec\n",
      "Training set accuracy 0.2869833333333333\n",
      "Training set loss 1.850993786562252\n",
      "Epoch 26 in 20.90 sec\n",
      "Training set accuracy 0.2989833333333333\n",
      "Training set loss 1.8093844849589769\n",
      "Epoch 27 in 20.83 sec\n",
      "Training set accuracy 0.3143166666666667\n",
      "Training set loss 1.7708759878862144\n",
      "Epoch 28 in 21.03 sec\n",
      "Training set accuracy 0.3302833333333333\n",
      "Training set loss 1.7412090665449185\n",
      "Epoch 29 in 20.90 sec\n",
      "Training set accuracy 0.3416\n",
      "Training set loss 1.7180695858412365\n",
      "Epoch 30 in 20.98 sec\n",
      "Training set accuracy 0.3503\n",
      "Training set loss 1.6987085906750405\n",
      "Epoch 31 in 21.11 sec\n",
      "Training set accuracy 0.35651666666666665\n",
      "Training set loss 1.6819898074038093\n",
      "Epoch 32 in 20.99 sec\n",
      "Training set accuracy 0.3605333333333333\n",
      "Training set loss 1.6676789344412495\n",
      "Epoch 33 in 20.92 sec\n",
      "Training set accuracy 0.36566666666666664\n",
      "Training set loss 1.6548497320735458\n",
      "Epoch 34 in 21.03 sec\n",
      "Training set accuracy 0.37075\n",
      "Training set loss 1.643129510218458\n",
      "Epoch 35 in 21.01 sec\n",
      "Training set accuracy 0.37665\n",
      "Training set loss 1.6318404283372065\n",
      "Epoch 36 in 20.89 sec\n",
      "Training set accuracy 0.3847\n",
      "Training set loss 1.6204766199860465\n",
      "Epoch 37 in 20.99 sec\n",
      "Training set accuracy 0.39566666666666667\n",
      "Training set loss 1.6084944770535503\n",
      "Epoch 38 in 21.02 sec\n",
      "Training set accuracy 0.41115\n",
      "Training set loss 1.5947506564666167\n",
      "Epoch 39 in 20.87 sec\n",
      "Training set accuracy 0.43093333333333333\n",
      "Training set loss 1.5718500021983088\n",
      "Epoch 40 in 20.96 sec\n",
      "Training set accuracy 0.45703333333333335\n",
      "Training set loss 1.5308910182742475\n",
      "Epoch 41 in 21.05 sec\n",
      "Training set accuracy 0.4860833333333333\n",
      "Training set loss 1.4765037933241212\n",
      "Epoch 42 in 21.00 sec\n",
      "Training set accuracy 0.5130833333333333\n",
      "Training set loss 1.4140856335512628\n",
      "Epoch 43 in 20.95 sec\n",
      "Training set accuracy 0.5396166666666666\n",
      "Training set loss 1.3515129420428265\n",
      "Epoch 44 in 21.12 sec\n",
      "Training set accuracy 0.56425\n",
      "Training set loss 1.2819869747710244\n",
      "Epoch 45 in 21.04 sec\n",
      "Training set accuracy 0.58725\n",
      "Training set loss 1.216191152907556\n",
      "Epoch 46 in 20.96 sec\n",
      "Training set accuracy 0.6054333333333334\n",
      "Training set loss 1.1688832528278643\n",
      "Epoch 47 in 20.95 sec\n",
      "Training set accuracy 0.6215666666666667\n",
      "Training set loss 1.1296923652086375\n",
      "Epoch 48 in 20.91 sec\n",
      "Training set accuracy 0.6575\n",
      "Training set loss 1.0904109095233807\n",
      "Epoch 49 in 21.04 sec\n",
      "Training set accuracy 0.6835333333333333\n",
      "Training set loss 1.0438621395687715\n",
      "Epoch 50 in 20.94 sec\n",
      "Training set accuracy 0.6994666666666667\n",
      "Training set loss 0.9917113548848382\n",
      "Epoch 51 in 20.95 sec\n",
      "Training set accuracy 0.7149\n",
      "Training set loss 0.936896871732943\n",
      "Epoch 52 in 20.89 sec\n",
      "Training set accuracy 0.7280166666666666\n",
      "Training set loss 0.8906550829595753\n",
      "Epoch 53 in 21.05 sec\n",
      "Training set accuracy 0.7410333333333333\n",
      "Training set loss 0.8517587295286676\n",
      "Epoch 54 in 21.19 sec\n",
      "Training set accuracy 0.7517333333333334\n",
      "Training set loss 0.8188707711588162\n",
      "Epoch 55 in 21.02 sec\n",
      "Training set accuracy 0.7610333333333333\n",
      "Training set loss 0.7901624227239541\n",
      "Epoch 56 in 21.09 sec\n",
      "Training set accuracy 0.7696333333333333\n",
      "Training set loss 0.7630786311382068\n",
      "Epoch 57 in 21.21 sec\n",
      "Training set accuracy 0.77845\n",
      "Training set loss 0.7351987834764744\n",
      "Epoch 58 in 21.02 sec\n",
      "Training set accuracy 0.7866166666666666\n",
      "Training set loss 0.7076913000556152\n",
      "Epoch 59 in 20.89 sec\n",
      "Training set accuracy 0.7930833333333334\n",
      "Training set loss 0.6815781166844559\n",
      "Epoch 60 in 20.95 sec\n",
      "Training set accuracy 0.8004666666666667\n",
      "Training set loss 0.6588667941441203\n",
      "Epoch 61 in 21.05 sec\n",
      "Training set accuracy 0.8059333333333333\n",
      "Training set loss 0.639265522287587\n",
      "Epoch 62 in 20.90 sec\n",
      "Training set accuracy 0.8108666666666666\n",
      "Training set loss 0.6219877382092538\n",
      "Epoch 63 in 20.92 sec\n",
      "Training set accuracy 0.81535\n",
      "Training set loss 0.6062040380756407\n",
      "Epoch 64 in 20.90 sec\n",
      "Training set accuracy 0.8189833333333333\n",
      "Training set loss 0.5918456834521028\n",
      "Epoch 65 in 20.95 sec\n",
      "Training set accuracy 0.8222833333333334\n",
      "Training set loss 0.5789312973799325\n",
      "Epoch 66 in 21.07 sec\n",
      "Training set accuracy 0.8246\n",
      "Training set loss 0.5672688709981198\n",
      "Epoch 67 in 20.82 sec\n",
      "Training set accuracy 0.8271333333333334\n",
      "Training set loss 0.5568104275286112\n",
      "Epoch 68 in 20.86 sec\n",
      "Training set accuracy 0.8291\n",
      "Training set loss 0.5473676049493404\n",
      "Epoch 69 in 20.97 sec\n",
      "Training set accuracy 0.8316166666666667\n",
      "Training set loss 0.538612340149957\n",
      "Epoch 70 in 20.97 sec\n",
      "Training set accuracy 0.8335666666666667\n",
      "Training set loss 0.5303509415112653\n",
      "Epoch 71 in 20.96 sec\n",
      "Training set accuracy 0.83525\n",
      "Training set loss 0.5225513713725597\n",
      "Epoch 72 in 21.04 sec\n",
      "Training set accuracy 0.8373833333333334\n",
      "Training set loss 0.5151589895860347\n",
      "Epoch 73 in 20.96 sec\n",
      "Training set accuracy 0.8395\n",
      "Training set loss 0.508080586472008\n",
      "Epoch 74 in 20.95 sec\n",
      "Training set accuracy 0.84195\n",
      "Training set loss 0.5012362842186688\n",
      "Epoch 75 in 20.93 sec\n",
      "Training set accuracy 0.84405\n",
      "Training set loss 0.49457699243409525\n",
      "Epoch 76 in 21.03 sec\n",
      "Training set accuracy 0.8462166666666666\n",
      "Training set loss 0.48806689214448745\n",
      "Epoch 77 in 21.24 sec\n",
      "Training set accuracy 0.8485\n",
      "Training set loss 0.48167355033841314\n",
      "Epoch 78 in 21.16 sec\n",
      "Training set accuracy 0.8504833333333334\n",
      "Training set loss 0.47537868057934696\n",
      "Epoch 79 in 21.14 sec\n",
      "Training set accuracy 0.8527\n",
      "Training set loss 0.46918305393840615\n",
      "Epoch 80 in 21.26 sec\n",
      "Training set accuracy 0.8546833333333334\n",
      "Training set loss 0.4630992909244522\n",
      "Epoch 81 in 21.14 sec\n",
      "Training set accuracy 0.85675\n",
      "Training set loss 0.45714106189222514\n",
      "Epoch 82 in 21.08 sec\n",
      "Training set accuracy 0.8586666666666667\n",
      "Training set loss 0.45131428814342955\n",
      "Epoch 83 in 21.15 sec\n",
      "Training set accuracy 0.86085\n",
      "Training set loss 0.44562503081421717\n",
      "Epoch 84 in 21.05 sec\n",
      "Training set accuracy 0.8626166666666667\n",
      "Training set loss 0.4400643761438507\n",
      "Epoch 85 in 20.93 sec\n",
      "Training set accuracy 0.86515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set loss 0.4345866805056046\n",
      "Epoch 86 in 21.12 sec\n",
      "Training set accuracy 0.8674333333333333\n",
      "Training set loss 0.42915118704986405\n",
      "Epoch 87 in 21.17 sec\n",
      "Training set accuracy 0.86925\n",
      "Training set loss 0.42373033218182093\n",
      "Epoch 88 in 21.22 sec\n",
      "Training set accuracy 0.8712833333333333\n",
      "Training set loss 0.41831340668379224\n",
      "Epoch 89 in 21.10 sec\n",
      "Training set accuracy 0.8730833333333333\n",
      "Training set loss 0.4129081223978637\n",
      "Epoch 90 in 21.12 sec\n",
      "Training set accuracy 0.8751\n",
      "Training set loss 0.4075303163203324\n",
      "Epoch 91 in 21.14 sec\n",
      "Training set accuracy 0.8767666666666667\n",
      "Training set loss 0.40221923381477864\n",
      "Epoch 92 in 21.18 sec\n",
      "Training set accuracy 0.8783166666666666\n",
      "Training set loss 0.39702072456890264\n",
      "Epoch 93 in 21.18 sec\n",
      "Training set accuracy 0.8799166666666667\n",
      "Training set loss 0.3919647321225327\n",
      "Epoch 94 in 21.31 sec\n",
      "Training set accuracy 0.8818833333333334\n",
      "Training set loss 0.3870527755937818\n",
      "Epoch 95 in 21.22 sec\n",
      "Training set accuracy 0.8835666666666666\n",
      "Training set loss 0.3822647469942213\n",
      "Epoch 96 in 21.13 sec\n",
      "Training set accuracy 0.88575\n",
      "Training set loss 0.3775772007390614\n",
      "Epoch 97 in 21.16 sec\n",
      "Training set accuracy 0.8872\n",
      "Training set loss 0.372972156446685\n",
      "Epoch 98 in 21.23 sec\n",
      "Training set accuracy 0.8883666666666666\n",
      "Training set loss 0.36844021572643365\n",
      "Epoch 99 in 21.17 sec\n",
      "Training set accuracy 0.8895833333333333\n",
      "Training set loss 0.3639854548155955\n",
      "Epoch 100 in 21.13 sec\n",
      "Training set accuracy 0.8907166666666667\n",
      "Training set loss 0.35961425749857284\n",
      "Epoch 101 in 21.21 sec\n",
      "Training set accuracy 0.8923666666666666\n",
      "Training set loss 0.3553259967622186\n",
      "Epoch 102 in 21.11 sec\n",
      "Training set accuracy 0.8937333333333334\n",
      "Training set loss 0.3511173300048638\n",
      "Epoch 103 in 21.09 sec\n",
      "Training set accuracy 0.8950333333333333\n",
      "Training set loss 0.34698571632414055\n",
      "Epoch 104 in 21.10 sec\n",
      "Training set accuracy 0.896\n",
      "Training set loss 0.3429310321663014\n",
      "Epoch 105 in 21.09 sec\n",
      "Training set accuracy 0.8972666666666667\n",
      "Training set loss 0.33895613597790397\n",
      "Epoch 106 in 21.23 sec\n",
      "Training set accuracy 0.8984166666666666\n",
      "Training set loss 0.33506513205869315\n",
      "Epoch 107 in 21.05 sec\n",
      "Training set accuracy 0.8998833333333334\n",
      "Training set loss 0.3312608644462945\n",
      "Epoch 108 in 21.19 sec\n",
      "Training set accuracy 0.9010333333333334\n",
      "Training set loss 0.32754375256182733\n",
      "Epoch 109 in 21.09 sec\n",
      "Training set accuracy 0.9021\n",
      "Training set loss 0.32391184674897167\n",
      "Epoch 110 in 21.24 sec\n",
      "Training set accuracy 0.9032333333333333\n",
      "Training set loss 0.32036181950746473\n",
      "Epoch 111 in 21.15 sec\n",
      "Training set accuracy 0.9044333333333333\n",
      "Training set loss 0.31689054146975454\n",
      "Epoch 112 in 21.05 sec\n",
      "Training set accuracy 0.9058333333333334\n",
      "Training set loss 0.31349645561120554\n",
      "Epoch 113 in 21.09 sec\n",
      "Training set accuracy 0.90685\n",
      "Training set loss 0.31017796896838606\n",
      "Epoch 114 in 20.98 sec\n",
      "Training set accuracy 0.9077833333333334\n",
      "Training set loss 0.3069292179482542\n",
      "Epoch 115 in 21.26 sec\n",
      "Training set accuracy 0.9088666666666667\n",
      "Training set loss 0.30374040526378626\n",
      "Epoch 116 in 21.10 sec\n",
      "Training set accuracy 0.9099\n",
      "Training set loss 0.30060158319829616\n",
      "Epoch 117 in 21.19 sec\n",
      "Training set accuracy 0.9106833333333333\n",
      "Training set loss 0.2975041178817434\n",
      "Epoch 118 in 21.07 sec\n",
      "Training set accuracy 0.912\n",
      "Training set loss 0.2944405880971324\n",
      "Epoch 119 in 21.14 sec\n",
      "Training set accuracy 0.9132166666666667\n",
      "Training set loss 0.2914050271848075\n",
      "Epoch 120 in 21.45 sec\n",
      "Training set accuracy 0.914\n",
      "Training set loss 0.2883934847210569\n",
      "Epoch 121 in 21.14 sec\n",
      "Training set accuracy 0.9148166666666666\n",
      "Training set loss 0.2854043864403313\n",
      "Epoch 122 in 21.16 sec\n",
      "Training set accuracy 0.9158166666666666\n",
      "Training set loss 0.2824384887978547\n",
      "Epoch 123 in 21.10 sec\n",
      "Training set accuracy 0.9164333333333333\n",
      "Training set loss 0.2794984329314109\n",
      "Epoch 124 in 21.21 sec\n",
      "Training set accuracy 0.9173333333333333\n",
      "Training set loss 0.2765880180677789\n",
      "Epoch 125 in 21.06 sec\n",
      "Training set accuracy 0.9182166666666667\n",
      "Training set loss 0.2737114218940211\n",
      "Epoch 126 in 20.93 sec\n",
      "Training set accuracy 0.9190166666666667\n",
      "Training set loss 0.2708725935363955\n",
      "Epoch 127 in 21.01 sec\n",
      "Training set accuracy 0.9196166666666666\n",
      "Training set loss 0.26807496432705635\n",
      "Epoch 128 in 20.98 sec\n",
      "Training set accuracy 0.9205833333333333\n",
      "Training set loss 0.2653215097736114\n",
      "Epoch 129 in 21.01 sec\n",
      "Training set accuracy 0.9218\n",
      "Training set loss 0.262615030989777\n",
      "Epoch 130 in 20.99 sec\n",
      "Training set accuracy 0.92285\n",
      "Training set loss 0.2599583952243252\n",
      "Epoch 131 in 21.10 sec\n",
      "Training set accuracy 0.9237\n",
      "Training set loss 0.25735453473113673\n",
      "Epoch 132 in 21.05 sec\n",
      "Training set accuracy 0.9244166666666667\n",
      "Training set loss 0.2548062127609819\n",
      "Epoch 133 in 20.97 sec\n",
      "Training set accuracy 0.925\n",
      "Training set loss 0.252315731787736\n",
      "Epoch 134 in 21.02 sec\n",
      "Training set accuracy 0.9257\n",
      "Training set loss 0.2498847375477536\n",
      "Epoch 135 in 20.96 sec\n",
      "Training set accuracy 0.9263166666666667\n",
      "Training set loss 0.24751412843131013\n",
      "Epoch 136 in 21.00 sec\n",
      "Training set accuracy 0.9269833333333334\n",
      "Training set loss 0.24520402684894713\n",
      "Epoch 137 in 20.98 sec\n",
      "Training set accuracy 0.9277833333333333\n",
      "Training set loss 0.24295382556954048\n",
      "Epoch 138 in 21.00 sec\n",
      "Training set accuracy 0.9287833333333333\n",
      "Training set loss 0.24076233580435707\n",
      "Epoch 139 in 20.96 sec\n",
      "Training set accuracy 0.9291833333333334\n",
      "Training set loss 0.23862801210830475\n",
      "Epoch 140 in 20.96 sec\n",
      "Training set accuracy 0.9296166666666666\n",
      "Training set loss 0.23654919195115162\n",
      "Epoch 141 in 20.84 sec\n",
      "Training set accuracy 0.9300833333333334\n",
      "Training set loss 0.23452430061013999\n",
      "Epoch 142 in 21.11 sec\n",
      "Training set accuracy 0.9306\n",
      "Training set loss 0.23255200595728678\n",
      "Epoch 143 in 21.05 sec\n",
      "Training set accuracy 0.9309666666666667\n",
      "Training set loss 0.23063132188620936\n",
      "Epoch 144 in 21.07 sec\n",
      "Training set accuracy 0.9315\n",
      "Training set loss 0.22876164241302346\n",
      "Epoch 145 in 20.94 sec\n",
      "Training set accuracy 0.93215\n",
      "Training set loss 0.22694267650231917\n",
      "Epoch 146 in 20.84 sec\n",
      "Training set accuracy 0.9325333333333333\n",
      "Training set loss 0.225174288789902\n",
      "Epoch 147 in 20.96 sec\n",
      "Training set accuracy 0.9328\n",
      "Training set loss 0.22345630537113756\n",
      "Epoch 148 in 20.95 sec\n",
      "Training set accuracy 0.9332\n",
      "Training set loss 0.2217883460610722\n",
      "Epoch 149 in 20.96 sec\n",
      "Training set accuracy 0.9337666666666666\n",
      "Training set loss 0.22016970332421237\n",
      "Epoch 150 in 21.10 sec\n",
      "Training set accuracy 0.9342166666666667\n",
      "Training set loss 0.21859926666099674\n",
      "Epoch 151 in 21.09 sec\n",
      "Training set accuracy 0.9348833333333333\n",
      "Training set loss 0.21707547697452695\n",
      "Epoch 152 in 20.94 sec\n",
      "Training set accuracy 0.9353\n",
      "Training set loss 0.2155962607289427\n",
      "Epoch 153 in 20.94 sec\n",
      "Training set accuracy 0.9357\n",
      "Training set loss 0.21415892472849574\n",
      "Epoch 154 in 20.81 sec\n",
      "Training set accuracy 0.9360666666666667\n",
      "Training set loss 0.21276010978458937\n",
      "Epoch 155 in 20.98 sec\n",
      "Training set accuracy 0.93645\n",
      "Training set loss 0.21139591301894878\n",
      "Epoch 156 in 20.92 sec\n",
      "Training set accuracy 0.93685\n",
      "Training set loss 0.2100621284398939\n",
      "Epoch 157 in 21.14 sec\n",
      "Training set accuracy 0.9371833333333334\n",
      "Training set loss 0.2087544264136484\n",
      "Epoch 158 in 21.01 sec\n",
      "Training set accuracy 0.9375\n",
      "Training set loss 0.2074684044445763\n",
      "Epoch 159 in 20.95 sec\n",
      "Training set accuracy 0.9379166666666666\n",
      "Training set loss 0.20619998578281248\n",
      "Epoch 160 in 21.08 sec\n",
      "Training set accuracy 0.9382833333333334\n",
      "Training set loss 0.20494746328740324\n",
      "Epoch 161 in 21.01 sec\n",
      "Training set accuracy 0.9385333333333333\n",
      "Training set loss 0.20371522458884422\n",
      "Epoch 162 in 20.91 sec\n",
      "Training set accuracy 0.9388666666666666\n",
      "Training set loss 0.2025128720024057\n",
      "Epoch 163 in 20.99 sec\n",
      "Training set accuracy 0.9392166666666667\n",
      "Training set loss 0.20134532672298278\n",
      "Epoch 164 in 21.03 sec\n",
      "Training set accuracy 0.9393666666666667\n",
      "Training set loss 0.20020895281936374\n",
      "Epoch 165 in 21.14 sec\n",
      "Training set accuracy 0.9396666666666667\n",
      "Training set loss 0.19909877622680444\n",
      "Epoch 166 in 20.98 sec\n",
      "Training set accuracy 0.94005\n",
      "Training set loss 0.198011352060713\n",
      "Epoch 167 in 21.03 sec\n",
      "Training set accuracy 0.9404166666666667\n",
      "Training set loss 0.19694387183717438\n",
      "Epoch 168 in 21.04 sec\n",
      "Training set accuracy 0.9408666666666666\n",
      "Training set loss 0.19589432461340256\n",
      "Epoch 169 in 20.85 sec\n",
      "Training set accuracy 0.94115\n",
      "Training set loss 0.19486172741932375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170 in 20.95 sec\n",
      "Training set accuracy 0.9417\n",
      "Training set loss 0.1938457064296704\n",
      "Epoch 171 in 20.98 sec\n",
      "Training set accuracy 0.9419666666666666\n",
      "Training set loss 0.19284601353024056\n",
      "Epoch 172 in 21.10 sec\n",
      "Training set accuracy 0.9422333333333334\n",
      "Training set loss 0.19186246793564332\n",
      "Epoch 173 in 20.93 sec\n",
      "Training set accuracy 0.94245\n",
      "Training set loss 0.1908949126769982\n",
      "Epoch 174 in 21.02 sec\n",
      "Training set accuracy 0.94275\n",
      "Training set loss 0.1899430551061559\n",
      "Epoch 175 in 20.97 sec\n",
      "Training set accuracy 0.9428833333333333\n",
      "Training set loss 0.18900635431710294\n",
      "Epoch 176 in 20.85 sec\n",
      "Training set accuracy 0.9431333333333334\n",
      "Training set loss 0.1880840560926771\n",
      "Epoch 177 in 20.98 sec\n",
      "Training set accuracy 0.9434333333333333\n",
      "Training set loss 0.18717532161078734\n",
      "Epoch 178 in 20.97 sec\n",
      "Training set accuracy 0.9437333333333333\n",
      "Training set loss 0.18627932985049883\n",
      "Epoch 179 in 20.91 sec\n",
      "Training set accuracy 0.94425\n",
      "Training set loss 0.18539531335959133\n",
      "Epoch 180 in 20.97 sec\n",
      "Training set accuracy 0.9444833333333333\n",
      "Training set loss 0.18452255523146324\n",
      "Epoch 181 in 20.98 sec\n",
      "Training set accuracy 0.9447833333333333\n",
      "Training set loss 0.18366037838130797\n",
      "Epoch 182 in 20.99 sec\n",
      "Training set accuracy 0.9451333333333334\n",
      "Training set loss 0.18280813644477012\n",
      "Epoch 183 in 20.92 sec\n",
      "Training set accuracy 0.9453666666666667\n",
      "Training set loss 0.1819651887919126\n",
      "Epoch 184 in 20.93 sec\n",
      "Training set accuracy 0.9455166666666667\n",
      "Training set loss 0.18113080050612776\n",
      "Epoch 185 in 20.97 sec\n",
      "Training set accuracy 0.94575\n",
      "Training set loss 0.1803039006262147\n",
      "Epoch 186 in 20.94 sec\n",
      "Training set accuracy 0.94605\n",
      "Training set loss 0.1794829486986317\n",
      "Epoch 187 in 21.16 sec\n",
      "Training set accuracy 0.9463666666666667\n",
      "Training set loss 0.17866668125071974\n",
      "Epoch 188 in 20.86 sec\n",
      "Training set accuracy 0.9465166666666667\n",
      "Training set loss 0.17785515392873233\n",
      "Epoch 189 in 20.95 sec\n",
      "Training set accuracy 0.94665\n",
      "Training set loss 0.17704904926554518\n",
      "Epoch 190 in 20.93 sec\n",
      "Training set accuracy 0.9468666666666666\n",
      "Training set loss 0.17624861775003323\n",
      "Epoch 191 in 21.10 sec\n",
      "Training set accuracy 0.94715\n",
      "Training set loss 0.17545381329913867\n",
      "Epoch 192 in 21.12 sec\n",
      "Training set accuracy 0.9473166666666667\n",
      "Training set loss 0.1746645731129384\n",
      "Epoch 193 in 20.94 sec\n",
      "Training set accuracy 0.9476666666666667\n",
      "Training set loss 0.1738808855433411\n",
      "Epoch 194 in 20.91 sec\n",
      "Training set accuracy 0.9480833333333333\n",
      "Training set loss 0.1731028054032347\n",
      "Epoch 195 in 20.98 sec\n",
      "Training set accuracy 0.9483166666666667\n",
      "Training set loss 0.17233044735407552\n",
      "Epoch 196 in 20.97 sec\n",
      "Training set accuracy 0.9485\n",
      "Training set loss 0.17156397386683372\n",
      "Epoch 197 in 21.20 sec\n",
      "Training set accuracy 0.9487\n",
      "Training set loss 0.1708035812936746\n",
      "Epoch 198 in 21.00 sec\n",
      "Training set accuracy 0.9489333333333333\n",
      "Training set loss 0.1700494858692756\n",
      "Epoch 199 in 21.08 sec\n",
      "Training set accuracy 0.949\n",
      "Training set loss 0.16930190880955306\n",
      "Epoch 200 in 21.07 sec\n",
      "Training set accuracy 0.9490833333333333\n",
      "Training set loss 0.16856106153816625\n",
      "Epoch 201 in 20.96 sec\n",
      "Training set accuracy 0.9494166666666667\n",
      "Training set loss 0.16782713188066428\n",
      "Epoch 202 in 21.02 sec\n",
      "Training set accuracy 0.9496333333333333\n",
      "Training set loss 0.16710027317855347\n",
      "Epoch 203 in 20.99 sec\n",
      "Training set accuracy 0.9498166666666666\n",
      "Training set loss 0.16638059637205213\n",
      "Epoch 204 in 21.19 sec\n",
      "Training set accuracy 0.9499166666666666\n",
      "Training set loss 0.1656681650543218\n",
      "Epoch 205 in 20.96 sec\n",
      "Training set accuracy 0.95015\n",
      "Training set loss 0.16496299248653754\n",
      "Epoch 206 in 20.90 sec\n",
      "Training set accuracy 0.9502166666666667\n",
      "Training set loss 0.16426504147385582\n",
      "Epoch 207 in 21.03 sec\n",
      "Training set accuracy 0.95045\n",
      "Training set loss 0.16357422730143287\n",
      "Epoch 208 in 21.01 sec\n",
      "Training set accuracy 0.9507333333333333\n",
      "Training set loss 0.1628904245123802\n",
      "Epoch 209 in 21.09 sec\n",
      "Training set accuracy 0.951\n",
      "Training set loss 0.1622134755012056\n",
      "Epoch 210 in 21.01 sec\n",
      "Training set accuracy 0.9511333333333334\n",
      "Training set loss 0.16154320012456724\n",
      "Epoch 211 in 20.98 sec\n",
      "Training set accuracy 0.9513666666666667\n",
      "Training set loss 0.1608794036469142\n",
      "Epoch 212 in 21.08 sec\n",
      "Training set accuracy 0.9515333333333333\n",
      "Training set loss 0.16022188396163656\n",
      "Epoch 213 in 20.97 sec\n",
      "Training set accuracy 0.95175\n",
      "Training set loss 0.15957043606211258\n",
      "Epoch 214 in 21.00 sec\n",
      "Training set accuracy 0.9518166666666666\n",
      "Training set loss 0.15892485666899914\n",
      "Epoch 215 in 21.01 sec\n",
      "Training set accuracy 0.9519166666666666\n",
      "Training set loss 0.15828494588382966\n",
      "Epoch 216 in 20.97 sec\n",
      "Training set accuracy 0.9520833333333333\n",
      "Training set loss 0.1576505107423994\n",
      "Epoch 217 in 20.91 sec\n",
      "Training set accuracy 0.9522833333333334\n",
      "Training set loss 0.1570213647710766\n",
      "Epoch 218 in 21.14 sec\n",
      "Training set accuracy 0.95245\n",
      "Training set loss 0.1563973319475613\n",
      "Epoch 219 in 20.99 sec\n",
      "Training set accuracy 0.9527166666666667\n",
      "Training set loss 0.15577824417048353\n",
      "Epoch 220 in 20.91 sec\n",
      "Training set accuracy 0.9528333333333333\n",
      "Training set loss 0.1551639470857019\n",
      "Epoch 221 in 21.14 sec\n",
      "Training set accuracy 0.9529166666666666\n",
      "Training set loss 0.15455429458483214\n",
      "Epoch 222 in 21.08 sec\n",
      "Training set accuracy 0.9529\n",
      "Training set loss 0.15394915784185015\n",
      "Epoch 223 in 20.96 sec\n",
      "Training set accuracy 0.95305\n",
      "Training set loss 0.15334841507077057\n",
      "Epoch 224 in 20.82 sec\n",
      "Training set accuracy 0.9531333333333334\n",
      "Training set loss 0.1527519644602603\n",
      "Epoch 225 in 20.86 sec\n",
      "Training set accuracy 0.9533166666666667\n",
      "Training set loss 0.15215970825401934\n",
      "Epoch 226 in 20.96 sec\n",
      "Training set accuracy 0.95345\n",
      "Training set loss 0.15157156731154336\n",
      "Epoch 227 in 20.98 sec\n",
      "Training set accuracy 0.95355\n",
      "Training set loss 0.15098746604226654\n",
      "Epoch 228 in 20.97 sec\n",
      "Training set accuracy 0.9535333333333333\n",
      "Training set loss 0.15040733340179324\n",
      "Epoch 229 in 21.04 sec\n",
      "Training set accuracy 0.9537\n",
      "Training set loss 0.14983112210503735\n",
      "Epoch 230 in 20.88 sec\n",
      "Training set accuracy 0.9538333333333333\n",
      "Training set loss 0.14925873388472088\n",
      "Epoch 231 in 21.04 sec\n",
      "Training set accuracy 0.9539333333333333\n",
      "Training set loss 0.14869018827846164\n",
      "Epoch 232 in 20.89 sec\n",
      "Training set accuracy 0.95405\n",
      "Training set loss 0.14812526603922252\n",
      "Epoch 233 in 21.04 sec\n",
      "Training set accuracy 0.9541833333333334\n",
      "Training set loss 0.14756417621566756\n",
      "Epoch 234 in 21.01 sec\n",
      "Training set accuracy 0.9544\n",
      "Training set loss 0.14700632402199784\n",
      "Epoch 235 in 20.90 sec\n",
      "Training set accuracy 0.9544666666666667\n",
      "Training set loss 0.14645244415389638\n",
      "Epoch 236 in 21.02 sec\n",
      "Training set accuracy 0.9545833333333333\n",
      "Training set loss 0.14590095063238187\n",
      "Epoch 237 in 21.03 sec\n",
      "Training set accuracy 0.9547\n",
      "Training set loss 0.1453539652648108\n",
      "Epoch 238 in 20.93 sec\n",
      "Training set accuracy 0.9547166666666667\n",
      "Training set loss 0.14480757764791705\n",
      "Epoch 239 in 21.00 sec\n",
      "Training set accuracy 0.9548833333333333\n",
      "Training set loss 0.1442672202853195\n",
      "Epoch 240 in 20.93 sec\n",
      "Training set accuracy 0.9550833333333333\n",
      "Training set loss 0.1437239839646017\n",
      "Epoch 241 in 21.05 sec\n",
      "Training set accuracy 0.9551833333333334\n",
      "Training set loss 0.14319032588417266\n",
      "Epoch 242 in 20.99 sec\n",
      "Training set accuracy 0.95545\n",
      "Training set loss 0.14264757117206095\n",
      "Epoch 243 in 20.97 sec\n",
      "Training set accuracy 0.9556\n",
      "Training set loss 0.1421213554068948\n",
      "Epoch 244 in 21.06 sec\n",
      "Training set accuracy 0.9558166666666666\n",
      "Training set loss 0.1415761857946016\n",
      "Epoch 245 in 20.99 sec\n",
      "Training set accuracy 0.9558833333333333\n",
      "Training set loss 0.1410585511237126\n",
      "Epoch 246 in 21.02 sec\n",
      "Training set accuracy 0.9559833333333333\n",
      "Training set loss 0.1405093453606504\n",
      "Epoch 247 in 21.03 sec\n",
      "Training set accuracy 0.9560166666666666\n",
      "Training set loss 0.1400000060638474\n",
      "Epoch 248 in 21.00 sec\n",
      "Training set accuracy 0.9560833333333333\n",
      "Training set loss 0.13944861756633606\n",
      "Epoch 249 in 20.94 sec\n",
      "Training set accuracy 0.9561833333333334\n",
      "Training set loss 0.13894372760853846\n",
      "Epoch 250 in 21.02 sec\n",
      "Training set accuracy 0.9564\n",
      "Training set loss 0.13839550339118575\n",
      "Epoch 251 in 21.08 sec\n",
      "Training set accuracy 0.95655\n",
      "Training set loss 0.13788903013936193\n",
      "Epoch 252 in 21.05 sec\n",
      "Training set accuracy 0.9568333333333333\n",
      "Training set loss 0.1373500703750429\n",
      "Epoch 253 in 20.90 sec\n",
      "Training set accuracy 0.9569333333333333\n",
      "Training set loss 0.1368382227335274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 254 in 21.06 sec\n",
      "Training set accuracy 0.9570166666666666\n",
      "Training set loss 0.13631187365483485\n",
      "Epoch 255 in 20.96 sec\n",
      "Training set accuracy 0.9570833333333333\n",
      "Training set loss 0.13579617165868685\n",
      "Epoch 256 in 21.09 sec\n",
      "Training set accuracy 0.9571666666666667\n",
      "Training set loss 0.13528107603137635\n",
      "Epoch 257 in 20.89 sec\n",
      "Training set accuracy 0.9573\n",
      "Training set loss 0.1347668673924167\n",
      "Epoch 258 in 20.89 sec\n",
      "Training set accuracy 0.9574333333333334\n",
      "Training set loss 0.13425968345574768\n",
      "Epoch 259 in 21.07 sec\n",
      "Training set accuracy 0.95755\n",
      "Training set loss 0.1337516490343424\n",
      "Epoch 260 in 21.06 sec\n",
      "Training set accuracy 0.9576166666666667\n",
      "Training set loss 0.13325071027173896\n",
      "Epoch 261 in 21.01 sec\n",
      "Training set accuracy 0.9578\n",
      "Training set loss 0.13275089230426346\n",
      "Epoch 262 in 21.10 sec\n",
      "Training set accuracy 0.9579666666666666\n",
      "Training set loss 0.132256648055028\n",
      "Epoch 263 in 21.00 sec\n",
      "Training set accuracy 0.9581\n",
      "Training set loss 0.1317653678127755\n",
      "Epoch 264 in 20.99 sec\n",
      "Training set accuracy 0.95825\n",
      "Training set loss 0.1312790365769812\n",
      "Epoch 265 in 21.02 sec\n",
      "Training set accuracy 0.9583333333333334\n",
      "Training set loss 0.1307964083351844\n",
      "Epoch 266 in 21.01 sec\n",
      "Training set accuracy 0.9583833333333334\n",
      "Training set loss 0.13031885454120526\n",
      "Epoch 267 in 20.98 sec\n",
      "Training set accuracy 0.9585166666666667\n",
      "Training set loss 0.12984538569687254\n",
      "Epoch 268 in 21.06 sec\n",
      "Training set accuracy 0.9586333333333333\n",
      "Training set loss 0.12937712593466816\n",
      "Epoch 269 in 21.11 sec\n",
      "Training set accuracy 0.9588333333333333\n",
      "Training set loss 0.12891345372630914\n",
      "Epoch 270 in 21.01 sec\n",
      "Training set accuracy 0.9590166666666666\n",
      "Training set loss 0.12845512445634596\n",
      "Epoch 271 in 21.09 sec\n",
      "Training set accuracy 0.9592333333333334\n",
      "Training set loss 0.12800191299235694\n",
      "Epoch 272 in 21.06 sec\n",
      "Training set accuracy 0.9593\n",
      "Training set loss 0.1275544365298771\n",
      "Epoch 273 in 21.08 sec\n",
      "Training set accuracy 0.9593666666666667\n",
      "Training set loss 0.12711255627811652\n",
      "Epoch 274 in 20.96 sec\n",
      "Training set accuracy 0.9594666666666667\n",
      "Training set loss 0.1266769403177539\n",
      "Epoch 275 in 20.94 sec\n",
      "Training set accuracy 0.95975\n",
      "Training set loss 0.1262471888904369\n",
      "Epoch 276 in 21.03 sec\n",
      "Training set accuracy 0.95995\n",
      "Training set loss 0.12582396070138524\n",
      "Epoch 277 in 21.03 sec\n",
      "Training set accuracy 0.9600166666666666\n",
      "Training set loss 0.1254064263245379\n",
      "Epoch 278 in 20.95 sec\n",
      "Training set accuracy 0.96015\n",
      "Training set loss 0.12499539388074773\n",
      "Epoch 279 in 20.88 sec\n",
      "Training set accuracy 0.9602166666666667\n",
      "Training set loss 0.12458962660041766\n",
      "Epoch 280 in 20.90 sec\n",
      "Training set accuracy 0.9603\n",
      "Training set loss 0.12419042276854604\n",
      "Epoch 281 in 20.93 sec\n",
      "Training set accuracy 0.96035\n",
      "Training set loss 0.1237959855067611\n",
      "Epoch 282 in 20.84 sec\n",
      "Training set accuracy 0.9605\n",
      "Training set loss 0.12340837000955332\n",
      "Epoch 283 in 21.24 sec\n",
      "Training set accuracy 0.96055\n",
      "Training set loss 0.12302488415057247\n",
      "Epoch 284 in 20.94 sec\n",
      "Training set accuracy 0.9607\n",
      "Training set loss 0.12264863813234866\n",
      "Epoch 285 in 21.00 sec\n",
      "Training set accuracy 0.9607666666666667\n",
      "Training set loss 0.12227569330555761\n",
      "Epoch 286 in 21.07 sec\n",
      "Training set accuracy 0.9609\n",
      "Training set loss 0.12191056573810546\n",
      "Epoch 287 in 21.01 sec\n",
      "Training set accuracy 0.961\n",
      "Training set loss 0.1215477660997456\n",
      "Epoch 288 in 21.02 sec\n",
      "Training set accuracy 0.9611166666666666\n",
      "Training set loss 0.12119349570315556\n",
      "Epoch 289 in 20.93 sec\n",
      "Training set accuracy 0.9611833333333333\n",
      "Training set loss 0.1208405303862308\n",
      "Epoch 290 in 21.08 sec\n",
      "Training set accuracy 0.9612333333333334\n",
      "Training set loss 0.12049683712224925\n",
      "Epoch 291 in 21.02 sec\n",
      "Training set accuracy 0.9613666666666667\n",
      "Training set loss 0.12015350033658444\n",
      "Epoch 292 in 21.00 sec\n",
      "Training set accuracy 0.9614833333333334\n",
      "Training set loss 0.1198200521039114\n",
      "Epoch 293 in 21.02 sec\n",
      "Training set accuracy 0.9616666666666667\n",
      "Training set loss 0.11948622592029548\n",
      "Epoch 294 in 21.00 sec\n",
      "Training set accuracy 0.9617\n",
      "Training set loss 0.11916262662309875\n",
      "Epoch 295 in 21.12 sec\n",
      "Training set accuracy 0.9618333333333333\n",
      "Training set loss 0.118838258104564\n",
      "Epoch 296 in 20.99 sec\n",
      "Training set accuracy 0.9619833333333333\n",
      "Training set loss 0.11852409123532619\n",
      "Epoch 297 in 20.88 sec\n",
      "Training set accuracy 0.9621\n",
      "Training set loss 0.11820920395625299\n",
      "Epoch 298 in 21.00 sec\n",
      "Training set accuracy 0.9621666666666666\n",
      "Training set loss 0.11790415514413236\n",
      "Epoch 299 in 20.92 sec\n",
      "Training set accuracy 0.9623666666666667\n",
      "Training set loss 0.11759892825122228\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "batch_size = 30\n",
    "num_epochs = 300\n",
    "blocks = create_blocks(rank, dim, bond_dim, label_dim)\n",
    "train_hist = []\n",
    "loss_hist = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    batch_num = 1\n",
    "    batches = batch_gen(x_train_1d, y_train_1h, batch_size)\n",
    "    for x, y in batches:\n",
    "        blocks = update(blocks, x, y, params, 1e-3)\n",
    "        #print('epoch: {}, batch: {}/{:0.0f}'.format(epoch, batch_num, len(x_train_1d)/batch_size))\n",
    "        batch_num += 1\n",
    "\n",
    "    train_acc = accuracy(blocks, x_train_1d, y_train_1h, params)\n",
    "    train_hist.append(train_acc)\n",
    "    loss_val = loss(blocks, x_train_1d, y_train_1h, params)\n",
    "    loss_hist.append(loss_val)\n",
    "    epoch_time = time.time() - start_time\n",
    "    #test_acc = accuracy(blocks, x, y, params)\n",
    "    print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
    "    print(\"Training set accuracy {}\".format(train_acc))\n",
    "    print(\"Training set loss {}\".format(loss_val))\n",
    "    #print(\"Test set accuracy {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = loss(blocks, x_test_1d, y_test_1h, params)\n",
    "test_acc = accuracy(blocks, x_test_1d, y_test_1h, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.118\n",
      "train accuracy:0.962\n",
      "test loss:0.216\n",
      "test accuracy:0.952\n"
     ]
    }
   ],
   "source": [
    "print('train loss:{:.3f}'.format(loss_val))\n",
    "print('train accuracy:{:.3f}'.format(train_acc))\n",
    "print('test loss:{:.3f}'.format(test_loss))\n",
    "print('test accuracy:{:.3f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.1)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lr 2e-5 -> 2e-4\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax1.plot(loss_hist,\n",
    "         color='r', label=\"Loss\")\n",
    "ax2.plot(train_hist, color='b',\n",
    "        label=\"Accuracy\")\n",
    " \n",
    "handler1, label1 = ax1.get_legend_handles_labels()\n",
    "handler2, label2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(handler1 + handler2, label1 + label2, loc=3, borderaxespad=0.)\n",
    "ax1.set_xlabel('Epoch', fontsize=14)\n",
    "ax1.set_ylabel('Loss', fontsize=14)\n",
    "ax2.set_ylabel('Accuracy', fontsize=14)\n",
    " \n",
    "ax1.set_ylim([0, 2.5])\n",
    "ax2.set_ylim([0, 1.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
