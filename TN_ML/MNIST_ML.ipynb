{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "#tf.enable_v2_behavior()\n",
    "# Import tensornetwork\n",
    "import tensornetwork as tn\n",
    "# Set the backend to tesorflow\n",
    "# (default is numpy)\n",
    "tn.set_default_backend(\"tensorflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 0-255の値が入っているので、0-1に収まるよう正規化します\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vec = []\n",
    "for i in range(x_train.shape[0]):\n",
    "    x_vec_ = np.concatenate([x_train[i][j, ::-2*(j%2)+1] for j in range(x_train.shape[1])])\n",
    "    x_vec.append(x_vec_)\n",
    "x_train_1d = np.vstack(x_vec)\n",
    "\n",
    "x_vec = []\n",
    "for i in range(x_test.shape[0]):\n",
    "    x_vec_ = np.concatenate([x_test[i][j, ::-2*(j%2)+1] for j in range(x_test.shape[1])])\n",
    "    x_vec.append(x_vec_)\n",
    "x_test_1d = np.vstack(x_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_1d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving a component\n",
    "\n",
    "def block(*dimensions):\n",
    "    '''Construct a new matrix for the MPS with random numbers from 0 to 1'''\n",
    "    size = tuple([x for x in dimensions])\n",
    "    return np.random.random_sample(size) / np.max(size)**0.7\n",
    "\n",
    "def create_MPS(rank, dim, bond_dim):\n",
    "    '''Build the MPS tensor'''\n",
    "    mps = [\n",
    "        tn.Node( block(dim, bond_dim) )] + \\\n",
    "        [tn.Node( block(bond_dim, dim, bond_dim)) for _ in range(rank-2)] + \\\n",
    "        [tn.Node( block(bond_dim, dim) )\n",
    "        ]\n",
    "\n",
    "    #connect edges to build mps\n",
    "    connected_edges=[]\n",
    "    conn=mps[0][1]^mps[1][0]\n",
    "    connected_edges.append(conn)\n",
    "    for k in range(1,rank-1):\n",
    "        conn=mps[k][2]^mps[k+1][0]\n",
    "        connected_edges.append(conn)\n",
    "\n",
    "    return mps, connected_edges\n",
    "\n",
    "def create_MPS_labeled(rank, dim, bond_dim):\n",
    "    '''Build the MPS tensor'''\n",
    "    half = np.int((rank - 2) / 2)\n",
    "    mps = [\n",
    "        tn.Node( block(dim, bond_dim) )] + \\\n",
    "        [tn.Node( block(bond_dim, dim, bond_dim)) for _ in range(half)] + \\\n",
    "        [tn.Node( block(bond_dim, label_dim, bond_dim) )] + \\\n",
    "        [tn.Node( block(bond_dim, dim, bond_dim)) for _ in range(half, rank-2)] + \\\n",
    "        [tn.Node( block(bond_dim, dim) )\n",
    "        ]\n",
    "\n",
    "    #connect edges to build mps\n",
    "    connected_edges=[]\n",
    "    conn=mps[0][1]^mps[1][0]\n",
    "    connected_edges.append(conn)\n",
    "    for k in range(1,rank):\n",
    "        conn=mps[k][2]^mps[k+1][0]\n",
    "        connected_edges.append(conn)\n",
    "\n",
    "    return mps, connected_edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_map(p):\n",
    "    phi = [1-p, p]\n",
    "    return phi\n",
    "\n",
    "def data_tensorize(vec):\n",
    "    data_tensor = [tn.Node(feature_map(p)) for p in vec]\n",
    "    return data_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_len = 1\n",
    "label_dim = 10\n",
    "data_len = x_train_1d.shape[1]\n",
    "rank = data_len\n",
    "dim = 2\n",
    "bond_dim = 10\n",
    "# mps, edges = create_MPS(rank, dim, bond_dim)\n",
    "mps, edges = create_MPS_labeled(rank, dim, bond_dim)\n",
    "\n",
    "test_vec = x_train_1d[0]\n",
    "data_tensor = data_tensorize(test_vec)\n",
    "\n",
    "edges.append(data_tensor[0][0] ^ mps[0][0])\n",
    "half_len = np.int(len(data_tensor) / 2)\n",
    "[edges.append(data_tensor[i][0] ^ mps[i][1]) for i in range(1, half_len)]\n",
    "[edges.append(data_tensor[i-label_len][0] ^ mps[i][1]) for i in range(half_len + label_len, data_len + label_len)]\n",
    "for k in reversed(range(len(edges))):\n",
    "    A = tn.contract(edges[k])\n",
    "result = A.tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00073183 0.00070497 0.00083472 0.00063803 0.0007883  0.00069745\n",
      " 0.00083175 0.00074088 0.00067797 0.00072349]\n",
      "[0.00073183 0.00070497 0.00083472 0.00063803 0.0007883  0.00069745\n",
      " 0.00083175 0.00074088 0.00067797 0.00072349]\n"
     ]
    }
   ],
   "source": [
    "print(A.tensor.numpy())\n",
    "print(A.tensor.numpy().astype(\"float32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block(*dimensions):\n",
    "    '''Construct a new matrix for the MPS with random numbers from 0 to 1'''\n",
    "    size = tuple([x for x in dimensions])\n",
    "    return tf.Variable(\n",
    "        tf.random.uniform(shape=size, dtype=tf.dtypes.float32, maxval= 1/ np.max(size)**0.7),\n",
    "        trainable=True)\n",
    "\n",
    "def create_blocks(rank, dim, bond_dim, label_dim):\n",
    "    half = np.int((rank - 2) / 2)\n",
    "    blocks = [\n",
    "        block(dim, bond_dim) ] + \\\n",
    "        [ block(bond_dim, dim, bond_dim) for _ in range(half)] + \\\n",
    "        [ block(bond_dim, label_dim, bond_dim) ] + \\\n",
    "        [ block(bond_dim, dim, bond_dim) for _ in range(half, rank-2)] + \\\n",
    "        [ block(bond_dim, dim) \n",
    "        ]\n",
    "    return blocks\n",
    "\n",
    "def create_MPS_labeled(blocks, rank, dim, bond_dim):\n",
    "    '''Build the MPS tensor'''\n",
    "    half = np.int((rank - 2) / 2)\n",
    "    mps = []\n",
    "    for b in blocks:\n",
    "        mps.append(tn.Node(b))\n",
    "\n",
    "    #connect edges to build mps\n",
    "    connected_edges=[]\n",
    "    conn=mps[0][1]^mps[1][0]\n",
    "    connected_edges.append(conn)\n",
    "    for k in range(1,rank):\n",
    "        conn=mps[k][2]^mps[k+1][0]\n",
    "        connected_edges.append(conn)\n",
    "\n",
    "    return mps, connected_edges\n",
    "\n",
    "class TNLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_len, label_num, bond_dim):\n",
    "        self.label_len = 1\n",
    "        self.label_dim = label_num\n",
    "        self.rank = input_len\n",
    "        self.dim = 2\n",
    "        self.bond_dim = bond_dim\n",
    "        #super(TNLayer, self).__init__()\n",
    "        super().__init__()\n",
    "        # Create the variables for the layer.\n",
    "        self.blocks = create_blocks(self.rank, self.dim, self.bond_dim, self.label_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        def f(input_vec, blocks, rank, dim, bond_dim, label_len):\n",
    "            mps, edges = create_MPS_labeled(blocks, rank, dim, bond_dim)\n",
    "            data_tensor = []\n",
    "            for p in tf.unstack(input_vec):\n",
    "                data_tensor.append(tn.Node([1-p, p]))\n",
    "            edges.append(data_tensor[0][0] ^ mps[0][0])\n",
    "            half_len = np.int(rank / 2)\n",
    "            [edges.append(data_tensor[i][0] ^ mps[i][1]) for i in range(1, half_len)]\n",
    "            [edges.append(data_tensor[i-label_len][0] ^ mps[i][1]) \\\n",
    "                 for i in range(half_len + label_len, rank + label_len)]\n",
    "            for k in reversed(range(len(edges))):\n",
    "                A = tn.contract(edges[k])\n",
    "            result = A.tensor\n",
    "            return result\n",
    "\n",
    "        result = tf.vectorized_map(\n",
    "        lambda vec: f(vec, self.blocks, self.rank, self.dim, self.bond_dim, self.label_len), inputs)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "tn_layer_4 (TNLayer)         (None, 10)                157440    \n",
      "_________________________________________________________________\n",
      "softmax_4 (Softmax)          (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 157,440\n",
      "Trainable params: 157,440\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "N = x_train_1d.shape[1]\n",
    "label_len = 1\n",
    "label_num = 10\n",
    "data_len = x_train_1d.shape[1]\n",
    "rank = data_len\n",
    "dim = 2\n",
    "bond_dim = 10\n",
    "\n",
    "tn_model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(N,)),\n",
    "        TNLayer(N, label_num, bond_dim),\n",
    "        tf.keras.layers.Softmax()\n",
    "    ])\n",
    "tn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1875/1875 [==============================] - 94s 50ms/step - loss: 2.1834 - accuracy: 0.1118\n",
      "Epoch 2/300\n",
      "1875/1875 [==============================] - 95s 51ms/step - loss: 2.2195 - accuracy: 0.1139\n",
      "Epoch 3/300\n",
      "1875/1875 [==============================] - 95s 51ms/step - loss: 2.2228 - accuracy: 0.1266\n",
      "Epoch 4/300\n",
      "1875/1875 [==============================] - 95s 51ms/step - loss: 2.2124 - accuracy: 0.1142\n",
      "Epoch 5/300\n",
      "1875/1875 [==============================] - 94s 50ms/step - loss: 2.0559 - accuracy: 0.1033\n",
      "Epoch 6/300\n",
      "1875/1875 [==============================] - 94s 50ms/step - loss: 2.2577 - accuracy: 0.1399\n",
      "Epoch 7/300\n",
      "1875/1875 [==============================] - 95s 51ms/step - loss: 2.2305 - accuracy: 0.1514\n",
      "Epoch 8/300\n",
      "1875/1875 [==============================] - 95s 50ms/step - loss: 2.2506 - accuracy: 0.1225\n",
      "Epoch 9/300\n",
      "1875/1875 [==============================] - 95s 51ms/step - loss: 2.3026 - accuracy: 0.1254\n",
      "Epoch 10/300\n",
      "1875/1875 [==============================] - 95s 50ms/step - loss: 2.3026 - accuracy: 0.1254\n",
      "Epoch 11/300\n",
      " 215/1875 [==>...........................] - ETA: 1:22 - loss: 2.3026 - accuracy: 0.1233"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-167-4016e9e9bb86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                  \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseCategoricalCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                  metrics=['accuracy'])\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_1d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "tn_model.compile(optimizer=optimizer, \n",
    "                 loss=tf.keras.losses.SparseCategoricalCrossentropy(), \n",
    "                 metrics=['accuracy'])\n",
    "tn_model.fit(x_train_1d, y_train, epochs=300, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a channels dimension\n",
    "x_train_1d = x_train_1d.astype(\"float32\")\n",
    "x_test_1d = x_test_1d.astype(\"float32\")\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train_1d, y_train)).shuffle(60000).batch(32)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test_1d, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "\n",
    "class MyModel(Model):\n",
    "    def __init__(self, input_len, label_num, bond_dim):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.label_len = 1\n",
    "        self.label_dim = label_num\n",
    "        self.rank = input_len\n",
    "        self.dim = 2\n",
    "        self.bond_dim = bond_dim\n",
    "        self.blocks = create_blocks(self.rank, self.dim, self.bond_dim, self.label_dim)\n",
    "        self.softmax = tf.keras.layers.Softmax()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        def f(input_vec, blocks, rank, dim, bond_dim, label_len):\n",
    "            mps, edges = create_MPS_labeled(blocks, rank, dim, bond_dim)\n",
    "            data_tensor = []\n",
    "            for p in tf.unstack(input_vec):\n",
    "                data_tensor.append(tn.Node([1-p, p]))\n",
    "            edges.append(data_tensor[0][0] ^ mps[0][0])\n",
    "            half_len = np.int(rank / 2)\n",
    "            [edges.append(data_tensor[i][0] ^ mps[i][1]) for i in range(1, half_len)]\n",
    "            [edges.append(data_tensor[i-label_len][0] ^ mps[i][1]) \\\n",
    "                 for i in range(half_len + label_len, rank + label_len)]\n",
    "            for k in reversed(range(len(edges))):\n",
    "                A = tn.contract(edges[k])\n",
    "            result = A.tensor\n",
    "            return self.softmax(result)\n",
    "\n",
    "        result = tf.vectorized_map(\n",
    "        lambda vec: f(vec, self.blocks, self.rank, self.dim, self.bond_dim, self.label_len), inputs)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = x_train_1d.shape[1]\n",
    "label_len = 1\n",
    "label_num = 10\n",
    "data_len = x_train_1d.shape[1]\n",
    "rank = data_len\n",
    "dim = 2\n",
    "bond_dim = 10\n",
    "\n",
    "model = MyModel(N, label_num, bond_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images, training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    print('gradients sample:', gradients[0].numpy())\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = model(images, training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function pfor.<locals>.f at 0x7f724cf87ef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "gradients sample: [[3.6746396e-06 3.3691854e-06 3.1413842e-06 2.8855334e-06 3.9668039e-06\n",
      "  3.0522365e-06 3.1187069e-06 3.3026095e-06 2.9956864e-06 2.8563418e-06]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]]\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function pfor.<locals>.f at 0x7f724faf3f80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "gradients sample: [[5.7549914e-06 5.2766363e-06 4.9187611e-06 4.5194179e-06 6.2101740e-06\n",
      "  4.7793137e-06 4.8833635e-06 5.1697107e-06 4.6896384e-06 4.4687740e-06]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]]\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function pfor.<locals>.f at 0x7f724d496c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "gradients sample: [[-9.2146763e-07 -8.4487743e-07 -7.8728436e-07 -7.2371444e-07\n",
      "  -9.9373153e-07 -7.6499981e-07 -7.8164118e-07 -8.2705776e-07\n",
      "  -7.5034848e-07 -7.1432862e-07]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]]\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function pfor.<locals>.f at 0x7f720dca4cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "gradients sample: [[-2.3355683e-06 -2.1414346e-06 -1.9949559e-06 -1.8344751e-06\n",
      "  -2.5176560e-06 -1.9385516e-06 -1.9806987e-06 -2.0950724e-06\n",
      "  -1.9008960e-06 -1.8084711e-06]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]]\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function pfor.<locals>.f at 0x7f720d611d40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "gradients sample: [[8.0213550e-07 7.3546261e-07 6.8506358e-07 6.3006212e-07 8.6448375e-07\n",
      "  6.6570499e-07 6.8017414e-07 7.1932106e-07 6.5268148e-07 6.2073781e-07]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]]\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function pfor.<locals>.f at 0x7f73a2899c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-203-55de3efbb858>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-201-7994f6f35f64>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(images, labels)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gradients sample:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1073\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_backward_function_wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   1264\u001b[0m           \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m       return backward._call_flat(  # pylint: disable=protected-access\n\u001b[0;32m-> 1266\u001b[0;31m           processed_args, remapped_captures)\n\u001b[0m\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_backward_function_wrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecorded_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "    for images, labels in train_ds:\n",
    "        train_step(images, labels)\n",
    "\n",
    "    for test_images, test_labels in test_ds:\n",
    "        test_step(test_images, test_labels)\n",
    "\n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "    print(template.format(epoch + 1,\n",
    "                        train_loss.result(),\n",
    "                        train_accuracy.result() * 100,\n",
    "                        test_loss.result(),\n",
    "                        test_accuracy.result() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function pfor.<locals>.f at 0x7f73a18c4dd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "(image, label) = (x_train_1d[0], y_train[0])\n",
    "with tf.GradientTape() as tape:\n",
    "    # training=True is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = model(images, training=True)\n",
    "    loss = loss_object(labels, predictions)\n",
    "gradients = tape.gradient(loss, model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 10), dtype=float32, numpy=\n",
       "array([[0.10000021, 0.09999987, 0.1       , 0.10000007, 0.10000008,\n",
       "        0.09999987, 0.10000004, 0.10000004, 0.09999996, 0.0999999 ],\n",
       "       [0.10000028, 0.09999982, 0.09999999, 0.10000009, 0.1000001 ,\n",
       "        0.09999983, 0.10000005, 0.10000005, 0.09999993, 0.09999987],\n",
       "       [0.10000018, 0.09999987, 0.09999999, 0.10000005, 0.10000006,\n",
       "        0.09999988, 0.10000002, 0.10000002, 0.09999996, 0.0999999 ],\n",
       "       [0.10000017, 0.0999999 , 0.10000001, 0.10000006, 0.10000007,\n",
       "        0.0999999 , 0.10000004, 0.10000004, 0.09999997, 0.09999993],\n",
       "       [0.10000011, 0.09999991, 0.09999999, 0.10000003, 0.10000004,\n",
       "        0.09999991, 0.10000001, 0.10000001, 0.09999996, 0.09999993],\n",
       "       [0.10000022, 0.09999985, 0.09999999, 0.10000007, 0.10000008,\n",
       "        0.09999986, 0.10000004, 0.10000004, 0.09999995, 0.0999999 ],\n",
       "       [0.10000015, 0.0999999 , 0.1       , 0.10000005, 0.10000005,\n",
       "        0.0999999 , 0.10000002, 0.10000002, 0.09999996, 0.09999993],\n",
       "       [0.10000031, 0.09999982, 0.1       , 0.10000011, 0.10000011,\n",
       "        0.09999982, 0.10000006, 0.10000006, 0.09999994, 0.09999987],\n",
       "       [0.10000043, 0.09999972, 0.09999999, 0.10000014, 0.10000016,\n",
       "        0.09999973, 0.10000008, 0.10000008, 0.0999999 , 0.09999979],\n",
       "       [0.10000051, 0.09999967, 0.09999999, 0.10000017, 0.10000019,\n",
       "        0.09999969, 0.10000009, 0.10000009, 0.09999989, 0.09999976],\n",
       "       [0.10000031, 0.0999998 , 0.09999999, 0.1000001 , 0.10000011,\n",
       "        0.09999982, 0.10000005, 0.10000005, 0.09999993, 0.09999985],\n",
       "       [0.10000018, 0.09999987, 0.09999999, 0.10000005, 0.10000006,\n",
       "        0.09999988, 0.10000002, 0.10000002, 0.09999996, 0.0999999 ],\n",
       "       [0.1000001 , 0.09999994, 0.1       , 0.10000003, 0.10000004,\n",
       "        0.09999994, 0.10000002, 0.10000002, 0.09999999, 0.09999996],\n",
       "       [0.10000053, 0.09999964, 0.09999999, 0.10000017, 0.10000019,\n",
       "        0.09999967, 0.10000009, 0.10000009, 0.09999987, 0.09999974],\n",
       "       [0.10000019, 0.09999987, 0.09999999, 0.10000005, 0.10000006,\n",
       "        0.09999987, 0.10000002, 0.10000002, 0.09999995, 0.0999999 ],\n",
       "       [0.10000018, 0.09999987, 0.09999999, 0.10000005, 0.10000006,\n",
       "        0.09999988, 0.10000002, 0.10000003, 0.09999996, 0.0999999 ],\n",
       "       [0.10000042, 0.09999972, 0.09999999, 0.10000014, 0.10000015,\n",
       "        0.09999974, 0.10000008, 0.10000008, 0.0999999 , 0.0999998 ],\n",
       "       [0.10000008, 0.09999994, 0.1       , 0.10000002, 0.10000003,\n",
       "        0.09999995, 0.10000001, 0.10000001, 0.09999998, 0.09999996],\n",
       "       [0.10000018, 0.09999987, 0.09999999, 0.10000005, 0.10000007,\n",
       "        0.09999989, 0.10000003, 0.10000003, 0.09999996, 0.09999991],\n",
       "       [0.10000022, 0.09999986, 0.09999999, 0.10000007, 0.10000008,\n",
       "        0.09999987, 0.10000004, 0.10000004, 0.09999995, 0.0999999 ],\n",
       "       [0.10000025, 0.09999982, 0.09999999, 0.10000008, 0.10000008,\n",
       "        0.09999984, 0.10000004, 0.10000004, 0.09999993, 0.09999987],\n",
       "       [0.10000072, 0.09999952, 0.09999999, 0.10000023, 0.10000026,\n",
       "        0.09999955, 0.10000013, 0.10000013, 0.09999984, 0.09999966],\n",
       "       [0.10000007, 0.09999996, 0.1       , 0.10000002, 0.10000002,\n",
       "        0.09999996, 0.10000002, 0.10000002, 0.09999999, 0.09999997],\n",
       "       [0.10000014, 0.09999991, 0.1       , 0.10000005, 0.10000005,\n",
       "        0.09999992, 0.10000002, 0.10000002, 0.09999997, 0.09999994],\n",
       "       [0.1000002 , 0.09999987, 0.1       , 0.10000007, 0.10000008,\n",
       "        0.09999987, 0.10000004, 0.10000004, 0.09999995, 0.0999999 ],\n",
       "       [0.10000157, 0.09999896, 0.09999997, 0.10000051, 0.10000057,\n",
       "        0.09999903, 0.10000027, 0.10000027, 0.09999964, 0.09999924],\n",
       "       [0.10000039, 0.09999973, 0.09999999, 0.10000012, 0.10000014,\n",
       "        0.09999975, 0.10000006, 0.10000006, 0.0999999 , 0.09999981],\n",
       "       [0.10000031, 0.09999979, 0.09999999, 0.1000001 , 0.10000011,\n",
       "        0.09999982, 0.10000005, 0.10000005, 0.09999993, 0.09999985],\n",
       "       [0.10000048, 0.09999969, 0.09999999, 0.10000016, 0.10000017,\n",
       "        0.09999971, 0.10000008, 0.10000008, 0.0999999 , 0.09999978],\n",
       "       [0.10000025, 0.09999984, 0.09999999, 0.10000008, 0.10000008,\n",
       "        0.09999985, 0.10000004, 0.10000004, 0.09999994, 0.09999987],\n",
       "       [0.10000023, 0.09999985, 0.09999999, 0.10000008, 0.10000008,\n",
       "        0.09999986, 0.10000004, 0.10000004, 0.09999995, 0.09999989],\n",
       "       [0.10000027, 0.09999983, 0.1       , 0.10000009, 0.10000011,\n",
       "        0.09999984, 0.10000005, 0.10000005, 0.09999994, 0.09999987]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=2.3025851>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_object(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
