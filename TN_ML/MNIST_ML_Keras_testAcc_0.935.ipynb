{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensornetwork as tn\n",
    "# Set the backend to tesorflow\n",
    "# (default is numpy)\n",
    "tn.set_default_backend(\"tensorflow\")\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Tensor Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_map(p):\n",
    "    phi = [1-p, p]\n",
    "    return phi\n",
    "\n",
    "def data_tensorize(vec):\n",
    "    data_tensor = [tn.Node(feature_map(p)) for p in vec]\n",
    "    return data_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block(*dimensions):\n",
    "    '''Construct a new matrix for the MPS with random numbers from 0 to 1'''\n",
    "    size = tuple([x for x in dimensions])\n",
    "    return tf.Variable(\n",
    "        tf.random.normal(shape=size, dtype=tf.dtypes.float64, mean= 0, stddev = 0.5),\n",
    "        trainable=True)\n",
    "\n",
    "def create_blocks(rank, dim, bond_dim, label_dim):\n",
    "    half = np.int((rank - 2) / 2)\n",
    "    blocks = [\n",
    "        block(dim, bond_dim) ] + \\\n",
    "        [ block(bond_dim, dim, bond_dim) for _ in range(half)] + \\\n",
    "        [ block(bond_dim, label_dim, bond_dim) ] + \\\n",
    "        [ block(bond_dim, dim, bond_dim) for _ in range(half, rank-2)] + \\\n",
    "        [ block(bond_dim, dim) \n",
    "        ]\n",
    "    return blocks\n",
    "\n",
    "def create_MPS_labeled(blocks, rank, dim, bond_dim):\n",
    "    '''Build the MPS tensor'''\n",
    "    half = np.int((rank - 2) / 2)\n",
    "    mps = []\n",
    "    for b in blocks:\n",
    "        mps.append(tn.Node(b))\n",
    "\n",
    "    #connect edges to build mps\n",
    "    connected_edges=[]\n",
    "    conn=mps[0][1]^mps[1][0]\n",
    "    connected_edges.append(conn)\n",
    "    for k in range(1,rank):\n",
    "        conn=mps[k][2]^mps[k+1][0]\n",
    "        connected_edges.append(conn)\n",
    "\n",
    "    return mps, connected_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras custom layer for Tensor Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TNLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_len, label_num, bond_dim):\n",
    "        self.label_len = 1\n",
    "        self.label_dim = label_num\n",
    "        self.rank = input_len\n",
    "        self.dim = 2\n",
    "        self.bond_dim = bond_dim\n",
    "        #super(TNLayer, self).__init__()\n",
    "        super().__init__()\n",
    "        # Create the variables for the layer.\n",
    "        self.blocks = create_blocks(self.rank, self.dim, self.bond_dim, self.label_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        def f(input_vec, blocks, rank, dim, bond_dim, label_len):\n",
    "            mps, edges = create_MPS_labeled(blocks, rank, dim, bond_dim)\n",
    "            data_tensor = []\n",
    "            for p in tf.unstack(input_vec):\n",
    "                data_tensor.append(tn.Node([1-p, p]))\n",
    "            edges.append(data_tensor[0][0] ^ mps[0][0])\n",
    "            half_len = np.int(rank / 2)\n",
    "            [edges.append(data_tensor[i][0] ^ mps[i][1]) for i in range(1, half_len)]\n",
    "            [edges.append(data_tensor[i-label_len][0] ^ mps[i][1]) \\\n",
    "                 for i in range(half_len + label_len, rank + label_len)]\n",
    "            for k in reversed(range(len(edges))):\n",
    "                A = tn.contract(edges[k])\n",
    "            #result = tf.math.log(A.tensor)\n",
    "            result = A.tensor - tf.math.reduce_max(A.tensor)\n",
    "            return result\n",
    "\n",
    "        result = tf.vectorized_map(\n",
    "        lambda vec: f(vec, self.blocks, self.rank, self.dim, self.bond_dim, self.label_len), inputs)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "average_pooling2d_1 (Average (None, 14, 14, 1)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 196)               0         \n",
      "_________________________________________________________________\n",
      "tn_layer_2 (TNLayer)         (None, 10)                9970      \n",
      "_________________________________________________________________\n",
      "softmax_2 (Softmax)          (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 9,970\n",
      "Trainable params: 9,970\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "label_len = 1\n",
    "label_num = 10\n",
    "dim = 2\n",
    "bond_dim = 5\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "tn_model2 = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(28, 28, 1)),\n",
    "        tf.keras.layers.AveragePooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        TNLayer(196, label_num, bond_dim),\n",
    "        tf.keras.layers.Softmax()\n",
    "    ],name=\"sequential_2\")\n",
    "\n",
    "tn_model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 1/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.2854 - accuracy: 0.1112\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 2/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.2559 - accuracy: 0.1368\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 3/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 2.2369 - accuracy: 0.1379\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 4/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 2.2215 - accuracy: 0.1403\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 5/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.2078 - accuracy: 0.1418\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 6/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.1953 - accuracy: 0.1464\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 7/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.1821 - accuracy: 0.1513\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 8/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 2.1694 - accuracy: 0.1540\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 9/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.1563 - accuracy: 0.1552\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 10/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 2.1455 - accuracy: 0.1575\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 11/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 2.1369 - accuracy: 0.1595\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 12/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 2.1301 - accuracy: 0.1616\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 13/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 2.1245 - accuracy: 0.1628\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 14/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 2.1189 - accuracy: 0.1637\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 15/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.1131 - accuracy: 0.1639\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 16/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.1030 - accuracy: 0.1658\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 17/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0932 - accuracy: 0.1674\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 18/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0878 - accuracy: 0.1691\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 19/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0845 - accuracy: 0.1693\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 20/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0812 - accuracy: 0.1689\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 21/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0786 - accuracy: 0.1702\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 22/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 2.0762 - accuracy: 0.1703\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 23/300\n",
      "1875/1875 [==============================] - 46s 24ms/step - loss: 2.0738 - accuracy: 0.1716\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 24/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 2.0717 - accuracy: 0.1719\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 25/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0700 - accuracy: 0.1720\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 26/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0683 - accuracy: 0.1721\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 27/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 2.0668 - accuracy: 0.1731\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 28/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 2.0655 - accuracy: 0.1731\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 29/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 2.0646 - accuracy: 0.1727\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 30/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 2.0637 - accuracy: 0.1726\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 31/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 2.0629 - accuracy: 0.1726\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 32/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 2.0623 - accuracy: 0.1729\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 33/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0617 - accuracy: 0.1736\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 34/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 2.0612 - accuracy: 0.1735\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 35/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0608 - accuracy: 0.1742\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 36/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0604 - accuracy: 0.1747\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 37/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 2.0600 - accuracy: 0.1751\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 38/300\n",
      "1875/1875 [==============================] - 45s 24ms/step - loss: 2.0593 - accuracy: 0.1759\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 39/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 2.0591 - accuracy: 0.1770\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 40/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 2.0587 - accuracy: 0.1775\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 41/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0585 - accuracy: 0.1784\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 42/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0581 - accuracy: 0.1799\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 43/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0578 - accuracy: 0.1802\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 44/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0575 - accuracy: 0.1801\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 45/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0573 - accuracy: 0.1806\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 46/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0570 - accuracy: 0.1819\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 47/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0569 - accuracy: 0.1826\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 48/300\n",
      "1875/1875 [==============================] - 46s 24ms/step - loss: 2.0566 - accuracy: 0.1840\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 49/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0564 - accuracy: 0.1842\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 50/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 2.0562 - accuracy: 0.1852\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 51/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 2.0560 - accuracy: 0.1858\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 52/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0558 - accuracy: 0.1869\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 53/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0557 - accuracy: 0.1873\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 54/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 2.0555 - accuracy: 0.1882\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 55/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0554 - accuracy: 0.1891\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 56/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 2.0551 - accuracy: 0.1903\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 57/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 2.0550 - accuracy: 0.1910\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 58/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 2.0548 - accuracy: 0.1913\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 59/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0546 - accuracy: 0.1926\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 60/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0545 - accuracy: 0.1926\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 61/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 2.0543 - accuracy: 0.1935\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 62/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0541 - accuracy: 0.1941\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 63/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 2.0540 - accuracy: 0.1951\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 64/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0538 - accuracy: 0.1958\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 65/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0537 - accuracy: 0.1962\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 66/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0535 - accuracy: 0.1968\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 67/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0533 - accuracy: 0.1974\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 68/300\n",
      "1875/1875 [==============================] - 46s 24ms/step - loss: 2.0532 - accuracy: 0.1985\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 69/300\n",
      "1875/1875 [==============================] - 45s 24ms/step - loss: 2.0531 - accuracy: 0.1988\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 70/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0529 - accuracy: 0.2006\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 71/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 2.0527 - accuracy: 0.2005\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 72/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0526 - accuracy: 0.2014\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 73/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0524 - accuracy: 0.2020\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 74/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0523 - accuracy: 0.2024\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 75/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 2.0521 - accuracy: 0.2037\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 76/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 2.0519 - accuracy: 0.2044\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 77/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 2.0517 - accuracy: 0.2046\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 78/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0514 - accuracy: 0.2057\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 79/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0511 - accuracy: 0.2070\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 80/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 2.0507 - accuracy: 0.2079\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 81/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 2.0501 - accuracy: 0.2091\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 82/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 2.0493 - accuracy: 0.2107\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 83/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 2.0477 - accuracy: 0.2122\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 84/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0447 - accuracy: 0.2150\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 85/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 2.0407 - accuracy: 0.2162\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 86/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0356 - accuracy: 0.2198\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 87/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0297 - accuracy: 0.2235\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 88/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 2.0231 - accuracy: 0.2274\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 89/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0160 - accuracy: 0.2298\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 90/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0082 - accuracy: 0.2327\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 91/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0000 - accuracy: 0.2351\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 92/300\n",
      "1875/1875 [==============================] - 46s 24ms/step - loss: 1.9910 - accuracy: 0.2381\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 93/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 1.9809 - accuracy: 0.2454\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 94/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 45s 24ms/step - loss: 1.9693 - accuracy: 0.2581\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 95/300\n",
      "1875/1875 [==============================] - 45s 24ms/step - loss: 1.9569 - accuracy: 0.2688\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 96/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 1.9438 - accuracy: 0.2748\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 97/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 1.9306 - accuracy: 0.2795\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 98/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 1.9159 - accuracy: 0.2844\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 99/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 1.9019 - accuracy: 0.2897\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 100/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 1.8877 - accuracy: 0.2939\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 101/300\n",
      "1875/1875 [==============================] - 46s 24ms/step - loss: 1.8359 - accuracy: 0.3115\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 102/300\n",
      "1875/1875 [==============================] - 46s 24ms/step - loss: 1.7415 - accuracy: 0.3284\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 103/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 1.6765 - accuracy: 0.3402\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 104/300\n",
      "1875/1875 [==============================] - 45s 24ms/step - loss: 1.5868 - accuracy: 0.3857\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 105/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 1.5150 - accuracy: 0.4035\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 106/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 1.4733 - accuracy: 0.4103\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 107/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 1.4540 - accuracy: 0.4152\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 108/300\n",
      "1875/1875 [==============================] - 45s 24ms/step - loss: 1.4378 - accuracy: 0.4191\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 109/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 1.4220 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 110/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 1.3897 - accuracy: 0.4529\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 111/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 1.2853 - accuracy: 0.5310\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 112/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 1.1747 - accuracy: 0.5951\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 113/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 1.0954 - accuracy: 0.6219\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 114/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 1.0485 - accuracy: 0.6418\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 115/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 1.0131 - accuracy: 0.6614\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 116/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 0.9806 - accuracy: 0.6787\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 117/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 0.9528 - accuracy: 0.6910\n",
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 118/300\n",
      "1875/1875 [==============================] - 46s 24ms/step - loss: 0.9229 - accuracy: 0.7043\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 119/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 0.8926 - accuracy: 0.7148\n",
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 120/300\n",
      "1875/1875 [==============================] - 45s 24ms/step - loss: 0.8632 - accuracy: 0.7275\n",
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 121/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 0.8377 - accuracy: 0.7381\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 122/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.8120 - accuracy: 0.7486\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 123/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.7906 - accuracy: 0.7585\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 124/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.7702 - accuracy: 0.7661\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 125/300\n",
      "1875/1875 [==============================] - 45s 24ms/step - loss: 0.7523 - accuracy: 0.7716\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 126/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 0.7360 - accuracy: 0.7758\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 127/300\n",
      "1875/1875 [==============================] - 46s 24ms/step - loss: 0.7226 - accuracy: 0.7805\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 128/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 0.7094 - accuracy: 0.7831\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 129/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 0.6965 - accuracy: 0.7865\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 130/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 0.6869 - accuracy: 0.7900\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 131/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 0.6770 - accuracy: 0.7933\n",
      "\n",
      "Epoch 00132: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 132/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.6681 - accuracy: 0.7958\n",
      "\n",
      "Epoch 00133: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 133/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.6575 - accuracy: 0.7979\n",
      "\n",
      "Epoch 00134: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 134/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.6479 - accuracy: 0.8015\n",
      "\n",
      "Epoch 00135: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 135/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.6365 - accuracy: 0.8051\n",
      "\n",
      "Epoch 00136: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 136/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.6277 - accuracy: 0.8086\n",
      "\n",
      "Epoch 00137: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 137/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.6156 - accuracy: 0.8138\n",
      "\n",
      "Epoch 00138: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 138/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.6023 - accuracy: 0.8166\n",
      "\n",
      "Epoch 00139: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 139/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 0.5887 - accuracy: 0.8215\n",
      "\n",
      "Epoch 00140: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 140/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.5763 - accuracy: 0.8264\n",
      "\n",
      "Epoch 00141: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 141/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.5624 - accuracy: 0.8302\n",
      "\n",
      "Epoch 00142: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 142/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 0.5511 - accuracy: 0.8333\n",
      "\n",
      "Epoch 00143: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 143/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 0.5400 - accuracy: 0.8380\n",
      "\n",
      "Epoch 00144: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 144/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.5283 - accuracy: 0.8414\n",
      "\n",
      "Epoch 00145: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 145/300\n",
      "1875/1875 [==============================] - 46s 24ms/step - loss: 0.5185 - accuracy: 0.8448\n",
      "\n",
      "Epoch 00146: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 146/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.5081 - accuracy: 0.8455\n",
      "\n",
      "Epoch 00147: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 147/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 0.5005 - accuracy: 0.8481\n",
      "\n",
      "Epoch 00148: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 148/300\n",
      "1875/1875 [==============================] - 45s 24ms/step - loss: 0.4945 - accuracy: 0.8489\n",
      "\n",
      "Epoch 00149: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 149/300\n",
      "1875/1875 [==============================] - 45s 24ms/step - loss: 0.4862 - accuracy: 0.8511\n",
      "\n",
      "Epoch 00150: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 150/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 0.4789 - accuracy: 0.8548\n",
      "\n",
      "Epoch 00151: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 151/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 0.4747 - accuracy: 0.8556\n",
      "\n",
      "Epoch 00152: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 152/300\n",
      "1875/1875 [==============================] - 45s 24ms/step - loss: 0.4692 - accuracy: 0.8569\n",
      "\n",
      "Epoch 00153: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 153/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 0.4617 - accuracy: 0.8591\n",
      "\n",
      "Epoch 00154: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 154/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.4532 - accuracy: 0.8619\n",
      "\n",
      "Epoch 00155: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 155/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 0.4477 - accuracy: 0.8639\n",
      "\n",
      "Epoch 00156: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 156/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 0.4403 - accuracy: 0.8668\n",
      "\n",
      "Epoch 00157: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 157/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.4328 - accuracy: 0.8692\n",
      "\n",
      "Epoch 00158: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 158/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.4265 - accuracy: 0.8719\n",
      "\n",
      "Epoch 00159: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 159/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 0.4200 - accuracy: 0.8735\n",
      "\n",
      "Epoch 00160: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 160/300\n",
      "1875/1875 [==============================] - 45s 24ms/step - loss: 0.4152 - accuracy: 0.8754\n",
      "\n",
      "Epoch 00161: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 161/300\n",
      "1875/1875 [==============================] - 45s 24ms/step - loss: 0.4083 - accuracy: 0.8774\n",
      "\n",
      "Epoch 00162: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 162/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.4026 - accuracy: 0.8803\n",
      "\n",
      "Epoch 00163: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 163/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.3987 - accuracy: 0.8821\n",
      "\n",
      "Epoch 00164: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 164/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 0.3918 - accuracy: 0.8834\n",
      "\n",
      "Epoch 00165: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 165/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.3880 - accuracy: 0.8850\n",
      "\n",
      "Epoch 00166: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 166/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.3823 - accuracy: 0.8868\n",
      "\n",
      "Epoch 00167: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 167/300\n",
      "1875/1875 [==============================] - 46s 24ms/step - loss: 0.3761 - accuracy: 0.8878\n",
      "\n",
      "Epoch 00168: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 168/300\n",
      "1875/1875 [==============================] - 46s 24ms/step - loss: 0.3715 - accuracy: 0.8900\n",
      "\n",
      "Epoch 00169: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 169/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.3671 - accuracy: 0.8920\n",
      "\n",
      "Epoch 00170: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 170/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 0.3631 - accuracy: 0.8931\n",
      "\n",
      "Epoch 00171: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 171/300\n",
      "1875/1875 [==============================] - 46s 24ms/step - loss: 0.3576 - accuracy: 0.8940\n",
      "\n",
      "Epoch 00172: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 172/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 0.3538 - accuracy: 0.8954\n",
      "\n",
      "Epoch 00173: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 173/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.3507 - accuracy: 0.8963\n",
      "\n",
      "Epoch 00174: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 174/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.3493 - accuracy: 0.8971\n",
      "\n",
      "Epoch 00175: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 175/300\n",
      "1875/1875 [==============================] - 46s 24ms/step - loss: 0.3443 - accuracy: 0.89760s - loss: 0.3440 - \n",
      "\n",
      "Epoch 00176: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 176/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.3408 - accuracy: 0.8993\n",
      "\n",
      "Epoch 00177: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 177/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 0.3394 - accuracy: 0.9004\n",
      "\n",
      "Epoch 00178: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 178/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 0.3350 - accuracy: 0.9004\n",
      "\n",
      "Epoch 00179: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 179/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 0.3328 - accuracy: 0.9014\n",
      "\n",
      "Epoch 00180: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 180/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.3308 - accuracy: 0.9027\n",
      "\n",
      "Epoch 00181: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 181/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.3281 - accuracy: 0.9027\n",
      "\n",
      "Epoch 00182: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 182/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.3251 - accuracy: 0.9031\n",
      "\n",
      "Epoch 00183: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 183/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 0.3227 - accuracy: 0.9040\n",
      "\n",
      "Epoch 00184: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 184/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.3212 - accuracy: 0.9043\n",
      "\n",
      "Epoch 00185: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 185/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.3181 - accuracy: 0.9059\n",
      "\n",
      "Epoch 00186: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 186/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.3170 - accuracy: 0.9050\n",
      "\n",
      "Epoch 00187: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 187/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 0.3156 - accuracy: 0.9056\n",
      "\n",
      "Epoch 00188: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 188/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.3136 - accuracy: 0.9060\n",
      "\n",
      "Epoch 00189: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 189/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.3119 - accuracy: 0.9073\n",
      "\n",
      "Epoch 00190: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 190/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.3109 - accuracy: 0.9076\n",
      "\n",
      "Epoch 00191: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 191/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.3073 - accuracy: 0.9084\n",
      "\n",
      "Epoch 00192: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 192/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.3053 - accuracy: 0.9076\n",
      "\n",
      "Epoch 00193: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 193/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 0.3064 - accuracy: 0.9085\n",
      "\n",
      "Epoch 00194: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 194/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.3021 - accuracy: 0.9084\n",
      "\n",
      "Epoch 00195: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 195/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.3004 - accuracy: 0.9086\n",
      "\n",
      "Epoch 00196: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 196/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.2990 - accuracy: 0.9102\n",
      "\n",
      "Epoch 00197: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 197/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 0.2963 - accuracy: 0.9105\n",
      "\n",
      "Epoch 00198: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 198/300\n",
      "1875/1875 [==============================] - 45s 24ms/step - loss: 0.2945 - accuracy: 0.9122\n",
      "\n",
      "Epoch 00199: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 199/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 0.2918 - accuracy: 0.9116\n",
      "\n",
      "Epoch 00200: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 200/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2910 - accuracy: 0.9119\n",
      "\n",
      "Epoch 00201: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 201/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2887 - accuracy: 0.9117\n",
      "\n",
      "Epoch 00202: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 202/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2867 - accuracy: 0.9136\n",
      "\n",
      "Epoch 00203: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 203/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2850 - accuracy: 0.9133\n",
      "\n",
      "Epoch 00204: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 204/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 0.2834 - accuracy: 0.9140\n",
      "\n",
      "Epoch 00205: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 205/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2798 - accuracy: 0.9146\n",
      "\n",
      "Epoch 00206: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 206/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2779 - accuracy: 0.9159\n",
      "\n",
      "Epoch 00207: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 207/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2753 - accuracy: 0.9165\n",
      "\n",
      "Epoch 00208: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 208/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2736 - accuracy: 0.9165\n",
      "\n",
      "Epoch 00209: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 209/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2711 - accuracy: 0.9178\n",
      "\n",
      "Epoch 00210: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 210/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.2685 - accuracy: 0.9199\n",
      "\n",
      "Epoch 00211: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 211/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2658 - accuracy: 0.9195\n",
      "\n",
      "Epoch 00212: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 212/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 0.2640 - accuracy: 0.9203\n",
      "\n",
      "Epoch 00213: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 213/300\n",
      "1875/1875 [==============================] - 46s 24ms/step - loss: 0.2616 - accuracy: 0.9204\n",
      "\n",
      "Epoch 00214: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 214/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.2596 - accuracy: 0.9215\n",
      "\n",
      "Epoch 00215: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 215/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 0.2572 - accuracy: 0.9215\n",
      "\n",
      "Epoch 00216: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 216/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 0.2548 - accuracy: 0.9218\n",
      "\n",
      "Epoch 00217: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 217/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2542 - accuracy: 0.9224\n",
      "\n",
      "Epoch 00218: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 218/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2526 - accuracy: 0.9230\n",
      "\n",
      "Epoch 00219: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 219/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2508 - accuracy: 0.9236\n",
      "\n",
      "Epoch 00220: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 220/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.2501 - accuracy: 0.9241\n",
      "\n",
      "Epoch 00221: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 221/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.2473 - accuracy: 0.9255\n",
      "\n",
      "Epoch 00222: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 222/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 0.2464 - accuracy: 0.9246\n",
      "\n",
      "Epoch 00223: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 223/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.2447 - accuracy: 0.9254\n",
      "\n",
      "Epoch 00224: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 224/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 0.2427 - accuracy: 0.9261\n",
      "\n",
      "Epoch 00225: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 225/300\n",
      "1875/1875 [==============================] - 46s 24ms/step - loss: 0.2421 - accuracy: 0.9265\n",
      "\n",
      "Epoch 00226: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 226/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 0.2396 - accuracy: 0.9271\n",
      "\n",
      "Epoch 00227: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 227/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2397 - accuracy: 0.9267\n",
      "\n",
      "Epoch 00228: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 228/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2387 - accuracy: 0.9268\n",
      "\n",
      "Epoch 00229: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 229/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.2365 - accuracy: 0.9278\n",
      "\n",
      "Epoch 00230: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 230/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2359 - accuracy: 0.9275\n",
      "\n",
      "Epoch 00231: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 231/300\n",
      "1875/1875 [==============================] - 46s 24ms/step - loss: 0.2349 - accuracy: 0.9283\n",
      "\n",
      "Epoch 00232: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 232/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 46s 24ms/step - loss: 0.2336 - accuracy: 0.9281\n",
      "\n",
      "Epoch 00233: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 233/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2324 - accuracy: 0.9294\n",
      "\n",
      "Epoch 00234: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 234/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 0.2318 - accuracy: 0.9291\n",
      "\n",
      "Epoch 00235: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 235/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2295 - accuracy: 0.9298\n",
      "\n",
      "Epoch 00236: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 236/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 0.2305 - accuracy: 0.9293\n",
      "\n",
      "Epoch 00237: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 237/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 0.2292 - accuracy: 0.9293\n",
      "\n",
      "Epoch 00238: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 238/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2298 - accuracy: 0.9289\n",
      "\n",
      "Epoch 00239: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 239/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2269 - accuracy: 0.9297\n",
      "\n",
      "Epoch 00240: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 240/300\n",
      "1875/1875 [==============================] - 46s 24ms/step - loss: 0.2267 - accuracy: 0.9310\n",
      "\n",
      "Epoch 00241: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 241/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2257 - accuracy: 0.9305\n",
      "\n",
      "Epoch 00242: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 242/300\n",
      "1875/1875 [==============================] - 45s 24ms/step - loss: 0.2258 - accuracy: 0.9302\n",
      "\n",
      "Epoch 00243: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 243/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2240 - accuracy: 0.9315\n",
      "\n",
      "Epoch 00244: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 244/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 0.2229 - accuracy: 0.9315\n",
      "\n",
      "Epoch 00245: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 245/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 0.2228 - accuracy: 0.9310\n",
      "\n",
      "Epoch 00246: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 246/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2215 - accuracy: 0.9321\n",
      "\n",
      "Epoch 00247: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 247/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.2209 - accuracy: 0.9325\n",
      "\n",
      "Epoch 00248: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 248/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2204 - accuracy: 0.9324\n",
      "\n",
      "Epoch 00249: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 249/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 0.2185 - accuracy: 0.9332\n",
      "\n",
      "Epoch 00250: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 250/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2189 - accuracy: 0.9325\n",
      "\n",
      "Epoch 00251: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 251/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2184 - accuracy: 0.9328\n",
      "\n",
      "Epoch 00252: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 252/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2171 - accuracy: 0.9335\n",
      "\n",
      "Epoch 00253: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 253/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 0.2163 - accuracy: 0.9330\n",
      "\n",
      "Epoch 00254: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 254/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2157 - accuracy: 0.9332\n",
      "\n",
      "Epoch 00255: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 255/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 0.2154 - accuracy: 0.9347\n",
      "\n",
      "Epoch 00256: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 256/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2174 - accuracy: 0.9328\n",
      "\n",
      "Epoch 00257: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 257/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2128 - accuracy: 0.9346\n",
      "\n",
      "Epoch 00258: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 258/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2139 - accuracy: 0.9348\n",
      "\n",
      "Epoch 00259: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 259/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 0.2126 - accuracy: 0.9346\n",
      "\n",
      "Epoch 00260: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 260/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 0.2124 - accuracy: 0.9349\n",
      "\n",
      "Epoch 00261: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 261/300\n",
      "1875/1875 [==============================] - 46s 24ms/step - loss: 0.2108 - accuracy: 0.9353\n",
      "\n",
      "Epoch 00262: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 262/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2106 - accuracy: 0.9356\n",
      "\n",
      "Epoch 00263: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 263/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2098 - accuracy: 0.9348\n",
      "\n",
      "Epoch 00264: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 264/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2089 - accuracy: 0.9358\n",
      "\n",
      "Epoch 00265: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 265/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 0.2082 - accuracy: 0.9361\n",
      "\n",
      "Epoch 00266: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 266/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2081 - accuracy: 0.9359\n",
      "\n",
      "Epoch 00267: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 267/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2079 - accuracy: 0.9364\n",
      "\n",
      "Epoch 00268: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 268/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 0.2067 - accuracy: 0.9361\n",
      "\n",
      "Epoch 00269: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 269/300\n",
      "1875/1875 [==============================] - 45s 24ms/step - loss: 0.2067 - accuracy: 0.9359\n",
      "\n",
      "Epoch 00270: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 270/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2061 - accuracy: 0.9365\n",
      "\n",
      "Epoch 00271: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 271/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 0.2043 - accuracy: 0.9364\n",
      "\n",
      "Epoch 00272: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 272/300\n",
      "1875/1875 [==============================] - 46s 24ms/step - loss: 0.2046 - accuracy: 0.9375\n",
      "\n",
      "Epoch 00273: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 273/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2030 - accuracy: 0.9375\n",
      "\n",
      "Epoch 00274: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 274/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 0.2026 - accuracy: 0.9372\n",
      "\n",
      "Epoch 00275: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 275/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2021 - accuracy: 0.9381\n",
      "\n",
      "Epoch 00276: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 276/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2016 - accuracy: 0.9370\n",
      "\n",
      "Epoch 00277: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 277/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 0.2018 - accuracy: 0.9380\n",
      "\n",
      "Epoch 00278: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 278/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 48s 25ms/step - loss: 0.2008 - accuracy: 0.9385\n",
      "\n",
      "Epoch 00279: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 279/300\n",
      "1875/1875 [==============================] - 46s 24ms/step - loss: 0.2003 - accuracy: 0.9381\n",
      "\n",
      "Epoch 00280: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 280/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.1998 - accuracy: 0.9376\n",
      "\n",
      "Epoch 00281: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 281/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.1986 - accuracy: 0.9386\n",
      "\n",
      "Epoch 00282: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 282/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.1992 - accuracy: 0.9392\n",
      "\n",
      "Epoch 00283: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 283/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.1973 - accuracy: 0.9391\n",
      "\n",
      "Epoch 00284: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 284/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.1975 - accuracy: 0.9393\n",
      "\n",
      "Epoch 00285: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 285/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.1969 - accuracy: 0.9391\n",
      "\n",
      "Epoch 00286: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 286/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.1955 - accuracy: 0.9402\n",
      "\n",
      "Epoch 00287: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 287/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.1950 - accuracy: 0.9401\n",
      "\n",
      "Epoch 00288: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 288/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.1948 - accuracy: 0.9400\n",
      "\n",
      "Epoch 00289: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 289/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.1937 - accuracy: 0.9406\n",
      "\n",
      "Epoch 00290: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 290/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.1924 - accuracy: 0.9409\n",
      "\n",
      "Epoch 00291: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 291/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.1932 - accuracy: 0.9409\n",
      "\n",
      "Epoch 00292: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 292/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.1925 - accuracy: 0.9413\n",
      "\n",
      "Epoch 00293: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 293/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.1914 - accuracy: 0.9409\n",
      "\n",
      "Epoch 00294: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 294/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.1923 - accuracy: 0.9400\n",
      "\n",
      "Epoch 00295: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 295/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 0.1895 - accuracy: 0.9417\n",
      "\n",
      "Epoch 00296: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 296/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.1888 - accuracy: 0.9407\n",
      "\n",
      "Epoch 00297: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 297/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 0.1894 - accuracy: 0.9416\n",
      "\n",
      "Epoch 00298: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 298/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.1882 - accuracy: 0.9415\n",
      "\n",
      "Epoch 00299: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 299/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.1877 - accuracy: 0.9423\n",
      "\n",
      "Epoch 00300: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 300/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.1870 - accuracy: 0.9420\n"
     ]
    }
   ],
   "source": [
    "x_train_2d = x_train[:, :, :, tf.newaxis]\n",
    "\n",
    "def step_decay(epoch):\n",
    "    x = 2e-5\n",
    "    if epoch >= 100:# and epoch <= 200:\n",
    "        x = 2e-4\n",
    "    return x\n",
    "\n",
    "decay = LearningRateScheduler(step_decay, verbose=1)\n",
    "batch = 32\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "tn_model2.compile(optimizer=optimizer, \n",
    "                 loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                 metrics = [\"accuracy\"])\n",
    "history = tn_model2.fit(x_train_2d, y_train, batch_size=batch, shuffle=True, \n",
    "                        steps_per_epoch=int(60000 / batch), epochs=300, \n",
    "                        verbose=1, callbacks=[decay])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_2d = x_test[:, :, :, tf.newaxis]\n",
    "train_loss, train_acc = tn_model2.evaluate(x_train_2d, y_train, verbose=0)\n",
    "test_loss, test_acc = tn_model2.evaluate(x_test_2d, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.189\n",
      "train accuracy:0.941\n",
      "test loss:0.224\n",
      "test accuracy:0.935\n"
     ]
    }
   ],
   "source": [
    "print('train loss:{:.3f}'.format(train_loss))\n",
    "print('train accuracy:{:.3f}'.format(train_acc))\n",
    "print('test loss:{:.3f}'.format(test_loss))\n",
    "print('test accuracy:{:.3f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lr 2e-5 -> 2e-4\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax1.plot(history.epoch, history.history[\"loss\"],\n",
    "         color='r', label=\"Loss\")\n",
    "ax2.plot(history.epoch, history.history[\"accuracy\"], color='b',\n",
    "        label=\"Accuracy\")\n",
    " \n",
    "handler1, label1 = ax1.get_legend_handles_labels()\n",
    "handler2, label2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(handler1 + handler2, label1 + label2, loc=3, borderaxespad=0.)\n",
    "ax1.set_xlabel('Epoch', fontsize=14)\n",
    "ax1.set_ylabel('Loss', fontsize=14)\n",
    "ax2.set_ylabel('Accuracy', fontsize=14)\n",
    " \n",
    "ax1.set_ylim([0, 2.5])\n",
    "ax2.set_ylim([0, 1.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix\n",
    "For checking initialize condition for MPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving a component\n",
    "\n",
    "def block(*dimensions, norm=1):\n",
    "    '''Construct a new matrix for the MPS with random numbers from 0 to 1'''\n",
    "    size = tuple([x for x in dimensions])\n",
    "    return np.random.normal(loc = 0, scale = 0.5, size = size)\n",
    "\n",
    "def create_MPS(rank, dim, bond_dim):\n",
    "    '''Build the MPS tensor'''\n",
    "    mps = [\n",
    "        tn.Node( block(dim, bond_dim) )] + \\\n",
    "        [tn.Node( block(bond_dim, dim, bond_dim)) for _ in range(rank-2)] + \\\n",
    "        [tn.Node( block(bond_dim, dim) )\n",
    "        ]\n",
    "\n",
    "    #connect edges to build mps\n",
    "    connected_edges=[]\n",
    "    conn=mps[0][1]^mps[1][0]\n",
    "    connected_edges.append(conn)\n",
    "    for k in range(1,rank-1):\n",
    "        conn=mps[k][2]^mps[k+1][0]\n",
    "        connected_edges.append(conn)\n",
    "\n",
    "    return mps, connected_edges\n",
    "\n",
    "def create_MPS_labeled(rank, dim, bond_dim):\n",
    "    '''Build the MPS tensor'''\n",
    "    half = np.int((rank - 2) / 2)\n",
    "    norm = 1 / bond_dim\n",
    "    mps = [\n",
    "        tn.Node( block(dim, bond_dim, norm=norm) )] + \\\n",
    "        [tn.Node( block(bond_dim, dim, bond_dim, norm = norm)) for _ in range(half)] + \\\n",
    "        [tn.Node( block(bond_dim, label_dim, bond_dim, norm=norm) )] + \\\n",
    "        [tn.Node( block(bond_dim, dim, bond_dim, norm=norm)) for _ in range(half, rank-2)] + \\\n",
    "        [tn.Node( block(bond_dim, dim, norm=norm) )\n",
    "        ]\n",
    "\n",
    "    #connect edges to build mps\n",
    "    connected_edges=[]\n",
    "    conn=mps[0][1]^mps[1][0]\n",
    "    connected_edges.append(conn)\n",
    "    for k in range(1,rank):\n",
    "        conn=mps[k][2]^mps[k+1][0]\n",
    "        connected_edges.append(conn)\n",
    "\n",
    "    return mps, connected_edges\n",
    "\n",
    "\n",
    "test_vec = x_train[0].flatten()[0:196]\n",
    "data_tensor = data_tensorize(test_vec)\n",
    "\n",
    "label_len = 1\n",
    "label_dim = 10\n",
    "data_len = len(test_vec)\n",
    "rank = data_len\n",
    "dim = 2\n",
    "bond_dim = 5\n",
    "# mps, edges = create_MPS(rank, dim, bond_dim)\n",
    "mps, edges = create_MPS_labeled(rank, dim, bond_dim)\n",
    "\n",
    "\n",
    "edges.append(data_tensor[0][0] ^ mps[0][0])\n",
    "half_len = np.int(len(data_tensor) / 2)\n",
    "[edges.append(data_tensor[i][0] ^ mps[i][1]) for i in range(1, half_len)]\n",
    "[edges.append(data_tensor[i-label_len][0] ^ mps[i][1]) for i in range(half_len + label_len, data_len + label_len)]\n",
    "for k in reversed(range(len(edges))):\n",
    "    A = tn.contract(edges[k])\n",
    "result = A.tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00179872 -0.00415382  0.00202591  0.00064367  0.00126495  0.00123817\n",
      " -0.00200716  0.00046933 -0.00605777 -0.00103179]\n",
      "[-0.00179872 -0.00415382  0.00202591  0.00064367  0.00126495  0.00123817\n",
      " -0.00200716  0.00046933 -0.00605777 -0.00103179]\n",
      "[        nan         nan -6.2017354  -7.34832388 -6.67272408 -6.69411802\n",
      "         nan -7.66420059         nan         nan]\n",
      "[0.09991393 0.0996789  0.1002968  0.10015826 0.1002205  0.10021782\n",
      " 0.09989311 0.1001408  0.0994893  0.09999059]\n"
     ]
    }
   ],
   "source": [
    "print(A.tensor.numpy())\n",
    "print(A.tensor.numpy().astype(\"float32\"))\n",
    "print(tf.math.log(A.tensor).numpy())\n",
    "print(tf.nn.softmax(A.tensor).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
