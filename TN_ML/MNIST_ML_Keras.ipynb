{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "#tf.enable_v2_behavior()\n",
    "# Import tensornetwork\n",
    "import tensornetwork as tn\n",
    "# Set the backend to tesorflow\n",
    "# (default is numpy)\n",
    "tn.set_default_backend(\"tensorflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 0-255の値が入っているので、0-1に収まるよう正規化します\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vec = []\n",
    "for i in range(x_train.shape[0]):\n",
    "    x_vec_ = np.concatenate([x_train[i][j, ::-2*(j%2)+1] for j in range(x_train.shape[1])])\n",
    "    x_vec.append(x_vec_)\n",
    "x_train_1d = np.vstack(x_vec)\n",
    "\n",
    "x_vec = []\n",
    "for i in range(x_test.shape[0]):\n",
    "    x_vec_ = np.concatenate([x_test[i][j, ::-2*(j%2)+1] for j in range(x_test.shape[1])])\n",
    "    x_vec.append(x_vec_)\n",
    "x_test_1d = np.vstack(x_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving a component\n",
    "\n",
    "def block(*dimensions, norm = 1):\n",
    "    '''Construct a new matrix for the MPS with random numbers from 0 to 1'''\n",
    "    size = tuple([x for x in dimensions])\n",
    "    return np.random.normal(loc = 0, scale = 0.34, size = size)\n",
    "\n",
    "def create_MPS(rank, dim, bond_dim):\n",
    "    '''Build the MPS tensor'''\n",
    "    mps = [\n",
    "        tn.Node( block(dim, bond_dim) )] + \\\n",
    "        [tn.Node( block(bond_dim, dim, bond_dim)) for _ in range(rank-2)] + \\\n",
    "        [tn.Node( block(bond_dim, dim) )\n",
    "        ]\n",
    "\n",
    "    #connect edges to build mps\n",
    "    connected_edges=[]\n",
    "    conn=mps[0][1]^mps[1][0]\n",
    "    connected_edges.append(conn)\n",
    "    for k in range(1,rank-1):\n",
    "        conn=mps[k][2]^mps[k+1][0]\n",
    "        connected_edges.append(conn)\n",
    "\n",
    "    return mps, connected_edges\n",
    "\n",
    "def create_MPS_labeled(rank, dim, bond_dim):\n",
    "    '''Build the MPS tensor'''\n",
    "    half = np.int((rank - 2) / 2)\n",
    "    norm = 1 / bond_dim\n",
    "    mps = [\n",
    "        tn.Node( block(dim, bond_dim, norm=norm) )] + \\\n",
    "        [tn.Node( block(bond_dim, dim, bond_dim, norm = norm)) for _ in range(half)] + \\\n",
    "        [tn.Node( block(bond_dim, label_dim, bond_dim, norm=norm) )] + \\\n",
    "        [tn.Node( block(bond_dim, dim, bond_dim, norm=norm)) for _ in range(half, rank-2)] + \\\n",
    "        [tn.Node( block(bond_dim, dim, norm=norm) )\n",
    "        ]\n",
    "\n",
    "    #connect edges to build mps\n",
    "    connected_edges=[]\n",
    "    conn=mps[0][1]^mps[1][0]\n",
    "    connected_edges.append(conn)\n",
    "    for k in range(1,rank):\n",
    "        conn=mps[k][2]^mps[k+1][0]\n",
    "        connected_edges.append(conn)\n",
    "\n",
    "    return mps, connected_edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_map(p):\n",
    "    phi = [1-p, p]\n",
    "    return phi\n",
    "\n",
    "def data_tensorize(vec):\n",
    "    data_tensor = [tn.Node(feature_map(p)) for p in vec]\n",
    "    return data_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_len = 1\n",
    "label_dim = 10\n",
    "data_len = x_train_1d.shape[1]\n",
    "rank = data_len\n",
    "dim = 2\n",
    "bond_dim = 10\n",
    "# mps, edges = create_MPS(rank, dim, bond_dim)\n",
    "mps, edges = create_MPS_labeled(rank, dim, bond_dim)\n",
    "\n",
    "test_vec = x_train_1d[0]\n",
    "data_tensor = data_tensorize(test_vec)\n",
    "\n",
    "edges.append(data_tensor[0][0] ^ mps[0][0])\n",
    "half_len = np.int(len(data_tensor) / 2)\n",
    "[edges.append(data_tensor[i][0] ^ mps[i][1]) for i in range(1, half_len)]\n",
    "[edges.append(data_tensor[i-label_len][0] ^ mps[i][1]) for i in range(half_len + label_len, data_len + label_len)]\n",
    "for k in reversed(range(len(edges))):\n",
    "    A = tn.contract(edges[k])\n",
    "result = A.tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.1258935  -0.18227812 -0.9654826  -0.68427782  0.12575317 -0.22343786\n",
      " -0.29122844 -0.96080808  0.03186352  0.83127648]\n",
      "[-0.1258935  -0.18227813 -0.9654826  -0.68427783  0.12575316 -0.22343786\n",
      " -0.29122844 -0.9608081   0.03186351  0.8312765 ]\n",
      "[        nan         nan         nan         nan -2.07343429         nan\n",
      "         nan         nan -3.44629364 -0.18479284]\n",
      "[0.09804769 0.09267228 0.04234562 0.05609629 0.1261032  0.08893534\n",
      " 0.08310618 0.04254402 0.11480224 0.25534714]\n"
     ]
    }
   ],
   "source": [
    "print(A.tensor.numpy())\n",
    "print(A.tensor.numpy().astype(\"float32\"))\n",
    "print(tf.math.log(A.tensor).numpy())\n",
    "print(tf.nn.softmax(A.tensor).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block(*dimensions):\n",
    "    '''Construct a new matrix for the MPS with random numbers from 0 to 1'''\n",
    "    size = tuple([x for x in dimensions])\n",
    "    return tf.Variable(\n",
    "        tf.random.normal(shape=size, dtype=tf.dtypes.float64, mean= 0, stddev = 0.34),\n",
    "        trainable=True)\n",
    "\n",
    "def create_blocks(rank, dim, bond_dim, label_dim):\n",
    "    half = np.int((rank - 2) / 2)\n",
    "    blocks = [\n",
    "        block(dim, bond_dim) ] + \\\n",
    "        [ block(bond_dim, dim, bond_dim) for _ in range(half)] + \\\n",
    "        [ block(bond_dim, label_dim, bond_dim) ] + \\\n",
    "        [ block(bond_dim, dim, bond_dim) for _ in range(half, rank-2)] + \\\n",
    "        [ block(bond_dim, dim) \n",
    "        ]\n",
    "    return blocks\n",
    "\n",
    "def create_MPS_labeled(blocks, rank, dim, bond_dim):\n",
    "    '''Build the MPS tensor'''\n",
    "    half = np.int((rank - 2) / 2)\n",
    "    mps = []\n",
    "    for b in blocks:\n",
    "        mps.append(tn.Node(b))\n",
    "\n",
    "    #connect edges to build mps\n",
    "    connected_edges=[]\n",
    "    conn=mps[0][1]^mps[1][0]\n",
    "    connected_edges.append(conn)\n",
    "    for k in range(1,rank):\n",
    "        conn=mps[k][2]^mps[k+1][0]\n",
    "        connected_edges.append(conn)\n",
    "\n",
    "    return mps, connected_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TNLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_len, label_num, bond_dim):\n",
    "        self.label_len = 1\n",
    "        self.label_dim = label_num\n",
    "        self.rank = input_len\n",
    "        self.dim = 2\n",
    "        self.bond_dim = bond_dim\n",
    "        #super(TNLayer, self).__init__()\n",
    "        super().__init__()\n",
    "        # Create the variables for the layer.\n",
    "        self.blocks = create_blocks(self.rank, self.dim, self.bond_dim, self.label_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        def f(input_vec, blocks, rank, dim, bond_dim, label_len):\n",
    "            mps, edges = create_MPS_labeled(blocks, rank, dim, bond_dim)\n",
    "            data_tensor = []\n",
    "            for p in tf.unstack(input_vec):\n",
    "                data_tensor.append(tn.Node([1-p, p]))\n",
    "            edges.append(data_tensor[0][0] ^ mps[0][0])\n",
    "            half_len = np.int(rank / 2)\n",
    "            [edges.append(data_tensor[i][0] ^ mps[i][1]) for i in range(1, half_len)]\n",
    "            [edges.append(data_tensor[i-label_len][0] ^ mps[i][1]) \\\n",
    "                 for i in range(half_len + label_len, rank + label_len)]\n",
    "            for k in reversed(range(len(edges))):\n",
    "                A = tn.contract(edges[k])\n",
    "            #result = tf.math.log(A.tensor)\n",
    "            result = A.tensor - tf.math.reduce_max(A.tensor)\n",
    "            return result\n",
    "\n",
    "        result = tf.vectorized_map(\n",
    "        lambda vec: f(vec, self.blocks, self.rank, self.dim, self.bond_dim, self.label_len), inputs)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "tn_layer (TNLayer)           (None, 10)                157440    \n",
      "_________________________________________________________________\n",
      "softmax (Softmax)            (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 157,440\n",
      "Trainable params: 157,440\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "N = x_train_1d.shape[1]\n",
    "label_len = 1\n",
    "label_num = 10\n",
    "data_len = x_train_1d.shape[1]\n",
    "rank = data_len\n",
    "dim = 2\n",
    "bond_dim = 10\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "tn_model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(N,)),\n",
    "        TNLayer(N, label_num, bond_dim),\n",
    "        tf.keras.layers.Softmax()\n",
    "    ])\n",
    "\n",
    "tn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 1/300\n",
      "1875/1875 [==============================] - 179s 95ms/step - loss: 624064.2729 - accuracy: 0.0991\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 2/300\n",
      "1875/1875 [==============================] - 174s 93ms/step - loss: 79956.6907 - accuracy: 0.1021\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 3/300\n",
      "1875/1875 [==============================] - 176s 94ms/step - loss: 1160.5092 - accuracy: 0.10290s - loss: 1163.8058 - accuracy:\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 4/300\n",
      "1875/1875 [==============================] - 177s 95ms/step - loss: 649.7942 - accuracy: 0.1035\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 5/300\n",
      "1875/1875 [==============================] - 166s 89ms/step - loss: 492.8239 - accuracy: 0.1045\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 6/300\n",
      "1875/1875 [==============================] - 162s 86ms/step - loss: 605.9958 - accuracy: 0.1026\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 7/300\n",
      "1875/1875 [==============================] - 163s 87ms/step - loss: 434.7295 - accuracy: 0.1036\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 8/300\n",
      "1875/1875 [==============================] - 163s 87ms/step - loss: 202.0236 - accuracy: 0.1038\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 9/300\n",
      "1875/1875 [==============================] - 167s 89ms/step - loss: 584.6263 - accuracy: 0.1036\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 10/300\n",
      "1875/1875 [==============================] - 165s 88ms/step - loss: 116.3895 - accuracy: 0.1051\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 11/300\n",
      "1875/1875 [==============================] - 162s 86ms/step - loss: 15.8140 - accuracy: 0.1062\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 12/300\n",
      "1875/1875 [==============================] - 165s 88ms/step - loss: 4.5752 - accuracy: 0.1078\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 13/300\n",
      "1875/1875 [==============================] - 163s 87ms/step - loss: 3.4007 - accuracy: 0.1079\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 14/300\n",
      "1875/1875 [==============================] - 165s 88ms/step - loss: 2.7693 - accuracy: 0.1084\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 15/300\n",
      "1875/1875 [==============================] - 166s 89ms/step - loss: 2.5003 - accuracy: 0.1095\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 16/300\n",
      "1875/1875 [==============================] - 167s 89ms/step - loss: 2.4672 - accuracy: 0.1123\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 17/300\n",
      "1875/1875 [==============================] - 166s 88ms/step - loss: 2.4284 - accuracy: 0.1140\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 18/300\n",
      "1875/1875 [==============================] - 161s 86ms/step - loss: 2.3434 - accuracy: 0.1137\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 19/300\n",
      "1875/1875 [==============================] - 164s 87ms/step - loss: 2.3141 - accuracy: 0.1137\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 20/300\n",
      "1875/1875 [==============================] - 163s 87ms/step - loss: 2.3121 - accuracy: 0.1151\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 21/300\n",
      "1875/1875 [==============================] - 165s 88ms/step - loss: 2.3084 - accuracy: 0.1158\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 22/300\n",
      "1875/1875 [==============================] - 166s 89ms/step - loss: 2.3139 - accuracy: 0.1163\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 23/300\n",
      "1875/1875 [==============================] - 162s 86ms/step - loss: 2.3035 - accuracy: 0.1147\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 24/300\n",
      "1875/1875 [==============================] - 164s 88ms/step - loss: 2.3039 - accuracy: 0.1161\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 25/300\n",
      "1875/1875 [==============================] - 164s 87ms/step - loss: 2.3027 - accuracy: 0.1140\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 26/300\n",
      "1875/1875 [==============================] - 165s 88ms/step - loss: 2.3046 - accuracy: 0.1135\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 27/300\n",
      "1875/1875 [==============================] - 166s 89ms/step - loss: 2.3027 - accuracy: 0.1143\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 28/300\n",
      "1875/1875 [==============================] - 166s 88ms/step - loss: 2.3028 - accuracy: 0.1187\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 29/300\n",
      "1875/1875 [==============================] - 163s 87ms/step - loss: 2.3021 - accuracy: 0.1242\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 30/300\n",
      "1875/1875 [==============================] - 165s 88ms/step - loss: 2.3025 - accuracy: 0.1255\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 31/300\n",
      "1875/1875 [==============================] - 167s 89ms/step - loss: 2.3028 - accuracy: 0.1245\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 32/300\n",
      "1875/1875 [==============================] - 166s 89ms/step - loss: 2.3020 - accuracy: 0.1224\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 33/300\n",
      "1875/1875 [==============================] - 164s 87ms/step - loss: 2.3033 - accuracy: 0.1240\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 34/300\n",
      "1875/1875 [==============================] - 166s 88ms/step - loss: 2.3020 - accuracy: 0.1229\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 35/300\n",
      "1875/1875 [==============================] - 163s 87ms/step - loss: 2.3018 - accuracy: 0.1247\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 36/300\n",
      "1875/1875 [==============================] - 164s 87ms/step - loss: 2.3019 - accuracy: 0.1235\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 37/300\n",
      "1875/1875 [==============================] - 164s 87ms/step - loss: 2.3014 - accuracy: 0.1240\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 38/300\n",
      "1875/1875 [==============================] - 167s 89ms/step - loss: 2.3030 - accuracy: 0.1224\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 39/300\n",
      "1875/1875 [==============================] - 166s 88ms/step - loss: 2.3017 - accuracy: 0.1240\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 40/300\n",
      "1875/1875 [==============================] - 163s 87ms/step - loss: 2.3014 - accuracy: 0.1251\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 41/300\n",
      "1875/1875 [==============================] - 162s 86ms/step - loss: 2.3017 - accuracy: 0.1283\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 42/300\n",
      "1875/1875 [==============================] - 166s 89ms/step - loss: 2.3017 - accuracy: 0.1294\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 43/300\n",
      "1875/1875 [==============================] - 164s 87ms/step - loss: 2.3011 - accuracy: 0.1288\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 44/300\n",
      "1875/1875 [==============================] - 166s 88ms/step - loss: 2.2993 - accuracy: 0.1299\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 45/300\n",
      "1875/1875 [==============================] - 166s 88ms/step - loss: 2.2998 - accuracy: 0.1319\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 46/300\n",
      "1875/1875 [==============================] - 164s 87ms/step - loss: 2.3105 - accuracy: 0.1278\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 47/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 166s 89ms/step - loss: 2.3003 - accuracy: 0.1268\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 48/300\n",
      "1875/1875 [==============================] - 166s 89ms/step - loss: 2.2997 - accuracy: 0.1304\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 49/300\n",
      "1875/1875 [==============================] - 165s 88ms/step - loss: 2.2983 - accuracy: 0.1321\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 50/300\n",
      "1875/1875 [==============================] - 167s 89ms/step - loss: 2.2977 - accuracy: 0.1339\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 51/300\n",
      "1875/1875 [==============================] - 163s 87ms/step - loss: 2.2990 - accuracy: 0.1344\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 52/300\n",
      "1875/1875 [==============================] - 167s 89ms/step - loss: 2.2974 - accuracy: 0.1349\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 53/300\n",
      " 103/1875 [>.............................] - ETA: 2:36 - loss: 2.2986 - accuracy: 0.1390"
     ]
    }
   ],
   "source": [
    "def step_decay(epoch):\n",
    "    x = 1e-5\n",
    "    if epoch >= 100 and epoch <= 200:\n",
    "        x = 1e-4\n",
    "    return x\n",
    "\n",
    "decay = LearningRateScheduler(step_decay, verbose=1)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "tn_model.compile(optimizer=optimizer, \n",
    "                 loss=tf.keras.losses.SparseCategoricalCrossentropy(), \n",
    "                 metrics=['accuracy'])\n",
    "batch = 32\n",
    "tn_model.fit(x_train_1d, y_train, \n",
    "             batch_size=batch, epochs=300, \n",
    "             verbose=1, shuffle=True, \n",
    "             steps_per_epoch=int(60000 / batch), callbacks=[decay])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "average_pooling2d_4 (Average (None, 14, 14, 1)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 196)               0         \n",
      "_________________________________________________________________\n",
      "tn_layer_4 (TNLayer)         (None, 10)                9970      \n",
      "_________________________________________________________________\n",
      "softmax_4 (Softmax)          (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 9,970\n",
      "Trainable params: 9,970\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "label_len = 1\n",
    "label_num = 10\n",
    "dim = 2\n",
    "bond_dim = 5\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "tn_model2 = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(28, 28, 1)),\n",
    "        tf.keras.layers.AveragePooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        TNLayer(196, label_num, bond_dim),\n",
    "        tf.keras.layers.Softmax()\n",
    "    ],name=\"sequential_1\")\n",
    "\n",
    "tn_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 1/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 2.1696 - accuracy: 0.1044\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 2/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0331 - accuracy: 0.1045\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 3/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 1.9991 - accuracy: 0.10450s\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 4/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.9835 - accuracy: 0.1045\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 5/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.9742 - accuracy: 0.1045\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 6/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.9680 - accuracy: 0.1047\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 7/300\n",
      "1875/1875 [==============================] - 50s 26ms/step - loss: 1.9633 - accuracy: 0.1048\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 8/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.9594 - accuracy: 0.1049\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 9/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.9564 - accuracy: 0.1050\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 10/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.9538 - accuracy: 0.1052\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 11/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.9513 - accuracy: 0.1052\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 12/300\n",
      "1875/1875 [==============================] - 50s 26ms/step - loss: 1.9488 - accuracy: 0.1055\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 13/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.9469 - accuracy: 0.1058\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 14/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.9449 - accuracy: 0.1060\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 15/300\n",
      "1875/1875 [==============================] - 50s 26ms/step - loss: 1.9432 - accuracy: 0.1062\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 16/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.9418 - accuracy: 0.1064\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 17/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.9403 - accuracy: 0.1071\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 18/300\n",
      "1875/1875 [==============================] - 52s 27ms/step - loss: 1.9383 - accuracy: 0.1067\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 19/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.9369 - accuracy: 0.1074\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 20/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.9352 - accuracy: 0.1082\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 21/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.9342 - accuracy: 0.1078\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 22/300\n",
      "1875/1875 [==============================] - 52s 27ms/step - loss: 1.9328 - accuracy: 0.1091\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 23/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.9309 - accuracy: 0.1110\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 24/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.9294 - accuracy: 0.1393\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 25/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.9283 - accuracy: 0.1424\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 26/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.9268 - accuracy: 0.1431\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 27/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.9256 - accuracy: 0.1460\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 28/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.9243 - accuracy: 0.1478\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 29/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.9231 - accuracy: 0.1504\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 30/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.9213 - accuracy: 0.1524\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 31/300\n",
      "1875/1875 [==============================] - 52s 28ms/step - loss: 1.9206 - accuracy: 0.1536\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 32/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.9188 - accuracy: 0.1562\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 33/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.9172 - accuracy: 0.1585\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 34/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.9158 - accuracy: 0.1619\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 35/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.9138 - accuracy: 0.1622\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 36/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.9120 - accuracy: 0.1670\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 37/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.9087 - accuracy: 0.1701\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 38/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.9054 - accuracy: 0.1739\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 39/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.9008 - accuracy: 0.1812\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 40/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.8946 - accuracy: 0.1841\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 41/300\n",
      "1875/1875 [==============================] - 50s 26ms/step - loss: 1.8869 - accuracy: 0.1935\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 42/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 1.8768 - accuracy: 0.2000\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 43/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.8653 - accuracy: 0.2091\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 44/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.8531 - accuracy: 0.2145\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 45/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.8419 - accuracy: 0.2180\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 46/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 1.8308 - accuracy: 0.2207\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 47/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.8168 - accuracy: 0.2350\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 48/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.7928 - accuracy: 0.3041\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 49/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 1.7563 - accuracy: 0.3230\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 50/300\n",
      "1875/1875 [==============================] - 50s 26ms/step - loss: 1.7351 - accuracy: 0.3261\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 51/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.7251 - accuracy: 0.3288\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 52/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.7176 - accuracy: 0.3312\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 53/300\n",
      "1875/1875 [==============================] - 52s 28ms/step - loss: 1.7116 - accuracy: 0.3347\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 54/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.7053 - accuracy: 0.3377\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 55/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.6994 - accuracy: 0.3398\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 56/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 1.6939 - accuracy: 0.3407\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 57/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.6882 - accuracy: 0.3427\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 58/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.6834 - accuracy: 0.3439\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 59/300\n",
      "1875/1875 [==============================] - 52s 28ms/step - loss: 1.6784 - accuracy: 0.3452\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 60/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.6736 - accuracy: 0.3456\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 61/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.6693 - accuracy: 0.3462\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 62/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.6650 - accuracy: 0.3473\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 63/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.6613 - accuracy: 0.3479\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 64/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.6577 - accuracy: 0.3484\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 65/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.6539 - accuracy: 0.3495\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 66/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.6501 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 67/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.6471 - accuracy: 0.3502\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 68/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.6439 - accuracy: 0.3503\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 69/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.6402 - accuracy: 0.3528\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 70/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.6371 - accuracy: 0.3520\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 71/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.6336 - accuracy: 0.3544\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 72/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.6300 - accuracy: 0.3551\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 73/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.6272 - accuracy: 0.3568\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 74/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.6232 - accuracy: 0.3573\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 75/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.6203 - accuracy: 0.3584\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 76/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.6170 - accuracy: 0.3595\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 77/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.6136 - accuracy: 0.3613\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 78/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.6107 - accuracy: 0.3622\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 79/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.6076 - accuracy: 0.3632\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 80/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.6044 - accuracy: 0.3643\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 81/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.6014 - accuracy: 0.3657\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 82/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.5978 - accuracy: 0.3669\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 83/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.5941 - accuracy: 0.3684\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 84/300\n",
      "1875/1875 [==============================] - 52s 28ms/step - loss: 1.5904 - accuracy: 0.3700\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 85/300\n",
      "1875/1875 [==============================] - 52s 28ms/step - loss: 1.5863 - accuracy: 0.3716\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 86/300\n",
      "1875/1875 [==============================] - 50s 26ms/step - loss: 1.5819 - accuracy: 0.3734\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 87/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.5775 - accuracy: 0.3749\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 88/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.5739 - accuracy: 0.3752\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 89/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 1.5697 - accuracy: 0.3766\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 90/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.5664 - accuracy: 0.3773\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 91/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 1.5629 - accuracy: 0.3782\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 92/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 1.5598 - accuracy: 0.3789\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 93/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.5576 - accuracy: 0.3785\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 94/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.5549 - accuracy: 0.3805\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 95/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.5528 - accuracy: 0.3802\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 96/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.5508 - accuracy: 0.3816\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 97/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.5489 - accuracy: 0.3814\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 98/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.5469 - accuracy: 0.3817\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 99/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.5456 - accuracy: 0.3816\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 100/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.5439 - accuracy: 0.3819\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 101/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.5521 - accuracy: 0.3795\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 102/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.5402 - accuracy: 0.3825\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 103/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.5362 - accuracy: 0.3852\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 104/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.5301 - accuracy: 0.3863\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 105/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.5255 - accuracy: 0.3878\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 106/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.5174 - accuracy: 0.3885\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 107/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.5091 - accuracy: 0.3905\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 108/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.4839 - accuracy: 0.4194\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 109/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.4401 - accuracy: 0.4626\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 110/300\n",
      "1875/1875 [==============================] - 50s 26ms/step - loss: 1.3787 - accuracy: 0.5008\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 111/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.3288 - accuracy: 0.5220\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 112/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.2902 - accuracy: 0.5490\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 113/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.2679 - accuracy: 0.5582\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 114/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.2590 - accuracy: 0.5620\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 115/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.2499 - accuracy: 0.5625\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 116/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.2450 - accuracy: 0.5641\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 117/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.2391 - accuracy: 0.5662\n",
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 118/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.2340 - accuracy: 0.5690\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 119/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.2290 - accuracy: 0.5699\n",
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 120/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.2253 - accuracy: 0.5711\n",
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 121/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.2218 - accuracy: 0.5711\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 122/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.2171 - accuracy: 0.5722\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 123/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.2141 - accuracy: 0.5733\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 124/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.2106 - accuracy: 0.5737\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 125/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.2053 - accuracy: 0.5748\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 126/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.2004 - accuracy: 0.5769\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 127/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.1988 - accuracy: 0.5793\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 128/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.1901 - accuracy: 0.5831\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 129/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.1821 - accuracy: 0.5923\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 130/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.1705 - accuracy: 0.6032\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 131/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.1622 - accuracy: 0.6079\n",
      "\n",
      "Epoch 00132: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 132/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 1.1538 - accuracy: 0.6106\n",
      "\n",
      "Epoch 00133: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 133/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.1470 - accuracy: 0.6127\n",
      "\n",
      "Epoch 00134: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 134/300\n",
      "1875/1875 [==============================] - 52s 28ms/step - loss: 1.1419 - accuracy: 0.6151\n",
      "\n",
      "Epoch 00135: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 135/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.1347 - accuracy: 0.6179\n",
      "\n",
      "Epoch 00136: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 136/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.1266 - accuracy: 0.6236\n",
      "\n",
      "Epoch 00137: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 137/300\n",
      "1875/1875 [==============================] - 50s 26ms/step - loss: 1.1156 - accuracy: 0.6324\n",
      "\n",
      "Epoch 00138: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 138/300\n",
      "1875/1875 [==============================] - 50s 26ms/step - loss: 1.0963 - accuracy: 0.6403\n",
      "\n",
      "Epoch 00139: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 139/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.0708 - accuracy: 0.6468\n",
      "\n",
      "Epoch 00140: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 140/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.0499 - accuracy: 0.6491\n",
      "\n",
      "Epoch 00141: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 141/300\n",
      "1875/1875 [==============================] - 50s 26ms/step - loss: 1.0348 - accuracy: 0.6523\n",
      "\n",
      "Epoch 00142: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 142/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.0246 - accuracy: 0.6542\n",
      "\n",
      "Epoch 00143: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 143/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.0177 - accuracy: 0.6563\n",
      "\n",
      "Epoch 00144: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 144/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.0123 - accuracy: 0.6593\n",
      "\n",
      "Epoch 00145: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 145/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.0071 - accuracy: 0.6616\n",
      "\n",
      "Epoch 00146: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 146/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.0033 - accuracy: 0.6637\n",
      "\n",
      "Epoch 00147: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 147/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.0001 - accuracy: 0.6654\n",
      "\n",
      "Epoch 00148: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 148/300\n",
      "1875/1875 [==============================] - 52s 28ms/step - loss: 0.9986 - accuracy: 0.6657\n",
      "\n",
      "Epoch 00149: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 149/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.9954 - accuracy: 0.6678\n",
      "\n",
      "Epoch 00150: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 150/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.9940 - accuracy: 0.6671\n",
      "\n",
      "Epoch 00151: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 151/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.9934 - accuracy: 0.6693\n",
      "\n",
      "Epoch 00152: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 152/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.9887 - accuracy: 0.6702\n",
      "\n",
      "Epoch 00153: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 153/300\n",
      "1875/1875 [==============================] - 52s 28ms/step - loss: 0.9883 - accuracy: 0.6710\n",
      "\n",
      "Epoch 00154: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 154/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.9865 - accuracy: 0.6715\n",
      "\n",
      "Epoch 00155: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 155/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.9820 - accuracy: 0.6725\n",
      "\n",
      "Epoch 00156: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 156/300\n",
      "1875/1875 [==============================] - 52s 28ms/step - loss: 0.9793 - accuracy: 0.6738\n",
      "\n",
      "Epoch 00157: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 157/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.9763 - accuracy: 0.6750\n",
      "\n",
      "Epoch 00158: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 158/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.9730 - accuracy: 0.6772\n",
      "\n",
      "Epoch 00159: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 159/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.9703 - accuracy: 0.6767\n",
      "\n",
      "Epoch 00160: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 160/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.9680 - accuracy: 0.6793\n",
      "\n",
      "Epoch 00161: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 161/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.9652 - accuracy: 0.6802\n",
      "\n",
      "Epoch 00162: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 162/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.9620 - accuracy: 0.6812\n",
      "\n",
      "Epoch 00163: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 163/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.9578 - accuracy: 0.6835\n",
      "\n",
      "Epoch 00164: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 164/300\n",
      "1875/1875 [==============================] - 52s 28ms/step - loss: 0.9541 - accuracy: 0.6845\n",
      "\n",
      "Epoch 00165: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 165/300\n",
      "1875/1875 [==============================] - 52s 27ms/step - loss: 0.9508 - accuracy: 0.6864\n",
      "\n",
      "Epoch 00166: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 166/300\n",
      "1875/1875 [==============================] - 52s 28ms/step - loss: 0.9473 - accuracy: 0.6874\n",
      "\n",
      "Epoch 00167: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 167/300\n",
      "1875/1875 [==============================] - 53s 28ms/step - loss: 0.9453 - accuracy: 0.6895\n",
      "\n",
      "Epoch 00168: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 168/300\n",
      "1875/1875 [==============================] - 53s 28ms/step - loss: 0.9417 - accuracy: 0.6915\n",
      "\n",
      "Epoch 00169: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 169/300\n",
      "1875/1875 [==============================] - 52s 28ms/step - loss: 0.9391 - accuracy: 0.6925\n",
      "\n",
      "Epoch 00170: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 170/300\n",
      "1875/1875 [==============================] - 52s 28ms/step - loss: 0.9360 - accuracy: 0.6939\n",
      "\n",
      "Epoch 00171: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 171/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.9338 - accuracy: 0.6956\n",
      "\n",
      "Epoch 00172: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 172/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.9310 - accuracy: 0.6976\n",
      "\n",
      "Epoch 00173: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 173/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.9294 - accuracy: 0.6989\n",
      "\n",
      "Epoch 00174: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 174/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.9279 - accuracy: 0.6993\n",
      "\n",
      "Epoch 00175: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 175/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.9258 - accuracy: 0.7009\n",
      "\n",
      "Epoch 00176: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 176/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.9232 - accuracy: 0.7005\n",
      "\n",
      "Epoch 00177: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 177/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.9216 - accuracy: 0.7016\n",
      "\n",
      "Epoch 00178: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 178/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.9198 - accuracy: 0.7017\n",
      "\n",
      "Epoch 00179: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 179/300\n",
      "1875/1875 [==============================] - 50s 26ms/step - loss: 0.9193 - accuracy: 0.7032\n",
      "\n",
      "Epoch 00180: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 180/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.9178 - accuracy: 0.70310s\n",
      "\n",
      "Epoch 00181: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 181/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.9157 - accuracy: 0.7034\n",
      "\n",
      "Epoch 00182: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 182/300\n",
      "1875/1875 [==============================] - 50s 26ms/step - loss: 0.9159 - accuracy: 0.7034\n",
      "\n",
      "Epoch 00183: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 183/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.9139 - accuracy: 0.7037\n",
      "\n",
      "Epoch 00184: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 184/300\n",
      "1875/1875 [==============================] - 50s 26ms/step - loss: 0.9121 - accuracy: 0.7037\n",
      "\n",
      "Epoch 00185: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 185/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.9111 - accuracy: 0.7050\n",
      "\n",
      "Epoch 00186: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 186/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.9097 - accuracy: 0.7054\n",
      "\n",
      "Epoch 00187: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 187/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.9088 - accuracy: 0.7040\n",
      "\n",
      "Epoch 00188: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 188/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.9077 - accuracy: 0.7058\n",
      "\n",
      "Epoch 00189: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 189/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.9058 - accuracy: 0.7051\n",
      "\n",
      "Epoch 00190: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 190/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.9060 - accuracy: 0.7063\n",
      "\n",
      "Epoch 00191: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 191/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.9041 - accuracy: 0.7059\n",
      "\n",
      "Epoch 00192: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 192/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.9036 - accuracy: 0.70641s\n",
      "\n",
      "Epoch 00193: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 193/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.9030 - accuracy: 0.7078\n",
      "\n",
      "Epoch 00194: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 194/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.9010 - accuracy: 0.7072\n",
      "\n",
      "Epoch 00195: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 195/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.9005 - accuracy: 0.7072\n",
      "\n",
      "Epoch 00196: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 196/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.8991 - accuracy: 0.7064\n",
      "\n",
      "Epoch 00197: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 197/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.8978 - accuracy: 0.7077\n",
      "\n",
      "Epoch 00198: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 198/300\n",
      "1875/1875 [==============================] - 52s 28ms/step - loss: 0.8981 - accuracy: 0.7068\n",
      "\n",
      "Epoch 00199: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 199/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.8953 - accuracy: 0.7081\n",
      "\n",
      "Epoch 00200: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 200/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.8952 - accuracy: 0.7081\n",
      "\n",
      "Epoch 00201: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 201/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.8937 - accuracy: 0.7095\n",
      "\n",
      "Epoch 00202: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 202/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.8811 - accuracy: 0.7104\n",
      "\n",
      "Epoch 00203: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 203/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.8799 - accuracy: 0.7111\n",
      "\n",
      "Epoch 00204: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 204/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.8796 - accuracy: 0.7115\n",
      "\n",
      "Epoch 00205: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 205/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.8793 - accuracy: 0.7110\n",
      "\n",
      "Epoch 00206: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 206/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.8790 - accuracy: 0.7111\n",
      "\n",
      "Epoch 00207: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 207/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.8792 - accuracy: 0.7116\n",
      "\n",
      "Epoch 00208: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 208/300\n",
      "1875/1875 [==============================] - 50s 26ms/step - loss: 0.8790 - accuracy: 0.7114\n",
      "\n",
      "Epoch 00209: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 209/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.8791 - accuracy: 0.7111\n",
      "\n",
      "Epoch 00210: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 210/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.8788 - accuracy: 0.7113\n",
      "\n",
      "Epoch 00211: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 211/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.8788 - accuracy: 0.7118\n",
      "\n",
      "Epoch 00212: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 212/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.8784 - accuracy: 0.7115\n",
      "\n",
      "Epoch 00213: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 213/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.8787 - accuracy: 0.7111\n",
      "\n",
      "Epoch 00214: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 214/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.8784 - accuracy: 0.7117\n",
      "\n",
      "Epoch 00215: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 215/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.8781 - accuracy: 0.7119\n",
      "\n",
      "Epoch 00216: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 216/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.8781 - accuracy: 0.7115\n",
      "\n",
      "Epoch 00217: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 217/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.8780 - accuracy: 0.7117\n",
      "\n",
      "Epoch 00218: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 218/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.8782 - accuracy: 0.7116\n",
      "\n",
      "Epoch 00219: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 219/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.8782 - accuracy: 0.7116\n",
      "\n",
      "Epoch 00220: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 220/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.8779 - accuracy: 0.7117\n",
      "\n",
      "Epoch 00221: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 221/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.8775 - accuracy: 0.7120\n",
      "\n",
      "Epoch 00222: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 222/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.8778 - accuracy: 0.7119\n",
      "\n",
      "Epoch 00223: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 223/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.8776 - accuracy: 0.7123\n",
      "\n",
      "Epoch 00224: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 224/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.8774 - accuracy: 0.7122\n",
      "\n",
      "Epoch 00225: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 225/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.8775 - accuracy: 0.7119\n",
      "\n",
      "Epoch 00226: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 226/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.8773 - accuracy: 0.7127\n",
      "\n",
      "Epoch 00227: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 227/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.8774 - accuracy: 0.7123\n",
      "\n",
      "Epoch 00228: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 228/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.8772 - accuracy: 0.7121\n",
      "\n",
      "Epoch 00229: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 229/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.8771 - accuracy: 0.7126\n",
      "\n",
      "Epoch 00230: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 230/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.8770 - accuracy: 0.7124\n",
      "\n",
      "Epoch 00231: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 231/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.8768 - accuracy: 0.7122\n",
      "\n",
      "Epoch 00232: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 232/300\n",
      "1875/1875 [==============================] - 50s 26ms/step - loss: 0.8769 - accuracy: 0.7125\n",
      "\n",
      "Epoch 00233: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 233/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 0.8768 - accuracy: 0.7123\n",
      "\n",
      "Epoch 00234: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 234/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.8766 - accuracy: 0.7123\n",
      "\n",
      "Epoch 00235: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 235/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.8766 - accuracy: 0.7125\n",
      "\n",
      "Epoch 00236: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 236/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.8763 - accuracy: 0.7128\n",
      "\n",
      "Epoch 00237: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 237/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.8764 - accuracy: 0.7126\n",
      "\n",
      "Epoch 00238: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 238/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.8764 - accuracy: 0.7125\n",
      "\n",
      "Epoch 00239: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 239/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.8763 - accuracy: 0.7126\n",
      "\n",
      "Epoch 00240: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 240/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.8761 - accuracy: 0.7129\n",
      "\n",
      "Epoch 00241: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 241/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.8760 - accuracy: 0.7124\n",
      "\n",
      "Epoch 00242: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 242/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.8759 - accuracy: 0.7125\n",
      "\n",
      "Epoch 00243: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 243/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.8760 - accuracy: 0.7126\n",
      "\n",
      "Epoch 00244: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 244/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.8757 - accuracy: 0.7127\n",
      "\n",
      "Epoch 00245: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 245/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.8758 - accuracy: 0.7126\n",
      "\n",
      "Epoch 00246: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 246/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.8756 - accuracy: 0.7127\n",
      "\n",
      "Epoch 00247: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 247/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.8755 - accuracy: 0.7130\n",
      "\n",
      "Epoch 00248: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 248/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.8756 - accuracy: 0.7131\n",
      "\n",
      "Epoch 00249: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 249/300\n",
      "1875/1875 [==============================] - 52s 28ms/step - loss: 0.8753 - accuracy: 0.7128\n",
      "\n",
      "Epoch 00250: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 250/300\n",
      " 348/1875 [====>.........................] - ETA: 41s - loss: 0.8698 - accuracy: 0.7147"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-c83013a1271e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m                  \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseCategoricalCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                  metrics=['accuracy'])\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtn_model2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdecay\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_train_2d = x_train[:, :, :, tf.newaxis]\n",
    "\n",
    "def step_decay(epoch):\n",
    "    x = 2e-5\n",
    "    if epoch >= 100 and epoch <= 200:\n",
    "        x = 2e-4\n",
    "    return x\n",
    "\n",
    "decay = LearningRateScheduler(step_decay, verbose=1)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "tn_model2.compile(optimizer=optimizer, \n",
    "                 loss=tf.keras.losses.SparseCategoricalCrossentropy(), \n",
    "                 metrics=['accuracy'])\n",
    "history = tn_model2.fit(x_train_2d, y_train, batch_size=32, epochs=300, verbose=1, callbacks=[decay])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_2d = x_test[:, :, :, tf.newaxis]\n",
    "pred = tn_model2.predict(x_test_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.88396434e-05, 5.02506551e-05, 2.41412798e-01, 1.84134023e-02,\n",
       "       2.93382341e-05, 6.80038160e-02, 6.71760264e-01, 1.64887619e-07,\n",
       "       1.26278551e-04, 1.04848077e-04])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 8s 24ms/step - loss: 0.9037 - accuracy: 0.7687\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = tn_model2.evaluate(x_test_2d, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Layer TNLayer has arguments in `__init__` and therefore must override `get_config`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-0c94524f5f98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtn_model2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./TN_MNIST_model_lr5em4_batch64.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#del model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#model = load_model('/path/to/model.h5')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m   1977\u001b[0m     \"\"\"\n\u001b[1;32m   1978\u001b[0m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0;32m-> 1979\u001b[0;31m                     signatures, options)\n\u001b[0m\u001b[1;32m   1980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1981\u001b[0m   def save_weights(self,\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m    129\u001b[0m           'or using `save_weights`.')\n\u001b[1;32m    130\u001b[0m     hdf5_format.save_model_to_hdf5(\n\u001b[0;32m--> 131\u001b[0;31m         model, filepath, overwrite, include_optimizer)\n\u001b[0m\u001b[1;32m    132\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mmodel_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaving_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/saving/saving_utils.py\u001b[0m in \u001b[0;36mmodel_metadata\u001b[0;34m(model, include_optimizer, require_config)\u001b[0m\n\u001b[1;32m    155\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrequire_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m   metadata = dict(\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/saving/saving_utils.py\u001b[0m in \u001b[0;36mmodel_metadata\u001b[0;34m(model, include_optimizer, require_config)\u001b[0m\n\u001b[1;32m    152\u001b[0m   \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'class_name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0mmodel_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrequire_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mget_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    463\u001b[0m       \u001b[0;31m# of `self.layers`). Note that `self._layers` is managed by the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m       \u001b[0;31m# tracking infrastructure and should not be used.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m       \u001b[0mlayer_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneric_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialize_keras_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m     config = {\n\u001b[1;32m    467\u001b[0m         \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mserialize_keras_object\u001b[0;34m(instance)\u001b[0m\n\u001b[1;32m    248\u001b[0m         return serialize_keras_class_and_config(\n\u001b[1;32m    249\u001b[0m             name, {_LAYER_UNDEFINED_CONFIG_KEY: True})\n\u001b[0;32m--> 250\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m     \u001b[0mserialization_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mserialize_keras_object\u001b[0;34m(instance)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_registered_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m       \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_SKIP_FAILED_SERIALIZATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mget_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m       raise NotImplementedError('Layer %s has arguments in `__init__` and '\n\u001b[1;32m    677\u001b[0m                                 \u001b[0;34m'therefore must override `get_config`.'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m                                 self.__class__.__name__)\n\u001b[0m\u001b[1;32m    679\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Layer TNLayer has arguments in `__init__` and therefore must override `get_config`."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "tn_model2.save('./TN_MNIST_model_lr5em4_batch64.h5')\n",
    "#del model\n",
    "#model = load_model('/path/to/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
