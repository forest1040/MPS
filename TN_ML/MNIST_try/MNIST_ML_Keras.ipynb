{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "#tf.enable_v2_behavior()\n",
    "# Import tensornetwork\n",
    "import tensornetwork as tn\n",
    "# Set the backend to tesorflow\n",
    "# (default is numpy)\n",
    "tn.set_default_backend(\"tensorflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 0-255の値が入っているので、0-1に収まるよう正規化します\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vec = []\n",
    "for i in range(x_train.shape[0]):\n",
    "    x_vec_ = np.concatenate([x_train[i][j, ::-2*(j%2)+1] for j in range(x_train.shape[1])])\n",
    "    x_vec.append(x_vec_)\n",
    "x_train_1d = np.vstack(x_vec)\n",
    "\n",
    "x_vec = []\n",
    "for i in range(x_test.shape[0]):\n",
    "    x_vec_ = np.concatenate([x_test[i][j, ::-2*(j%2)+1] for j in range(x_test.shape[1])])\n",
    "    x_vec.append(x_vec_)\n",
    "x_test_1d = np.vstack(x_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving a component\n",
    "\n",
    "def block(*dimensions, norm = 1):\n",
    "    '''Construct a new matrix for the MPS with random numbers from 0 to 1'''\n",
    "    size = tuple([x for x in dimensions])\n",
    "    return np.random.normal(loc = 1/np.max(size), scale = 1/np.max(size), size = size)\n",
    "\n",
    "def create_MPS(rank, dim, bond_dim):\n",
    "    '''Build the MPS tensor'''\n",
    "    mps = [\n",
    "        tn.Node( block(dim, bond_dim) )] + \\\n",
    "        [tn.Node( block(bond_dim, dim, bond_dim)) for _ in range(rank-2)] + \\\n",
    "        [tn.Node( block(bond_dim, dim) )\n",
    "        ]\n",
    "\n",
    "    #connect edges to build mps\n",
    "    connected_edges=[]\n",
    "    conn=mps[0][1]^mps[1][0]\n",
    "    connected_edges.append(conn)\n",
    "    for k in range(1,rank-1):\n",
    "        conn=mps[k][2]^mps[k+1][0]\n",
    "        connected_edges.append(conn)\n",
    "\n",
    "    return mps, connected_edges\n",
    "\n",
    "def create_MPS_labeled(rank, dim, bond_dim):\n",
    "    '''Build the MPS tensor'''\n",
    "    half = np.int((rank - 2) / 2)\n",
    "    norm = 1 / bond_dim\n",
    "    mps = [\n",
    "        tn.Node( block(dim, bond_dim, norm=norm) )] + \\\n",
    "        [tn.Node( block(bond_dim, dim, bond_dim, norm = norm)) for _ in range(half)] + \\\n",
    "        [tn.Node( block(bond_dim, label_dim, bond_dim, norm=norm) )] + \\\n",
    "        [tn.Node( block(bond_dim, dim, bond_dim, norm=norm)) for _ in range(half, rank-2)] + \\\n",
    "        [tn.Node( block(bond_dim, dim, norm=norm) )\n",
    "        ]\n",
    "\n",
    "    #connect edges to build mps\n",
    "    connected_edges=[]\n",
    "    conn=mps[0][1]^mps[1][0]\n",
    "    connected_edges.append(conn)\n",
    "    for k in range(1,rank):\n",
    "        conn=mps[k][2]^mps[k+1][0]\n",
    "        connected_edges.append(conn)\n",
    "\n",
    "    return mps, connected_edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_map(p):\n",
    "    phi = [1-p, p]\n",
    "    return phi\n",
    "\n",
    "def data_tensorize(vec):\n",
    "    data_tensor = [tn.Node(feature_map(p)) for p in vec]\n",
    "    return data_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_len = 1\n",
    "label_dim = 10\n",
    "data_len = x_train_1d.shape[1]\n",
    "rank = data_len\n",
    "dim = 2\n",
    "bond_dim = 10\n",
    "# mps, edges = create_MPS(rank, dim, bond_dim)\n",
    "mps, edges = create_MPS_labeled(rank, dim, bond_dim)\n",
    "\n",
    "test_vec = x_train_1d[0]\n",
    "data_tensor = data_tensorize(test_vec)\n",
    "\n",
    "edges.append(data_tensor[0][0] ^ mps[0][0])\n",
    "half_len = np.int(len(data_tensor) / 2)\n",
    "[edges.append(data_tensor[i][0] ^ mps[i][1]) for i in range(1, half_len)]\n",
    "[edges.append(data_tensor[i-label_len][0] ^ mps[i][1]) for i in range(half_len + label_len, data_len + label_len)]\n",
    "for k in reversed(range(len(edges))):\n",
    "    A = tn.contract(edges[k])\n",
    "result = A.tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00721898 0.00809315 0.00991432 0.00682729 0.00746707 0.00656614\n",
      " 0.00692087 0.0070252  0.00647266 0.00567744]\n",
      "[0.00721898 0.00809315 0.00991432 0.00682729 0.00746707 0.00656614\n",
      " 0.00692087 0.0070252  0.00647266 0.00567744]\n",
      "[-4.93104215 -4.81673698 -4.61377472 -4.98682696 -4.8972527  -5.02582857\n",
      " -4.97321438 -4.95825138 -5.04016787 -5.17125548]\n",
      "[0.10000001 0.10008746 0.10026991 0.09996085 0.10002482 0.09993475\n",
      " 0.0999702  0.09998063 0.0999254  0.09984597]\n"
     ]
    }
   ],
   "source": [
    "print(A.tensor.numpy())\n",
    "print(A.tensor.numpy().astype(\"float32\"))\n",
    "print(tf.math.log(A.tensor).numpy())\n",
    "print(tf.nn.softmax(A.tensor).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block(*dimensions):\n",
    "    '''Construct a new matrix for the MPS with random numbers from 0 to 1'''\n",
    "    size = tuple([x for x in dimensions])\n",
    "    return tf.Variable(\n",
    "        tf.random.normal(shape=size, dtype=tf.dtypes.float64, mean= 1/np.max(size), stddev = 1/np.max(size)),\n",
    "        trainable=True)\n",
    "\n",
    "def create_blocks(rank, dim, bond_dim, label_dim):\n",
    "    half = np.int((rank - 2) / 2)\n",
    "    blocks = [\n",
    "        block(dim, bond_dim) ] + \\\n",
    "        [ block(bond_dim, dim, bond_dim) for _ in range(half)] + \\\n",
    "        [ block(bond_dim, label_dim, bond_dim) ] + \\\n",
    "        [ block(bond_dim, dim, bond_dim) for _ in range(half, rank-2)] + \\\n",
    "        [ block(bond_dim, dim) \n",
    "        ]\n",
    "    return blocks\n",
    "\n",
    "def create_MPS_labeled(blocks, rank, dim, bond_dim):\n",
    "    '''Build the MPS tensor'''\n",
    "    half = np.int((rank - 2) / 2)\n",
    "    mps = []\n",
    "    for b in blocks:\n",
    "        mps.append(tn.Node(b))\n",
    "\n",
    "    #connect edges to build mps\n",
    "    connected_edges=[]\n",
    "    conn=mps[0][1]^mps[1][0]\n",
    "    connected_edges.append(conn)\n",
    "    for k in range(1,rank):\n",
    "        conn=mps[k][2]^mps[k+1][0]\n",
    "        connected_edges.append(conn)\n",
    "\n",
    "    return mps, connected_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TNLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_len, label_num, bond_dim):\n",
    "        self.label_len = 1\n",
    "        self.label_dim = label_num\n",
    "        self.rank = input_len\n",
    "        self.dim = 2\n",
    "        self.bond_dim = bond_dim\n",
    "        #super(TNLayer, self).__init__()\n",
    "        super().__init__()\n",
    "        # Create the variables for the layer.\n",
    "        self.blocks = create_blocks(self.rank, self.dim, self.bond_dim, self.label_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        def f(input_vec, blocks, rank, dim, bond_dim, label_len):\n",
    "            mps, edges = create_MPS_labeled(blocks, rank, dim, bond_dim)\n",
    "            data_tensor = []\n",
    "            for p in tf.unstack(input_vec):\n",
    "                data_tensor.append(tn.Node([1-p, p]))\n",
    "            edges.append(data_tensor[0][0] ^ mps[0][0])\n",
    "            half_len = np.int(rank / 2)\n",
    "            [edges.append(data_tensor[i][0] ^ mps[i][1]) for i in range(1, half_len)]\n",
    "            [edges.append(data_tensor[i-label_len][0] ^ mps[i][1]) \\\n",
    "                 for i in range(half_len + label_len, rank + label_len)]\n",
    "            for k in reversed(range(len(edges))):\n",
    "                A = tn.contract(edges[k])\n",
    "            #result = tf.math.log(A.tensor)\n",
    "            result = A.tensor# - tf.math.reduce_max(A.tensor)\n",
    "            return result\n",
    "\n",
    "        result = tf.vectorized_map(\n",
    "        lambda vec: f(vec, self.blocks, self.rank, self.dim, self.bond_dim, self.label_len), inputs)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "tn_layer_6 (TNLayer)         (None, 10)                157440    \n",
      "_________________________________________________________________\n",
      "softmax_6 (Softmax)          (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 157,440\n",
      "Trainable params: 157,440\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "N = x_train_1d.shape[1]\n",
    "label_len = 1\n",
    "label_num = 10\n",
    "data_len = x_train_1d.shape[1]\n",
    "rank = data_len\n",
    "dim = 2\n",
    "bond_dim = 10\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "tn_model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(N,)),\n",
    "        TNLayer(N, label_num, bond_dim),\n",
    "        tf.keras.layers.Softmax()\n",
    "    ])\n",
    "\n",
    "tn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 1/300\n",
      "1875/1875 [==============================] - 170s 91ms/step - loss: 2.0786 - accuracy: 0.1019\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 2/300\n",
      "1875/1875 [==============================] - 167s 89ms/step - loss: 2.0098 - accuracy: 0.1020\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 3/300\n",
      "1875/1875 [==============================] - 168s 90ms/step - loss: 1.9897 - accuracy: 0.1022\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 4/300\n",
      "1875/1875 [==============================] - 166s 89ms/step - loss: 1.9743 - accuracy: 0.1024\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 5/300\n",
      "1875/1875 [==============================] - 169s 90ms/step - loss: 1.9680 - accuracy: 0.1032\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 6/300\n",
      "1875/1875 [==============================] - 170s 90ms/step - loss: 1.9598 - accuracy: 0.1148\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 7/300\n",
      "1875/1875 [==============================] - 167s 89ms/step - loss: 1.9951 - accuracy: 0.1243\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 8/300\n",
      "1875/1875 [==============================] - 169s 90ms/step - loss: 1.9475 - accuracy: 0.1226\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 9/300\n",
      "1875/1875 [==============================] - 170s 90ms/step - loss: 1.9416 - accuracy: 0.1248\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 10/300\n",
      "1875/1875 [==============================] - 172s 92ms/step - loss: 1.9343 - accuracy: 0.1291\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 11/300\n",
      "1875/1875 [==============================] - 172s 91ms/step - loss: 1.9311 - accuracy: 0.1321\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 12/300\n",
      "1875/1875 [==============================] - 169s 90ms/step - loss: 1.9250 - accuracy: 0.1337\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 13/300\n",
      "1875/1875 [==============================] - 162s 86ms/step - loss: 1.9630 - accuracy: 0.1330\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 14/300\n",
      "1875/1875 [==============================] - 161s 86ms/step - loss: 1.9505 - accuracy: 0.1317\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 15/300\n",
      "1875/1875 [==============================] - 161s 86ms/step - loss: 1.9281 - accuracy: 0.1356\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 16/300\n",
      "1875/1875 [==============================] - 158s 84ms/step - loss: 2.1839 - accuracy: 0.1358\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 17/300\n",
      "1875/1875 [==============================] - 157s 84ms/step - loss: 2.0593 - accuracy: 0.1416\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 18/300\n",
      "1875/1875 [==============================] - 163s 87ms/step - loss: 2.3026 - accuracy: 0.1415\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 19/300\n",
      "1875/1875 [==============================] - 164s 88ms/step - loss: 2.2531 - accuracy: 0.1430\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 20/300\n",
      "1875/1875 [==============================] - 163s 87ms/step - loss: 2.0696 - accuracy: 0.1401\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 21/300\n",
      "1875/1875 [==============================] - 163s 87ms/step - loss: 2.0339 - accuracy: 0.1424\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 22/300\n",
      "1875/1875 [==============================] - 170s 91ms/step - loss: 1.9482 - accuracy: 0.1419\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 23/300\n",
      "1875/1875 [==============================] - 165s 88ms/step - loss: 1.9084 - accuracy: 0.1481\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 24/300\n",
      "1875/1875 [==============================] - 166s 88ms/step - loss: 1.9017 - accuracy: 0.1465\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 25/300\n",
      "1875/1875 [==============================] - 168s 90ms/step - loss: 1.9026 - accuracy: 0.1471\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 26/300\n",
      "1875/1875 [==============================] - 170s 91ms/step - loss: 1.8948 - accuracy: 0.1472\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 27/300\n",
      "1875/1875 [==============================] - 170s 90ms/step - loss: 1.9044 - accuracy: 0.1467\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 28/300\n",
      "1875/1875 [==============================] - 160s 86ms/step - loss: 1.8956 - accuracy: 0.1469\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 29/300\n",
      "1875/1875 [==============================] - 160s 85ms/step - loss: 1.8998 - accuracy: 0.1468\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 30/300\n",
      "1875/1875 [==============================] - 162s 86ms/step - loss: 1.8904 - accuracy: 0.1466\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 31/300\n",
      "1875/1875 [==============================] - 156s 83ms/step - loss: 1.8842 - accuracy: 0.1465\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 32/300\n",
      "1875/1875 [==============================] - 158s 84ms/step - loss: 1.9563 - accuracy: 0.1490\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 33/300\n",
      "1875/1875 [==============================] - 170s 91ms/step - loss: 2.1054 - accuracy: 0.1466\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 34/300\n",
      "1875/1875 [==============================] - 171s 91ms/step - loss: 1.8938 - accuracy: 0.1504\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 35/300\n",
      "1875/1875 [==============================] - 172s 92ms/step - loss: 1.8832 - accuracy: 0.1506\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 36/300\n",
      "1875/1875 [==============================] - 173s 92ms/step - loss: 1.8984 - accuracy: 0.1526\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 37/300\n",
      "1875/1875 [==============================] - 171s 91ms/step - loss: 1.8791 - accuracy: 0.1513\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 38/300\n",
      "1875/1875 [==============================] - 173s 92ms/step - loss: 1.8890 - accuracy: 0.1530\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 39/300\n",
      "1875/1875 [==============================] - 173s 92ms/step - loss: 1.8788 - accuracy: 0.1577\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 40/300\n",
      "1875/1875 [==============================] - 173s 92ms/step - loss: 1.8789 - accuracy: 0.1562\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 41/300\n",
      "1875/1875 [==============================] - 173s 92ms/step - loss: 1.8754 - accuracy: 0.1568\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 42/300\n",
      "1875/1875 [==============================] - 173s 92ms/step - loss: 1.9166 - accuracy: 0.1557\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 43/300\n",
      "1875/1875 [==============================] - 173s 93ms/step - loss: 1.8836 - accuracy: 0.1574\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 44/300\n",
      "1875/1875 [==============================] - 174s 93ms/step - loss: 1.8725 - accuracy: 0.1582\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 45/300\n",
      "1875/1875 [==============================] - 173s 92ms/step - loss: 1.8716 - accuracy: 0.1570\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 46/300\n",
      "1875/1875 [==============================] - 165s 88ms/step - loss: 1.8773 - accuracy: 0.1596\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 47/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 160s 85ms/step - loss: 1.8870 - accuracy: 0.1601\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 48/300\n",
      "1875/1875 [==============================] - 160s 85ms/step - loss: 1.8755 - accuracy: 0.1599\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 49/300\n",
      "1875/1875 [==============================] - 161s 86ms/step - loss: 1.8673 - accuracy: 0.1613\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 50/300\n",
      "1875/1875 [==============================] - 159s 85ms/step - loss: 1.8654 - accuracy: 0.1623\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 51/300\n",
      "1875/1875 [==============================] - 160s 85ms/step - loss: 1.8672 - accuracy: 0.1643\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 52/300\n",
      "1875/1875 [==============================] - 158s 84ms/step - loss: 1.8770 - accuracy: 0.1643\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 53/300\n",
      "1875/1875 [==============================] - 162s 86ms/step - loss: 1.8711 - accuracy: 0.1652\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 54/300\n",
      "1875/1875 [==============================] - 159s 85ms/step - loss: 2.1705 - accuracy: 0.1658\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 55/300\n",
      "1875/1875 [==============================] - 163s 87ms/step - loss: 2.3026 - accuracy: 0.1648\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 56/300\n",
      "1875/1875 [==============================] - 167s 89ms/step - loss: 2.3026 - accuracy: 0.1648\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 57/300\n",
      "1875/1875 [==============================] - 165s 88ms/step - loss: 2.2936 - accuracy: 0.1691\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 58/300\n",
      "1875/1875 [==============================] - 165s 88ms/step - loss: 2.2656 - accuracy: 0.1782\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 59/300\n",
      "1875/1875 [==============================] - 162s 86ms/step - loss: 1.8973 - accuracy: 0.1660\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 60/300\n",
      "1875/1875 [==============================] - 163s 87ms/step - loss: 1.8625 - accuracy: 0.1659\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 61/300\n",
      "1875/1875 [==============================] - 162s 87ms/step - loss: 1.8696 - accuracy: 0.1666\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 62/300\n",
      "1875/1875 [==============================] - 162s 86ms/step - loss: 1.8576 - accuracy: 0.1667\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 63/300\n",
      "1875/1875 [==============================] - 163s 87ms/step - loss: 2.0257 - accuracy: 0.1673\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 64/300\n",
      "1875/1875 [==============================] - 164s 87ms/step - loss: 2.3026 - accuracy: 0.1674\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 65/300\n",
      "1875/1875 [==============================] - 163s 87ms/step - loss: 2.3026 - accuracy: 0.1674\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 66/300\n",
      "1875/1875 [==============================] - 164s 87ms/step - loss: 2.3026 - accuracy: 0.1674\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 67/300\n",
      "1875/1875 [==============================] - 164s 88ms/step - loss: 2.3026 - accuracy: 0.1674\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 68/300\n",
      "1875/1875 [==============================] - 164s 88ms/step - loss: 2.3026 - accuracy: 0.1674\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 69/300\n",
      "1875/1875 [==============================] - 162s 86ms/step - loss: 2.3026 - accuracy: 0.1673\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 70/300\n",
      "1875/1875 [==============================] - 163s 87ms/step - loss: 2.3026 - accuracy: 0.1673\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 71/300\n",
      "1875/1875 [==============================] - 164s 87ms/step - loss: 2.3035 - accuracy: 0.1780\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 72/300\n",
      "1875/1875 [==============================] - 163s 87ms/step - loss: 2.3026 - accuracy: 0.1790\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 73/300\n",
      "1875/1875 [==============================] - 163s 87ms/step - loss: 2.3026 - accuracy: 0.1790\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 74/300\n",
      "1875/1875 [==============================] - 159s 85ms/step - loss: 2.3026 - accuracy: 0.1790\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 75/300\n",
      "1875/1875 [==============================] - 159s 85ms/step - loss: 2.3026 - accuracy: 0.1758\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 76/300\n",
      "1875/1875 [==============================] - 160s 85ms/step - loss: 2.3026 - accuracy: 0.1675\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 77/300\n",
      "1875/1875 [==============================] - 162s 86ms/step - loss: 2.3026 - accuracy: 0.1675\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 78/300\n",
      "1875/1875 [==============================] - 162s 86ms/step - loss: 2.3026 - accuracy: 0.1675\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 79/300\n",
      "1875/1875 [==============================] - 162s 86ms/step - loss: 2.3026 - accuracy: 0.1675\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 80/300\n",
      "1875/1875 [==============================] - 159s 85ms/step - loss: 2.3026 - accuracy: 0.1674\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 81/300\n",
      "1875/1875 [==============================] - 160s 85ms/step - loss: 2.3026 - accuracy: 0.1674\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 82/300\n",
      "1875/1875 [==============================] - 158s 84ms/step - loss: 2.3037 - accuracy: 0.1679\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 83/300\n",
      "1875/1875 [==============================] - 157s 84ms/step - loss: 2.3170 - accuracy: 0.1700\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 84/300\n",
      "1875/1875 [==============================] - 159s 85ms/step - loss: 2.3026 - accuracy: 0.1757\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 85/300\n",
      "1875/1875 [==============================] - 159s 85ms/step - loss: 2.3026 - accuracy: 0.1757\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 86/300\n",
      "1875/1875 [==============================] - 160s 85ms/step - loss: 2.3026 - accuracy: 0.1755\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 87/300\n",
      "1875/1875 [==============================] - 158s 84ms/step - loss: 2.3026 - accuracy: 0.1754\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 88/300\n",
      "1875/1875 [==============================] - 163s 87ms/step - loss: 2.3026 - accuracy: 0.1744\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 89/300\n",
      "1875/1875 [==============================] - 163s 87ms/step - loss: 2.3051 - accuracy: 0.1773\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 90/300\n",
      "1875/1875 [==============================] - 163s 87ms/step - loss: 2.3026 - accuracy: 0.1768\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 91/300\n",
      "1875/1875 [==============================] - 160s 86ms/step - loss: 2.3032 - accuracy: 0.1746\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 92/300\n",
      "1875/1875 [==============================] - 160s 86ms/step - loss: 2.3026 - accuracy: 0.1798\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 93/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 161s 86ms/step - loss: 2.3026 - accuracy: 0.1781\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 94/300\n",
      "1875/1875 [==============================] - 162s 87ms/step - loss: 2.3106 - accuracy: 0.1930\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 95/300\n",
      "1875/1875 [==============================] - 158s 84ms/step - loss: 2.3026 - accuracy: 0.2074\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 96/300\n",
      "1875/1875 [==============================] - 161s 86ms/step - loss: 2.3026 - accuracy: 0.2011\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 97/300\n",
      "1875/1875 [==============================] - 162s 86ms/step - loss: 2.3026 - accuracy: 0.1895\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 98/300\n",
      "1875/1875 [==============================] - 162s 86ms/step - loss: 2.4224 - accuracy: 0.1789\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 99/300\n",
      "1875/1875 [==============================] - 162s 87ms/step - loss: 2.3020 - accuracy: 0.1748\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 100/300\n",
      "1875/1875 [==============================] - 160s 86ms/step - loss: 2.3382 - accuracy: 0.1713\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 101/300\n",
      "1875/1875 [==============================] - 163s 87ms/step - loss: 2.3026 - accuracy: 0.1791\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 102/300\n",
      "1875/1875 [==============================] - 160s 85ms/step - loss: 2.3026 - accuracy: 0.1792\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 103/300\n",
      "1875/1875 [==============================] - 162s 87ms/step - loss: 2.3026 - accuracy: 0.1794\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 104/300\n",
      "1875/1875 [==============================] - 159s 85ms/step - loss: 2.3026 - accuracy: 0.1804\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 105/300\n",
      "1875/1875 [==============================] - 161s 86ms/step - loss: 2.3026 - accuracy: 0.1856\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 106/300\n",
      "1875/1875 [==============================] - 163s 87ms/step - loss: 2.3029 - accuracy: 0.1970\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 107/300\n",
      "1875/1875 [==============================] - 162s 87ms/step - loss: 2.3026 - accuracy: 0.1729\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 108/300\n",
      "1875/1875 [==============================] - 161s 86ms/step - loss: 2.3026 - accuracy: 0.1739\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 109/300\n",
      "1875/1875 [==============================] - 162s 87ms/step - loss: 2.3026 - accuracy: 0.1795\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 110/300\n",
      "1875/1875 [==============================] - 162s 86ms/step - loss: 2.9208 - accuracy: 0.1898\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 111/300\n",
      "1875/1875 [==============================] - 162s 86ms/step - loss: 2.3021 - accuracy: 0.2176\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 112/300\n",
      "1875/1875 [==============================] - 161s 86ms/step - loss: 2.3002 - accuracy: 0.1982\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 113/300\n",
      "1875/1875 [==============================] - 160s 85ms/step - loss: 2.3036 - accuracy: 0.1922\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 114/300\n",
      "1875/1875 [==============================] - 160s 86ms/step - loss: 2.2957 - accuracy: 0.1866\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 115/300\n",
      "1875/1875 [==============================] - 161s 86ms/step - loss: 2.2947 - accuracy: 0.1777\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 116/300\n",
      "1875/1875 [==============================] - 161s 86ms/step - loss: 2.3017 - accuracy: 0.1842\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 117/300\n",
      "1875/1875 [==============================] - 162s 86ms/step - loss: 2.3109 - accuracy: 0.1786\n",
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 118/300\n",
      "1875/1875 [==============================] - 160s 85ms/step - loss: 2.3026 - accuracy: 0.1458\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 119/300\n",
      "1875/1875 [==============================] - 162s 87ms/step - loss: 2.3026 - accuracy: 0.1467\n",
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 120/300\n",
      "1875/1875 [==============================] - 162s 86ms/step - loss: 2.3026 - accuracy: 0.1491\n",
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 121/300\n",
      "1875/1875 [==============================] - 160s 86ms/step - loss: 2.3025 - accuracy: 0.1547\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 122/300\n",
      "1875/1875 [==============================] - 161s 86ms/step - loss: 5.8631 - accuracy: 0.1845\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 123/300\n",
      "1875/1875 [==============================] - 160s 85ms/step - loss: 2.3016 - accuracy: 0.1954\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 124/300\n",
      "1875/1875 [==============================] - 160s 85ms/step - loss: 2.3052 - accuracy: 0.1953\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 125/300\n",
      "1875/1875 [==============================] - 162s 86ms/step - loss: 2.2953 - accuracy: 0.1927\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 126/300\n",
      "1875/1875 [==============================] - 162s 86ms/step - loss: 2.2932 - accuracy: 0.1809\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 127/300\n",
      "1875/1875 [==============================] - 161s 86ms/step - loss: 2.2883 - accuracy: 0.1776\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 128/300\n",
      "1875/1875 [==============================] - 159s 85ms/step - loss: 2.2865 - accuracy: 0.1738\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 129/300\n",
      "1875/1875 [==============================] - 159s 85ms/step - loss: 2.2829 - accuracy: 0.1728\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 130/300\n",
      "1875/1875 [==============================] - 161s 86ms/step - loss: 2.3218 - accuracy: 0.1709\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 131/300\n",
      "1875/1875 [==============================] - 162s 86ms/step - loss: 2.3021 - accuracy: 0.1643\n",
      "\n",
      "Epoch 00132: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 132/300\n",
      "1875/1875 [==============================] - 160s 85ms/step - loss: 2.3017 - accuracy: 0.1695\n",
      "\n",
      "Epoch 00133: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 133/300\n",
      "1875/1875 [==============================] - 159s 85ms/step - loss: 2.3107 - accuracy: 0.1795\n",
      "\n",
      "Epoch 00134: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 134/300\n",
      "1875/1875 [==============================] - 159s 85ms/step - loss: 2.3007 - accuracy: 0.1671\n",
      "\n",
      "Epoch 00135: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 135/300\n",
      "1875/1875 [==============================] - 160s 86ms/step - loss: 2.2959 - accuracy: 0.1856\n",
      "\n",
      "Epoch 00136: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 136/300\n",
      "1875/1875 [==============================] - 160s 85ms/step - loss: 2.3091 - accuracy: 0.1456\n",
      "\n",
      "Epoch 00137: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 137/300\n",
      "1875/1875 [==============================] - 158s 84ms/step - loss: 2.3023 - accuracy: 0.1346\n",
      "\n",
      "Epoch 00138: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 138/300\n",
      "1875/1875 [==============================] - 159s 85ms/step - loss: 2.3022 - accuracy: 0.1401\n",
      "\n",
      "Epoch 00139: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 139/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 160s 85ms/step - loss: 2.2999 - accuracy: 0.1656\n",
      "\n",
      "Epoch 00140: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 140/300\n",
      "1875/1875 [==============================] - 161s 86ms/step - loss: 2.3038 - accuracy: 0.1507\n",
      "\n",
      "Epoch 00141: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 141/300\n",
      "1875/1875 [==============================] - 159s 85ms/step - loss: 2.3023 - accuracy: 0.1474\n",
      "\n",
      "Epoch 00142: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 142/300\n",
      "1875/1875 [==============================] - 160s 85ms/step - loss: 2.3017 - accuracy: 0.1630\n",
      "\n",
      "Epoch 00143: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 143/300\n",
      "1875/1875 [==============================] - 158s 85ms/step - loss: 2.3406 - accuracy: 0.1762\n",
      "\n",
      "Epoch 00144: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 144/300\n",
      "1875/1875 [==============================] - 158s 84ms/step - loss: 2.2943 - accuracy: 0.1838\n",
      "\n",
      "Epoch 00145: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 145/300\n",
      "1875/1875 [==============================] - 160s 86ms/step - loss: 2.2922 - accuracy: 0.1861\n",
      "\n",
      "Epoch 00146: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 146/300\n",
      "1875/1875 [==============================] - 155s 83ms/step - loss: 2.2924 - accuracy: 0.1875\n",
      "\n",
      "Epoch 00147: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 147/300\n",
      "1875/1875 [==============================] - 159s 85ms/step - loss: 2.3914 - accuracy: 0.1648\n",
      "\n",
      "Epoch 00148: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 148/300\n",
      "1875/1875 [==============================] - 159s 85ms/step - loss: 2.3020 - accuracy: 0.1403\n",
      "\n",
      "Epoch 00149: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 149/300\n",
      "1875/1875 [==============================] - 160s 85ms/step - loss: 2.3019 - accuracy: 0.1421\n",
      "\n",
      "Epoch 00150: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 150/300\n",
      "1875/1875 [==============================] - 160s 85ms/step - loss: 2.3008 - accuracy: 0.1520\n",
      "\n",
      "Epoch 00151: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 151/300\n",
      "1875/1875 [==============================] - 160s 85ms/step - loss: 2.2912 - accuracy: 0.1831\n",
      "\n",
      "Epoch 00152: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 152/300\n",
      "1875/1875 [==============================] - 158s 84ms/step - loss: 2.3879 - accuracy: 0.1594\n",
      "\n",
      "Epoch 00153: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 153/300\n",
      "1875/1875 [==============================] - 162s 86ms/step - loss: 2.3022 - accuracy: 0.1294\n",
      "\n",
      "Epoch 00154: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 154/300\n",
      "1875/1875 [==============================] - 157s 84ms/step - loss: 2.3022 - accuracy: 0.1307\n",
      "\n",
      "Epoch 00155: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 155/300\n",
      "1875/1875 [==============================] - 161s 86ms/step - loss: 2.3020 - accuracy: 0.1353\n",
      "\n",
      "Epoch 00156: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 156/300\n",
      "1875/1875 [==============================] - 159s 85ms/step - loss: 2.3006 - accuracy: 0.1587\n",
      "\n",
      "Epoch 00157: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 157/300\n",
      "1875/1875 [==============================] - 157s 84ms/step - loss: 2.3018 - accuracy: 0.1540\n",
      "\n",
      "Epoch 00158: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 158/300\n",
      "1875/1875 [==============================] - 159s 85ms/step - loss: 2.2966 - accuracy: 0.1776\n",
      "\n",
      "Epoch 00159: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 159/300\n",
      "1875/1875 [==============================] - 158s 85ms/step - loss: 2.3032 - accuracy: 0.1598\n",
      "\n",
      "Epoch 00160: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 160/300\n",
      "1875/1875 [==============================] - 160s 85ms/step - loss: 2.3017 - accuracy: 0.1489\n",
      "\n",
      "Epoch 00161: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 161/300\n",
      "1875/1875 [==============================] - 163s 87ms/step - loss: 3.3834 - accuracy: 0.1686\n",
      "\n",
      "Epoch 00162: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 162/300\n",
      "1875/1875 [==============================] - 162s 86ms/step - loss: 2.3018 - accuracy: 0.1389\n",
      "\n",
      "Epoch 00163: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 163/300\n",
      "1875/1875 [==============================] - 161s 86ms/step - loss: 2.3016 - accuracy: 0.1405\n",
      "\n",
      "Epoch 00164: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 164/300\n",
      "1875/1875 [==============================] - 159s 85ms/step - loss: 2.3009 - accuracy: 0.1460\n",
      "\n",
      "Epoch 00165: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 165/300\n",
      "1875/1875 [==============================] - 163s 87ms/step - loss: 2.2970 - accuracy: 0.1619\n",
      "\n",
      "Epoch 00166: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 166/300\n",
      " 279/1875 [===>..........................] - ETA: 2:19 - loss: 2.2867 - accuracy: 0.1717"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-4f6217314c78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m              \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m              \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m              steps_per_epoch=int(60000 / batch), callbacks=[decay])\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def step_decay(epoch):\n",
    "    x = 1e-4\n",
    "    if epoch >= 100 and epoch <= 200:\n",
    "        x = 1e-4\n",
    "    return x\n",
    "\n",
    "decay = LearningRateScheduler(step_decay, verbose=1)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "tn_model.compile(optimizer=optimizer, \n",
    "                 loss=tf.keras.losses.SparseCategoricalCrossentropy(), \n",
    "                 metrics=['accuracy'])\n",
    "batch = 32\n",
    "hist = tn_model.fit(x_train_1d, y_train, \n",
    "             batch_size=batch, epochs=300, \n",
    "             verbose=1, shuffle=True, \n",
    "             steps_per_epoch=int(60000 / batch), callbacks=[decay])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "average_pooling2d_2 (Average (None, 14, 14, 1)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 196)               0         \n",
      "_________________________________________________________________\n",
      "tn_layer_2 (TNLayer)         (None, 10)                39840     \n",
      "_________________________________________________________________\n",
      "softmax_2 (Softmax)          (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 39,840\n",
      "Trainable params: 39,840\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "label_len = 1\n",
    "label_num = 10\n",
    "dim = 2\n",
    "bond_dim = 10\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "tn_model2 = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(28, 28, 1)),\n",
    "        tf.keras.layers.AveragePooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        TNLayer(196, label_num, bond_dim),\n",
    "        tf.keras.layers.Softmax()\n",
    "    ],name=\"sequential_1\")\n",
    "\n",
    "tn_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 1/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 2.0648 - accuracy: 0.1188\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 2/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.9590 - accuracy: 0.1705\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 3/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.8918 - accuracy: 0.2611\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 4/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.8569 - accuracy: 0.2684\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 5/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.7742 - accuracy: 0.3078\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 6/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.6704 - accuracy: 0.3542\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 7/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.6319 - accuracy: 0.3830\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 8/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.5810 - accuracy: 0.4225\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 9/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.5377 - accuracy: 0.4427\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 10/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.5136 - accuracy: 0.4506\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 11/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.4979 - accuracy: 0.4608\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 12/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.4842 - accuracy: 0.4638\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 13/300\n",
      "1875/1875 [==============================] - 50s 26ms/step - loss: 1.4766 - accuracy: 0.4683\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 14/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.4675 - accuracy: 0.4698\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 15/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 1.4623 - accuracy: 0.4725\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 16/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.4540 - accuracy: 0.4759\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 17/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.4468 - accuracy: 0.4805\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 18/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.4322 - accuracy: 0.4863\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 19/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 1.4159 - accuracy: 0.4935\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 20/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.3829 - accuracy: 0.5068\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 21/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.3458 - accuracy: 0.5234\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 22/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.3215 - accuracy: 0.5292\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 23/300\n",
      "1875/1875 [==============================] - 50s 26ms/step - loss: 1.3015 - accuracy: 0.5352\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 24/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.2829 - accuracy: 0.5472\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 25/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.2648 - accuracy: 0.5607\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 26/300\n",
      "1875/1875 [==============================] - 50s 26ms/step - loss: 1.2460 - accuracy: 0.5746\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 27/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.2312 - accuracy: 0.5876\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 28/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.2193 - accuracy: 0.5956\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 29/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.2106 - accuracy: 0.5999\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 30/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.2018 - accuracy: 0.6025\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 31/300\n",
      "1875/1875 [==============================] - 53s 28ms/step - loss: 1.1960 - accuracy: 0.6054\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 32/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.1882 - accuracy: 0.6069\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 33/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.1821 - accuracy: 0.6089\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 34/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.1739 - accuracy: 0.6102\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 35/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.1671 - accuracy: 0.6118\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 36/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.1591 - accuracy: 0.6139\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 37/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.1512 - accuracy: 0.6158\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 38/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.1450 - accuracy: 0.6165\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 39/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.1366 - accuracy: 0.6191\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 40/300\n",
      "1875/1875 [==============================] - 50s 26ms/step - loss: 1.1286 - accuracy: 0.6200\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 41/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.1189 - accuracy: 0.6213\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 42/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.1101 - accuracy: 0.6223\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 43/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 1.0993 - accuracy: 0.6230\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 44/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 1.0865 - accuracy: 0.6224\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 45/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.0680 - accuracy: 0.6253\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 46/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.0463 - accuracy: 0.6345\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 47/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 50s 27ms/step - loss: 1.0149 - accuracy: 0.6535\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 48/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.9815 - accuracy: 0.6645\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 49/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.9643 - accuracy: 0.6693\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 50/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.9523 - accuracy: 0.6742\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 51/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.9441 - accuracy: 0.6761\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 52/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.9360 - accuracy: 0.6797\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 53/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.9302 - accuracy: 0.6800\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 54/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.9259 - accuracy: 0.6831\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 55/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.9182 - accuracy: 0.6854\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 56/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.9141 - accuracy: 0.6863\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 57/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.9093 - accuracy: 0.6879\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 58/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.9067 - accuracy: 0.6895\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 59/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.9027 - accuracy: 0.6897\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 60/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.8997 - accuracy: 0.6921\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 61/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.8969 - accuracy: 0.6936\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 62/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.8944 - accuracy: 0.6948\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 63/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.8908 - accuracy: 0.6964\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 64/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.8895 - accuracy: 0.6960\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 65/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.8848 - accuracy: 0.6983\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 66/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.8835 - accuracy: 0.6975\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 67/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.8802 - accuracy: 0.6984\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 68/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.8777 - accuracy: 0.7008\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 69/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.8751 - accuracy: 0.7001\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 70/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.8735 - accuracy: 0.7016\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 71/300\n",
      "1875/1875 [==============================] - 50s 26ms/step - loss: 0.8709 - accuracy: 0.7026\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 72/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.8685 - accuracy: 0.7024\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 73/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.8669 - accuracy: 0.7042\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 74/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.8657 - accuracy: 0.7049\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 75/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.8612 - accuracy: 0.7050\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 76/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.8586 - accuracy: 0.7060\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 77/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.8584 - accuracy: 0.7076\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 78/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.8549 - accuracy: 0.7050\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 79/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.8535 - accuracy: 0.7059\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 80/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.8525 - accuracy: 0.7070\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 81/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.8500 - accuracy: 0.7080\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 82/300\n",
      " 585/1875 [========>.....................] - ETA: 33s - loss: 0.8448 - accuracy: 0.7106"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-37c912473c6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m                  metrics=['accuracy'])\n\u001b[1;32m     16\u001b[0m history = tn_model2.fit(x_train_2d, y_train, batch_size=batch, shuffle=True, \n\u001b[0;32m---> 17\u001b[0;31m              steps_per_epoch=int(60000 / batch), epochs=300, verbose=1, callbacks=[decay])\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1101\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \"\"\"\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    287\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    307\u001b[0m       \u001b[0mbatch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    340\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_supports_tf_logs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1014\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1016\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1017\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 635\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 635\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    531\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \"\"\"\n\u001b[1;32m   1062\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1027\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_train_2d = x_train[:, :, :, tf.newaxis]\n",
    "\n",
    "def step_decay(epoch):\n",
    "    x = 1e-4\n",
    "    if epoch >= 100 and epoch <= 200:\n",
    "        x = 1e-5\n",
    "    return x\n",
    "\n",
    "decay = LearningRateScheduler(step_decay, verbose=1)\n",
    "batch = 32\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "tn_model2.compile(optimizer=optimizer, \n",
    "                 loss=tf.keras.losses.SparseCategoricalCrossentropy(), \n",
    "                 metrics=['accuracy'])\n",
    "history = tn_model2.fit(x_train_2d, y_train, batch_size=batch, shuffle=True, \n",
    "             steps_per_epoch=int(60000 / batch), epochs=300, verbose=1, callbacks=[decay])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_2d = x_test[:, :, :, tf.newaxis]\n",
    "pred = tn_model2.predict(x_test_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.88396434e-05, 5.02506551e-05, 2.41412798e-01, 1.84134023e-02,\n",
       "       2.93382341e-05, 6.80038160e-02, 6.71760264e-01, 1.64887619e-07,\n",
       "       1.26278551e-04, 1.04848077e-04])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 8s 24ms/step - loss: 0.9037 - accuracy: 0.7687\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = tn_model2.evaluate(x_test_2d, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Layer TNLayer has arguments in `__init__` and therefore must override `get_config`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-0c94524f5f98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtn_model2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./TN_MNIST_model_lr5em4_batch64.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#del model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#model = load_model('/path/to/model.h5')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m   1977\u001b[0m     \"\"\"\n\u001b[1;32m   1978\u001b[0m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0;32m-> 1979\u001b[0;31m                     signatures, options)\n\u001b[0m\u001b[1;32m   1980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1981\u001b[0m   def save_weights(self,\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m    129\u001b[0m           'or using `save_weights`.')\n\u001b[1;32m    130\u001b[0m     hdf5_format.save_model_to_hdf5(\n\u001b[0;32m--> 131\u001b[0;31m         model, filepath, overwrite, include_optimizer)\n\u001b[0m\u001b[1;32m    132\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mmodel_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaving_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/saving/saving_utils.py\u001b[0m in \u001b[0;36mmodel_metadata\u001b[0;34m(model, include_optimizer, require_config)\u001b[0m\n\u001b[1;32m    155\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrequire_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m   metadata = dict(\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/saving/saving_utils.py\u001b[0m in \u001b[0;36mmodel_metadata\u001b[0;34m(model, include_optimizer, require_config)\u001b[0m\n\u001b[1;32m    152\u001b[0m   \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'class_name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0mmodel_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrequire_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mget_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    463\u001b[0m       \u001b[0;31m# of `self.layers`). Note that `self._layers` is managed by the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m       \u001b[0;31m# tracking infrastructure and should not be used.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m       \u001b[0mlayer_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneric_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialize_keras_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m     config = {\n\u001b[1;32m    467\u001b[0m         \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mserialize_keras_object\u001b[0;34m(instance)\u001b[0m\n\u001b[1;32m    248\u001b[0m         return serialize_keras_class_and_config(\n\u001b[1;32m    249\u001b[0m             name, {_LAYER_UNDEFINED_CONFIG_KEY: True})\n\u001b[0;32m--> 250\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m     \u001b[0mserialization_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mserialize_keras_object\u001b[0;34m(instance)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_registered_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m       \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_SKIP_FAILED_SERIALIZATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mget_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m       raise NotImplementedError('Layer %s has arguments in `__init__` and '\n\u001b[1;32m    677\u001b[0m                                 \u001b[0;34m'therefore must override `get_config`.'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m                                 self.__class__.__name__)\n\u001b[0m\u001b[1;32m    679\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Layer TNLayer has arguments in `__init__` and therefore must override `get_config`."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "tn_model2.save('./TN_MNIST_model_lr5em4_batch64.h5')\n",
    "#del model\n",
    "#model = load_model('/path/to/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
