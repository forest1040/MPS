{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "#tf.enable_v2_behavior()\n",
    "# Import tensornetwork\n",
    "import tensornetwork as tn\n",
    "# Set the backend to tesorflow\n",
    "# (default is numpy)\n",
    "tn.set_default_backend(\"jax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 0-255の値が入っているので、0-1に収まるよう正規化します\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vec = []\n",
    "for i in range(x_train.shape[0]):\n",
    "    x_vec_ = np.concatenate([x_train[i][j, ::-2*(j%2)+1] for j in range(x_train.shape[1])])\n",
    "    x_vec.append(x_vec_)\n",
    "x_train_1d = np.vstack(x_vec)\n",
    "\n",
    "x_vec = []\n",
    "for i in range(x_test.shape[0]):\n",
    "    x_vec_ = np.concatenate([x_test[i][j, ::-2*(j%2)+1] for j in range(x_test.shape[1])])\n",
    "    x_vec.append(x_vec_)\n",
    "x_test_1d = np.vstack(x_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving a component\n",
    "\n",
    "def block(*dimensions, norm = 1):\n",
    "    '''Construct a new matrix for the MPS with random numbers from 0 to 1'''\n",
    "    size = tuple([x for x in dimensions])\n",
    "    return np.random.normal(loc = 0, scale = 0.5, size = size)\n",
    "\n",
    "def create_MPS(rank, dim, bond_dim):\n",
    "    '''Build the MPS tensor'''\n",
    "    mps = [\n",
    "        tn.Node( block(dim, bond_dim) )] + \\\n",
    "        [tn.Node( block(bond_dim, dim, bond_dim)) for _ in range(rank-2)] + \\\n",
    "        [tn.Node( block(bond_dim, dim) )\n",
    "        ]\n",
    "\n",
    "    #connect edges to build mps\n",
    "    connected_edges=[]\n",
    "    conn=mps[0][1]^mps[1][0]\n",
    "    connected_edges.append(conn)\n",
    "    for k in range(1,rank-1):\n",
    "        conn=mps[k][2]^mps[k+1][0]\n",
    "        connected_edges.append(conn)\n",
    "\n",
    "    return mps, connected_edges\n",
    "\n",
    "def create_MPS_labeled(rank, dim, bond_dim):\n",
    "    '''Build the MPS tensor'''\n",
    "    half = np.int((rank - 2) / 2)\n",
    "    norm = 1 / bond_dim\n",
    "    mps = [\n",
    "        tn.Node( block(dim, bond_dim, norm=norm) )] + \\\n",
    "        [tn.Node( block(bond_dim, dim, bond_dim, norm = norm)) for _ in range(half)] + \\\n",
    "        [tn.Node( block(bond_dim, label_dim, bond_dim, norm=norm) )] + \\\n",
    "        [tn.Node( block(bond_dim, dim, bond_dim, norm=norm)) for _ in range(half, rank-2)] + \\\n",
    "        [tn.Node( block(bond_dim, dim, norm=norm) )\n",
    "        ]\n",
    "\n",
    "    #connect edges to build mps\n",
    "    connected_edges=[]\n",
    "    conn=mps[0][1]^mps[1][0]\n",
    "    connected_edges.append(conn)\n",
    "    for k in range(1,rank):\n",
    "        conn=mps[k][2]^mps[k+1][0]\n",
    "        connected_edges.append(conn)\n",
    "\n",
    "    return mps, connected_edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_map(p):\n",
    "    phi = [1-p, p]\n",
    "    return phi\n",
    "\n",
    "def data_tensorize(vec):\n",
    "    data_tensor = [tn.Node(feature_map(p)) for p in vec]\n",
    "    return data_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected a `jnp.array` or scalar. Got <class 'list'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-c25830400825>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train_1d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m196\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_tensorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlabel_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlabel_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-56d623c2abf2>\u001b[0m in \u001b[0;36mdata_tensorize\u001b[0;34m(vec)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdata_tensorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdata_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-56d623c2abf2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdata_tensorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdata_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensornetwork/network_components.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tensor, name, axis_names, backend)\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m       \u001b[0mbackend_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend_factory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m     super().__init__(\n\u001b[1;32m    555\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensornetwork/backends/jax/jax_backend.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m       raise TypeError(\"Expected a `jnp.array` or scalar. Got {}\".format(\n\u001b[0;32m--> 124\u001b[0;31m           type(tensor)))\n\u001b[0m\u001b[1;32m    125\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected a `jnp.array` or scalar. Got <class 'list'>"
     ]
    }
   ],
   "source": [
    "test_vec = x_train_1d[0, 0:196]\n",
    "data_tensor = data_tensorize(test_vec)\n",
    "\n",
    "label_len = 1\n",
    "label_dim = 10\n",
    "data_len = len(test_vec)\n",
    "rank = data_len\n",
    "dim = 2\n",
    "bond_dim = 5\n",
    "# mps, edges = create_MPS(rank, dim, bond_dim)\n",
    "mps, edges = create_MPS_labeled(rank, dim, bond_dim)\n",
    "\n",
    "\n",
    "edges.append(data_tensor[0][0] ^ mps[0][0])\n",
    "half_len = np.int(len(data_tensor) / 2)\n",
    "[edges.append(data_tensor[i][0] ^ mps[i][1]) for i in range(1, half_len)]\n",
    "[edges.append(data_tensor[i-label_len][0] ^ mps[i][1]) for i in range(half_len + label_len, data_len + label_len)]\n",
    "for k in reversed(range(len(edges))):\n",
    "    A = tn.contract(edges[k])\n",
    "result = A.tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.22205943e-04 -1.43648065e-05 -1.26227900e-05  1.18715210e-04\n",
      "  2.37993200e-04 -1.28477517e-05 -5.72914473e-05 -1.96396738e-04\n",
      "  2.57734358e-06  1.82428327e-04]\n",
      "[-3.2220595e-04 -1.4364807e-05 -1.2622790e-05  1.1871521e-04\n",
      "  2.3799320e-04 -1.2847751e-05 -5.7291447e-05 -1.9639674e-04\n",
      "  2.5773436e-06  1.8242832e-04]\n",
      "[         nan          nan          nan  -9.03878313  -8.34326846\n",
      "          nan          nan          nan -12.86875131  -8.60915319]\n",
      "[0.09996852 0.0999993  0.09999948 0.10001261 0.10002454 0.09999945\n",
      " 0.09999501 0.0999811  0.100001   0.10001898]\n"
     ]
    }
   ],
   "source": [
    "print(A.tensor.numpy())\n",
    "print(A.tensor.numpy().astype(\"float32\"))\n",
    "print(tf.math.log(A.tensor).numpy())\n",
    "print(tf.nn.softmax(A.tensor).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block(*dimensions):\n",
    "    '''Construct a new matrix for the MPS with random numbers from 0 to 1'''\n",
    "    size = tuple([x for x in dimensions])\n",
    "    return tf.Variable(\n",
    "        tf.random.normal(shape=size, dtype=tf.dtypes.float64, mean= 0, stddev = 0.5),\n",
    "        trainable=True)\n",
    "\n",
    "def create_blocks(rank, dim, bond_dim, label_dim):\n",
    "    half = np.int((rank - 2) / 2)\n",
    "    blocks = [\n",
    "        block(dim, bond_dim) ] + \\\n",
    "        [ block(bond_dim, dim, bond_dim) for _ in range(half)] + \\\n",
    "        [ block(bond_dim, label_dim, bond_dim) ] + \\\n",
    "        [ block(bond_dim, dim, bond_dim) for _ in range(half, rank-2)] + \\\n",
    "        [ block(bond_dim, dim) \n",
    "        ]\n",
    "    return blocks\n",
    "\n",
    "def create_MPS_labeled(blocks, rank, dim, bond_dim):\n",
    "    '''Build the MPS tensor'''\n",
    "    half = np.int((rank - 2) / 2)\n",
    "    mps = []\n",
    "    for b in blocks:\n",
    "        mps.append(tn.Node(b))\n",
    "\n",
    "    #connect edges to build mps\n",
    "    connected_edges=[]\n",
    "    conn=mps[0][1]^mps[1][0]\n",
    "    connected_edges.append(conn)\n",
    "    for k in range(1,rank):\n",
    "        conn=mps[k][2]^mps[k+1][0]\n",
    "        connected_edges.append(conn)\n",
    "\n",
    "    return mps, connected_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TNLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_len, label_num, bond_dim):\n",
    "        self.label_len = 1\n",
    "        self.label_dim = label_num\n",
    "        self.rank = input_len\n",
    "        self.dim = 2\n",
    "        self.bond_dim = bond_dim\n",
    "        #super(TNLayer, self).__init__()\n",
    "        super().__init__()\n",
    "        # Create the variables for the layer.\n",
    "        self.blocks = create_blocks(self.rank, self.dim, self.bond_dim, self.label_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        def f(input_vec, blocks, rank, dim, bond_dim, label_len):\n",
    "            mps, edges = create_MPS_labeled(blocks, rank, dim, bond_dim)\n",
    "            data_tensor = []\n",
    "            for p in tf.unstack(input_vec):\n",
    "                data_tensor.append(tn.Node([1-p, p]))\n",
    "            edges.append(data_tensor[0][0] ^ mps[0][0])\n",
    "            half_len = np.int(rank / 2)\n",
    "            [edges.append(data_tensor[i][0] ^ mps[i][1]) for i in range(1, half_len)]\n",
    "            [edges.append(data_tensor[i-label_len][0] ^ mps[i][1]) \\\n",
    "                 for i in range(half_len + label_len, rank + label_len)]\n",
    "            for k in reversed(range(len(edges))):\n",
    "                A = tn.contract(edges[k])\n",
    "            #result = tf.math.log(A.tensor)\n",
    "            result = A.tensor - tf.math.reduce_max(A.tensor)\n",
    "            return result\n",
    "\n",
    "        result = tf.vectorized_map(\n",
    "        lambda vec: f(vec, self.blocks, self.rank, self.dim, self.bond_dim, self.label_len), inputs)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e1152a193765>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mTNLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbond_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     ])\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, layers, name)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    219\u001b[0m       \u001b[0;31m# If the model is being built continuously on top of an input layer:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m       \u001b[0;31m# refresh its output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m       \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSINGLE_LAYER_OUTPUT_ERROR_MSG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    924\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m--> 926\u001b[0;31m                                                 input_list)\n\u001b[0m\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m     \u001b[0;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1115\u001b[0m           \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m               \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOperatorNotAllowedInGraphError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    594\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/tmpnbhf9ncf.py\u001b[0m in \u001b[0;36mtf__call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     55\u001b[0m                             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mfscope_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval__1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_return_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorized_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbond_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_whitelisted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m   \u001b[0;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    338\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py\u001b[0m in \u001b[0;36mvectorized_map\u001b[0;34m(fn, elems, fallback_to_while_loop)\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_elem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m   return pfor(loop_fn, batch_size,\n\u001b[0;32m--> 432\u001b[0;31m               fallback_to_while_loop=fallback_to_while_loop)\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py\u001b[0m in \u001b[0;36mpfor\u001b[0;34m(loop_fn, iters, fallback_to_while_loop, parallel_iterations)\u001b[0m\n\u001b[1;32m    201\u001b[0m       \u001b[0mdef_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_functions_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdef_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mfunctions_run_eagerly\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0mdef_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_functions_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunctions_run_eagerly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m()\u001b[0m\n\u001b[1;32m    186\u001b[0m                       \u001b[0miters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                       \u001b[0mfallback_to_while_loop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfallback_to_while_loop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                       parallel_iterations=parallel_iterations)\n\u001b[0m\u001b[1;32m    189\u001b[0m   \u001b[0;31m# Note that we wrap into a tf.function if in eager execution mode or under\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m   \u001b[0;31m# XLA compilation. The latter is so that we don't compile operations like\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py\u001b[0m in \u001b[0;36m_pfor_impl\u001b[0;34m(loop_fn, iters, fallback_to_while_loop, parallel_iterations, pfor_config)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m       \u001b[0;32massert\u001b[0m \u001b[0mpfor_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m       \u001b[0mloop_fn_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloop_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m   \u001b[0;31m# Convert outputs to Tensor if needed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/ops/parallel_for/control_flow_ops.py\u001b[0m in \u001b[0;36mloop_fn\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m    422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mloop_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0mgathered_elems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgathered_elems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m   \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m   \u001b[0mfirst_elem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/tmpnbhf9ncf.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(vec)\u001b[0m\n\u001b[1;32m     55\u001b[0m                             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mfscope_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval__1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_return_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorized_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbond_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    463\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mis_autograph_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Permanently whitelisted: %s: AutoGraph artifact'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m   \u001b[0;31m# If this is a partial, unwrap it and redo all the checks.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    338\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/tmpnbhf9ncf.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(input_vec, blocks, rank, dim, bond_dim, label_len)\u001b[0m\n\u001b[1;32m     27\u001b[0m                             \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'p'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                         \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'iterate_names'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'p'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                         \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m^\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                         \u001b[0mhalf_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/autograph/operators/control_flow.py\u001b[0m in \u001b[0;36mfor_stmt\u001b[0;34m(iter_, extra_test, body, get_state, set_state, symbol_names, opts)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m     \u001b[0m_py_for_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/autograph/operators/control_flow.py\u001b[0m in \u001b[0;36m_py_for_stmt\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    395\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m       \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/autograph/operators/control_flow.py\u001b[0m in \u001b[0;36mprotected_body\u001b[0;34m(protected_iter)\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0moriginal_body\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprotected_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotected_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m       \u001b[0moriginal_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotected_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m       \u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m       \u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/tmpnbhf9ncf.py\u001b[0m in \u001b[0;36mloop_body\u001b[0;34m(itr)\u001b[0m\n\u001b[1;32m     25\u001b[0m                         \u001b[0;32mdef\u001b[0m \u001b[0mloop_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                             \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'p'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                         \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'iterate_names'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'p'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    455\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_in_whitelist_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Whitelisted %s: from cache'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    338\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensornetwork/network_components.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tensor, name, axis_names, backend)\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m       \u001b[0mbackend_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend_factory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m     super().__init__(\n\u001b[1;32m    555\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensornetwork/backends/tensorflow/tensorflow_backend.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1380\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1499\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_autopacking_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m   1500\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_cast_nested_seqs_to_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1502\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_autopacking_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"packed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_autopacking_helper\u001b[0;34m(list_or_tuple, dtype, name)\u001b[0m\n\u001b[1;32m   1436\u001b[0m           elems_as_tensors.append(\n\u001b[1;32m   1437\u001b[0m               constant_op.constant(elem, dtype=dtype, name=str(i)))\n\u001b[0;32m-> 1438\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melems_as_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1439\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_elems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mpack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   6475\u001b[0m   \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6476\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0;32m-> 6477\u001b[0;31m         \"Pack\", values=values, axis=axis, name=name)\n\u001b[0m\u001b[1;32m   6478\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6479\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    742\u001b[0m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001b[1;32m    743\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m                                  attrs=attr_protos, op_def=op_def)\n\u001b[0m\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;31m# `outputs` is returned as a separate return value so that the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    591\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[1;32m    592\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         compute_device)\n\u001b[0m\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3483\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3484\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3485\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3486\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3487\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1923\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1924\u001b[0m       if not all(\n\u001b[0;32m-> 1925\u001b[0;31m           x.is_compatible_with(i.dtype) for i, x in zip(inputs, input_types)):\n\u001b[0m\u001b[1;32m   1926\u001b[0m         raise TypeError(\"In op '%s', input types (%s) are not compatible \"\n\u001b[1;32m   1927\u001b[0m                         \u001b[0;34m\"with expected types (%s)\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N = x_train_1d.shape[1]\n",
    "label_len = 1\n",
    "label_num = 10\n",
    "data_len = x_train_1d.shape[1]\n",
    "rank = data_len\n",
    "dim = 2\n",
    "bond_dim = 10\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "tn_model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(N,)),\n",
    "        TNLayer(N, label_num, bond_dim),\n",
    "        tf.keras.layers.Softmax()\n",
    "    ])\n",
    "\n",
    "tn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "938/938 [==============================] - 164s 175ms/step - loss: 2.0227 - accuracy: 0.1125\n",
      "Epoch 2/100\n",
      "938/938 [==============================] - 163s 174ms/step - loss: 1.9593 - accuracy: 0.1124\n",
      "Epoch 3/100\n",
      "938/938 [==============================] - 164s 175ms/step - loss: 1.9492 - accuracy: 0.1124\n",
      "Epoch 4/100\n",
      "938/938 [==============================] - 163s 174ms/step - loss: 1.9450 - accuracy: 0.1124\n",
      "Epoch 5/100\n",
      "938/938 [==============================] - 164s 174ms/step - loss: 1.9388 - accuracy: 0.1124\n",
      "Epoch 6/100\n",
      "938/938 [==============================] - 163s 174ms/step - loss: 1.9346 - accuracy: 0.1135\n",
      "Epoch 7/100\n",
      "938/938 [==============================] - 163s 174ms/step - loss: 1.9355 - accuracy: 0.1306\n",
      "Epoch 8/100\n",
      "938/938 [==============================] - 164s 174ms/step - loss: 1.9316 - accuracy: 0.1460\n",
      "Epoch 9/100\n",
      "938/938 [==============================] - 164s 175ms/step - loss: 1.9272 - accuracy: 0.1572\n",
      "Epoch 10/100\n",
      "938/938 [==============================] - 163s 174ms/step - loss: 1.9243 - accuracy: 0.1646\n",
      "Epoch 11/100\n",
      "938/938 [==============================] - 163s 174ms/step - loss: 1.9262 - accuracy: 0.1634\n",
      "Epoch 12/100\n",
      "938/938 [==============================] - 164s 174ms/step - loss: 1.9238 - accuracy: 0.1638\n",
      "Epoch 13/100\n",
      "938/938 [==============================] - 164s 174ms/step - loss: 1.9183 - accuracy: 0.1626\n",
      "Epoch 14/100\n",
      "938/938 [==============================] - 164s 174ms/step - loss: 1.9159 - accuracy: 0.1613\n",
      "Epoch 15/100\n",
      "938/938 [==============================] - 164s 175ms/step - loss: 1.9177 - accuracy: 0.1615\n",
      "Epoch 16/100\n",
      "938/938 [==============================] - 161s 172ms/step - loss: 1.9147 - accuracy: 0.1617\n",
      "Epoch 17/100\n",
      "938/938 [==============================] - 130s 138ms/step - loss: 1.9109 - accuracy: 0.1607\n",
      "Epoch 18/100\n",
      "938/938 [==============================] - 129s 138ms/step - loss: 1.9127 - accuracy: 0.1608\n",
      "Epoch 19/100\n",
      "938/938 [==============================] - 130s 138ms/step - loss: 1.9108 - accuracy: 0.1602\n",
      "Epoch 20/100\n",
      "938/938 [==============================] - 130s 139ms/step - loss: 1.9109 - accuracy: 0.1598\n",
      "Epoch 21/100\n",
      "938/938 [==============================] - 132s 141ms/step - loss: 1.9041 - accuracy: 0.1596\n",
      "Epoch 22/100\n",
      "938/938 [==============================] - 131s 140ms/step - loss: 1.9049 - accuracy: 0.1598\n",
      "Epoch 23/100\n",
      "938/938 [==============================] - 155s 166ms/step - loss: 1.9074 - accuracy: 0.1580\n",
      "Epoch 24/100\n",
      "938/938 [==============================] - 132s 140ms/step - loss: 1.9041 - accuracy: 0.1503\n",
      "Epoch 25/100\n",
      "938/938 [==============================] - 144s 153ms/step - loss: 1.9027 - accuracy: 0.1555\n",
      "Epoch 26/100\n",
      "938/938 [==============================] - 132s 141ms/step - loss: 1.9047 - accuracy: 0.1552\n",
      "Epoch 27/100\n",
      "  3/938 [..............................] - ETA: 1:26 - loss: 1.9292 - accuracy: 0.1562"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-05f06577ff0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                  \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseCategoricalCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                  metrics=['accuracy'])\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_1d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.pyenv/versions/TFq_3.7/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "tn_model.compile(optimizer=optimizer, \n",
    "                 loss=tf.keras.losses.SparseCategoricalCrossentropy(), \n",
    "                 metrics=['accuracy'])\n",
    "tn_model.fit(x_train_1d, y_train, batch_size=64, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "average_pooling2d_1 (Average (None, 14, 14, 1)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 196)               0         \n",
      "_________________________________________________________________\n",
      "tn_layer_1 (TNLayer)         (None, 10)                9970      \n",
      "_________________________________________________________________\n",
      "softmax_1 (Softmax)          (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 9,970\n",
      "Trainable params: 9,970\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "label_len = 1\n",
    "label_num = 10\n",
    "dim = 2\n",
    "bond_dim = 5\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "tn_model2 = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(28, 28, 1)),\n",
    "        tf.keras.layers.AveragePooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        TNLayer(196, label_num, bond_dim),\n",
    "        tf.keras.layers.Softmax()\n",
    "    ],name=\"sequential_2\")\n",
    "\n",
    "tn_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 1/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 2.2952 - accuracy: 0.1229\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 2/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 2.2674 - accuracy: 0.1406\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 3/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 2.2454 - accuracy: 0.1489\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 4/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 2.2318 - accuracy: 0.1525\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 5/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 2.2219 - accuracy: 0.1518\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 6/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 2.2133 - accuracy: 0.1525\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 7/300\n",
      "1875/1875 [==============================] - 52s 27ms/step - loss: 2.2056 - accuracy: 0.1543\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 8/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.1972 - accuracy: 0.1567\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 9/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.1879 - accuracy: 0.1576\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 10/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.1789 - accuracy: 0.1594\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 11/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 2.1685 - accuracy: 0.1623\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 12/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.1579 - accuracy: 0.1640\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 13/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 2.1480 - accuracy: 0.1655\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 14/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 2.1382 - accuracy: 0.1674\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 15/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 2.1270 - accuracy: 0.1679\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 16/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 2.1173 - accuracy: 0.1692\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 17/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 2.1088 - accuracy: 0.1701\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 18/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.1026 - accuracy: 0.1704\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 19/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 2.0967 - accuracy: 0.1711\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 20/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0919 - accuracy: 0.1721\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 21/300\n",
      "1875/1875 [==============================] - 50s 26ms/step - loss: 2.0880 - accuracy: 0.1721\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 22/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0847 - accuracy: 0.1727\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 23/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0821 - accuracy: 0.1728\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 24/300\n",
      "1875/1875 [==============================] - 50s 26ms/step - loss: 2.0795 - accuracy: 0.1727\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 25/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0775 - accuracy: 0.1728\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 26/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0755 - accuracy: 0.1724\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 27/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0738 - accuracy: 0.1720\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 28/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0725 - accuracy: 0.1722\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 29/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 2.0712 - accuracy: 0.1723\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 30/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 2.0701 - accuracy: 0.1723\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 31/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0690 - accuracy: 0.1722\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 32/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0681 - accuracy: 0.1732\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 33/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 2.0674 - accuracy: 0.1728\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 34/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 2.0664 - accuracy: 0.1729\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 35/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 2.0657 - accuracy: 0.1726\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 36/300\n",
      "1875/1875 [==============================] - 50s 26ms/step - loss: 2.0650 - accuracy: 0.1731\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 37/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 2.0642 - accuracy: 0.1736\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 38/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 2.0639 - accuracy: 0.1738\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 39/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 2.0633 - accuracy: 0.1745\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 40/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 2.0628 - accuracy: 0.1746\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 41/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0624 - accuracy: 0.1746\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 42/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0619 - accuracy: 0.1759\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 43/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 2.0615 - accuracy: 0.1762\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 44/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0611 - accuracy: 0.1765\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 45/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0610 - accuracy: 0.1767\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 46/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 2.0603 - accuracy: 0.1778\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 47/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0600 - accuracy: 0.1797\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 48/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 2.0598 - accuracy: 0.1791\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 49/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0598 - accuracy: 0.1805\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 50/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 2.0591 - accuracy: 0.1800\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 51/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0590 - accuracy: 0.1807\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 52/300\n",
      "1875/1875 [==============================] - 50s 26ms/step - loss: 2.0586 - accuracy: 0.1813\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 53/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 2.0584 - accuracy: 0.1815\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 54/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0582 - accuracy: 0.1821\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 55/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 2.0580 - accuracy: 0.1826\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 56/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0577 - accuracy: 0.1825\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 57/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0576 - accuracy: 0.1832\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 58/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0573 - accuracy: 0.1837\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 59/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 2.0571 - accuracy: 0.1847\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 60/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 2.0570 - accuracy: 0.1852\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 61/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0568 - accuracy: 0.1857\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 62/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0566 - accuracy: 0.1864\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 63/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0565 - accuracy: 0.1870\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 64/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0563 - accuracy: 0.1875\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 65/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0562 - accuracy: 0.1880\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 66/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 2.0559 - accuracy: 0.1885\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 67/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0557 - accuracy: 0.1886\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 68/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 2.0556 - accuracy: 0.1895\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 69/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0559 - accuracy: 0.1904\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 70/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0552 - accuracy: 0.1909\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 71/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0552 - accuracy: 0.1916\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 72/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0550 - accuracy: 0.1917\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 73/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0550 - accuracy: 0.1934\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 74/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 2.0549 - accuracy: 0.1930\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 75/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0546 - accuracy: 0.1939\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 76/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0545 - accuracy: 0.1953\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 77/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0544 - accuracy: 0.1950\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 78/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0543 - accuracy: 0.1953\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 79/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0542 - accuracy: 0.1965\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 80/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 2.0541 - accuracy: 0.1963\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 81/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0539 - accuracy: 0.1977\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 82/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0538 - accuracy: 0.1983\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 83/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0537 - accuracy: 0.1992\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 84/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0536 - accuracy: 0.1999\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 85/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0534 - accuracy: 0.2001\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 86/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0533 - accuracy: 0.2012\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 87/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 2.0532 - accuracy: 0.2023\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 88/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 2.0530 - accuracy: 0.2030\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 89/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0530 - accuracy: 0.2036\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 90/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 2.0528 - accuracy: 0.2044\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 91/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 2.0526 - accuracy: 0.2043\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 92/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0526 - accuracy: 0.2049\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 93/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0525 - accuracy: 0.2063\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 94/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 48s 26ms/step - loss: 2.0523 - accuracy: 0.2068\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 95/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0522 - accuracy: 0.2079\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 96/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0521 - accuracy: 0.2077\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 97/300\n",
      "1875/1875 [==============================] - 50s 26ms/step - loss: 2.0520 - accuracy: 0.2084\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 98/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0519 - accuracy: 0.2092\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 99/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 2.0518 - accuracy: 0.2097\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 100/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 2.0516 - accuracy: 0.2103\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 101/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 2.0573 - accuracy: 0.2161\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 102/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 2.0510 - accuracy: 0.2229\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 103/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 2.0041 - accuracy: 0.2481\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 104/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.9163 - accuracy: 0.2777\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 105/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.8234 - accuracy: 0.2895\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 106/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.7369 - accuracy: 0.3039\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 107/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 1.6698 - accuracy: 0.3226\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 108/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 1.6306 - accuracy: 0.3440\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 109/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.5974 - accuracy: 0.3671\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 110/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 1.5634 - accuracy: 0.3857\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 111/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.4996 - accuracy: 0.4392\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 112/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.3812 - accuracy: 0.5169\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 113/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.2613 - accuracy: 0.5901\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 114/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 1.1220 - accuracy: 0.6520\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 115/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 1.0447 - accuracy: 0.6713\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 116/300\n",
      "1875/1875 [==============================] - 50s 26ms/step - loss: 0.9957 - accuracy: 0.6861\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 117/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.9631 - accuracy: 0.6977\n",
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 118/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.9351 - accuracy: 0.7091\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 119/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.9129 - accuracy: 0.7160\n",
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 120/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.8966 - accuracy: 0.7222\n",
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 121/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.8803 - accuracy: 0.7296\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 122/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.8656 - accuracy: 0.7356\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 123/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.8520 - accuracy: 0.7414\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 124/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.8409 - accuracy: 0.7484\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 125/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 0.8268 - accuracy: 0.7532\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 126/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.8137 - accuracy: 0.7587\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 127/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.8035 - accuracy: 0.7638\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 128/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.7907 - accuracy: 0.7684\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 129/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.7799 - accuracy: 0.7766\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 130/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.7685 - accuracy: 0.7782\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 131/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.7564 - accuracy: 0.7830\n",
      "\n",
      "Epoch 00132: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 132/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.7449 - accuracy: 0.7879\n",
      "\n",
      "Epoch 00133: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 133/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.7312 - accuracy: 0.7921\n",
      "\n",
      "Epoch 00134: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 134/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.7187 - accuracy: 0.7965\n",
      "\n",
      "Epoch 00135: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 135/300\n",
      "1875/1875 [==============================] - 50s 26ms/step - loss: 0.7030 - accuracy: 0.8001\n",
      "\n",
      "Epoch 00136: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 136/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.6898 - accuracy: 0.8037\n",
      "\n",
      "Epoch 00137: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 137/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.6789 - accuracy: 0.8065\n",
      "\n",
      "Epoch 00138: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 138/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.6711 - accuracy: 0.8085\n",
      "\n",
      "Epoch 00139: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 139/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.6614 - accuracy: 0.8111\n",
      "\n",
      "Epoch 00140: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 140/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.6542 - accuracy: 0.8125\n",
      "\n",
      "Epoch 00141: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 141/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.6462 - accuracy: 0.8143\n",
      "\n",
      "Epoch 00142: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 142/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.6391 - accuracy: 0.8161\n",
      "\n",
      "Epoch 00143: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 143/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.6328 - accuracy: 0.8179\n",
      "\n",
      "Epoch 00144: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 144/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.6247 - accuracy: 0.8197\n",
      "\n",
      "Epoch 00145: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 145/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.6179 - accuracy: 0.8229\n",
      "\n",
      "Epoch 00146: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 146/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.6094 - accuracy: 0.8246\n",
      "\n",
      "Epoch 00147: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 147/300\n",
      "1875/1875 [==============================] - 50s 26ms/step - loss: 0.6027 - accuracy: 0.8264\n",
      "\n",
      "Epoch 00148: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 148/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.5944 - accuracy: 0.8295\n",
      "\n",
      "Epoch 00149: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 149/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.5841 - accuracy: 0.8316\n",
      "\n",
      "Epoch 00150: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 150/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.5782 - accuracy: 0.8328\n",
      "\n",
      "Epoch 00151: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 151/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.5723 - accuracy: 0.8354\n",
      "\n",
      "Epoch 00152: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 152/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.5645 - accuracy: 0.8378\n",
      "\n",
      "Epoch 00153: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 153/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.5556 - accuracy: 0.8405\n",
      "\n",
      "Epoch 00154: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 154/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.5497 - accuracy: 0.8426\n",
      "\n",
      "Epoch 00155: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 155/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.5408 - accuracy: 0.8443\n",
      "\n",
      "Epoch 00156: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 156/300\n",
      "1875/1875 [==============================] - 50s 26ms/step - loss: 0.5311 - accuracy: 0.8490\n",
      "\n",
      "Epoch 00157: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 157/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.5222 - accuracy: 0.8515\n",
      "\n",
      "Epoch 00158: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 158/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.5117 - accuracy: 0.8554\n",
      "\n",
      "Epoch 00159: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 159/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.5052 - accuracy: 0.8573\n",
      "\n",
      "Epoch 00160: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 160/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 0.4973 - accuracy: 0.8609\n",
      "\n",
      "Epoch 00161: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 161/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.4910 - accuracy: 0.8638\n",
      "\n",
      "Epoch 00162: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 162/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.4810 - accuracy: 0.8671\n",
      "\n",
      "Epoch 00163: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 163/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.4745 - accuracy: 0.8692\n",
      "\n",
      "Epoch 00164: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 164/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.4644 - accuracy: 0.8735\n",
      "\n",
      "Epoch 00165: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 165/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.4579 - accuracy: 0.8766\n",
      "\n",
      "Epoch 00166: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 166/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.4506 - accuracy: 0.8784\n",
      "\n",
      "Epoch 00167: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 167/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.4446 - accuracy: 0.8801\n",
      "\n",
      "Epoch 00168: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 168/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.4371 - accuracy: 0.8826\n",
      "\n",
      "Epoch 00169: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 169/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.4308 - accuracy: 0.8851\n",
      "\n",
      "Epoch 00170: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 170/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.4239 - accuracy: 0.8866\n",
      "\n",
      "Epoch 00171: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 171/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.4167 - accuracy: 0.8885\n",
      "\n",
      "Epoch 00172: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 172/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.4109 - accuracy: 0.8901\n",
      "\n",
      "Epoch 00173: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 173/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.4054 - accuracy: 0.8925\n",
      "\n",
      "Epoch 00174: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 174/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.4002 - accuracy: 0.8939\n",
      "\n",
      "Epoch 00175: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 175/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.3958 - accuracy: 0.8953\n",
      "\n",
      "Epoch 00176: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 176/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.3916 - accuracy: 0.8976\n",
      "\n",
      "Epoch 00177: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 177/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.3869 - accuracy: 0.8978\n",
      "\n",
      "Epoch 00178: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 178/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.3813 - accuracy: 0.8998\n",
      "\n",
      "Epoch 00179: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 179/300\n",
      "1875/1875 [==============================] - 50s 26ms/step - loss: 0.3784 - accuracy: 0.8999\n",
      "\n",
      "Epoch 00180: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 180/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.3741 - accuracy: 0.9008\n",
      "\n",
      "Epoch 00181: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 181/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 0.3705 - accuracy: 0.9027\n",
      "\n",
      "Epoch 00182: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 182/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.3669 - accuracy: 0.9037\n",
      "\n",
      "Epoch 00183: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 183/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.3643 - accuracy: 0.9034\n",
      "\n",
      "Epoch 00184: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 184/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.3627 - accuracy: 0.9055\n",
      "\n",
      "Epoch 00185: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 185/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 0.3585 - accuracy: 0.9053\n",
      "\n",
      "Epoch 00186: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 186/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.3559 - accuracy: 0.9066\n",
      "\n",
      "Epoch 00187: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 187/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 0.3535 - accuracy: 0.9075\n",
      "\n",
      "Epoch 00188: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 188/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 0.3516 - accuracy: 0.9083\n",
      "\n",
      "Epoch 00189: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 189/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.3489 - accuracy: 0.9080\n",
      "\n",
      "Epoch 00190: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 190/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.3479 - accuracy: 0.9092\n",
      "\n",
      "Epoch 00191: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 191/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.3440 - accuracy: 0.9104\n",
      "\n",
      "Epoch 00192: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 192/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.3415 - accuracy: 0.9103\n",
      "\n",
      "Epoch 00193: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 193/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.3392 - accuracy: 0.9114\n",
      "\n",
      "Epoch 00194: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 194/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.3364 - accuracy: 0.9124\n",
      "\n",
      "Epoch 00195: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 195/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.3363 - accuracy: 0.9119\n",
      "\n",
      "Epoch 00196: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 196/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.3326 - accuracy: 0.9131\n",
      "\n",
      "Epoch 00197: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 197/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.3308 - accuracy: 0.9135\n",
      "\n",
      "Epoch 00198: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 198/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.3284 - accuracy: 0.9137\n",
      "\n",
      "Epoch 00199: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 199/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.3274 - accuracy: 0.9145\n",
      "\n",
      "Epoch 00200: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 200/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.3240 - accuracy: 0.9144\n",
      "\n",
      "Epoch 00201: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 201/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.3212 - accuracy: 0.9155\n",
      "\n",
      "Epoch 00202: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 202/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.3026 - accuracy: 0.9192\n",
      "\n",
      "Epoch 00203: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 203/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.2997 - accuracy: 0.9198\n",
      "\n",
      "Epoch 00204: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 204/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.2989 - accuracy: 0.9198\n",
      "\n",
      "Epoch 00205: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 205/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2986 - accuracy: 0.9198\n",
      "\n",
      "Epoch 00206: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 206/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.2980 - accuracy: 0.9201\n",
      "\n",
      "Epoch 00207: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 207/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.2977 - accuracy: 0.9201\n",
      "\n",
      "Epoch 00208: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 208/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.2972 - accuracy: 0.9206\n",
      "\n",
      "Epoch 00209: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 209/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.2971 - accuracy: 0.9206\n",
      "\n",
      "Epoch 00210: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 210/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.2966 - accuracy: 0.9205\n",
      "\n",
      "Epoch 00211: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 211/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.2961 - accuracy: 0.9208\n",
      "\n",
      "Epoch 00212: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 212/300\n",
      "1875/1875 [==============================] - 50s 26ms/step - loss: 0.2960 - accuracy: 0.9204\n",
      "\n",
      "Epoch 00213: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 213/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.2957 - accuracy: 0.9210\n",
      "\n",
      "Epoch 00214: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 214/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.2953 - accuracy: 0.9209\n",
      "\n",
      "Epoch 00215: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 215/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2947 - accuracy: 0.9211\n",
      "\n",
      "Epoch 00216: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 216/300\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 0.2946 - accuracy: 0.9209\n",
      "\n",
      "Epoch 00217: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 217/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2941 - accuracy: 0.9209\n",
      "\n",
      "Epoch 00218: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 218/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.2937 - accuracy: 0.9210\n",
      "\n",
      "Epoch 00219: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 219/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.2936 - accuracy: 0.9212\n",
      "\n",
      "Epoch 00220: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 220/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.2933 - accuracy: 0.9211\n",
      "\n",
      "Epoch 00221: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 221/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.2928 - accuracy: 0.9214\n",
      "\n",
      "Epoch 00222: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 222/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2923 - accuracy: 0.9210\n",
      "\n",
      "Epoch 00223: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 223/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.2918 - accuracy: 0.9208\n",
      "\n",
      "Epoch 00224: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 224/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.2919 - accuracy: 0.9211\n",
      "\n",
      "Epoch 00225: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 225/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.2916 - accuracy: 0.9214\n",
      "\n",
      "Epoch 00226: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 226/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.2907 - accuracy: 0.9217\n",
      "\n",
      "Epoch 00227: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 227/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.2905 - accuracy: 0.9222\n",
      "\n",
      "Epoch 00228: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 228/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2903 - accuracy: 0.9220\n",
      "\n",
      "Epoch 00229: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 229/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.2900 - accuracy: 0.9217\n",
      "\n",
      "Epoch 00230: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 230/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 0.2895 - accuracy: 0.9217\n",
      "\n",
      "Epoch 00231: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 231/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.2894 - accuracy: 0.9216\n",
      "\n",
      "Epoch 00232: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 232/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.2888 - accuracy: 0.9220\n",
      "\n",
      "Epoch 00233: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 233/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.2887 - accuracy: 0.9221\n",
      "\n",
      "Epoch 00234: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 234/300\n",
      "1875/1875 [==============================] - 50s 26ms/step - loss: 0.2883 - accuracy: 0.9217\n",
      "\n",
      "Epoch 00235: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 235/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.2879 - accuracy: 0.9223\n",
      "\n",
      "Epoch 00236: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 236/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.2876 - accuracy: 0.9217\n",
      "\n",
      "Epoch 00237: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 237/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.2869 - accuracy: 0.9224\n",
      "\n",
      "Epoch 00238: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 238/300\n",
      "1875/1875 [==============================] - 52s 28ms/step - loss: 0.2869 - accuracy: 0.9224\n",
      "\n",
      "Epoch 00239: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 239/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.2866 - accuracy: 0.9217\n",
      "\n",
      "Epoch 00240: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 240/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.2862 - accuracy: 0.9221\n",
      "\n",
      "Epoch 00241: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 241/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.2860 - accuracy: 0.9223\n",
      "\n",
      "Epoch 00242: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 242/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.2855 - accuracy: 0.9228\n",
      "\n",
      "Epoch 00243: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 243/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.2853 - accuracy: 0.9225\n",
      "\n",
      "Epoch 00244: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 244/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.2846 - accuracy: 0.9230\n",
      "\n",
      "Epoch 00245: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 245/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.2844 - accuracy: 0.9227\n",
      "\n",
      "Epoch 00246: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 246/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.2841 - accuracy: 0.9230\n",
      "\n",
      "Epoch 00247: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 247/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.2838 - accuracy: 0.9227\n",
      "\n",
      "Epoch 00248: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 248/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.2833 - accuracy: 0.9230\n",
      "\n",
      "Epoch 00249: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 249/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.2830 - accuracy: 0.9226\n",
      "\n",
      "Epoch 00250: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 250/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.2827 - accuracy: 0.9227\n",
      "\n",
      "Epoch 00251: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 251/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.2822 - accuracy: 0.9224\n",
      "\n",
      "Epoch 00252: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 252/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.2822 - accuracy: 0.9232\n",
      "\n",
      "Epoch 00253: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 253/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.2817 - accuracy: 0.9232\n",
      "\n",
      "Epoch 00254: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 254/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.2813 - accuracy: 0.9236\n",
      "\n",
      "Epoch 00255: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 255/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.2808 - accuracy: 0.9235\n",
      "\n",
      "Epoch 00256: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 256/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.2808 - accuracy: 0.9237\n",
      "\n",
      "Epoch 00257: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 257/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.2803 - accuracy: 0.9235\n",
      "\n",
      "Epoch 00258: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 258/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.2798 - accuracy: 0.9233\n",
      "\n",
      "Epoch 00259: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 259/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.2795 - accuracy: 0.9235\n",
      "\n",
      "Epoch 00260: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 260/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.2793 - accuracy: 0.9237\n",
      "\n",
      "Epoch 00261: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 261/300\n",
      "1875/1875 [==============================] - 50s 26ms/step - loss: 0.2790 - accuracy: 0.9233\n",
      "\n",
      "Epoch 00262: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 262/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.2786 - accuracy: 0.9241\n",
      "\n",
      "Epoch 00263: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 263/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.2783 - accuracy: 0.9240\n",
      "\n",
      "Epoch 00264: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 264/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.2779 - accuracy: 0.9242\n",
      "\n",
      "Epoch 00265: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 265/300\n",
      "1875/1875 [==============================] - 48s 25ms/step - loss: 0.2776 - accuracy: 0.9242\n",
      "\n",
      "Epoch 00266: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 266/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.2775 - accuracy: 0.9239\n",
      "\n",
      "Epoch 00267: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 267/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.2770 - accuracy: 0.9243\n",
      "\n",
      "Epoch 00268: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 268/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.2768 - accuracy: 0.9244\n",
      "\n",
      "Epoch 00269: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 269/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.2762 - accuracy: 0.9240\n",
      "\n",
      "Epoch 00270: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 270/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.2762 - accuracy: 0.9247\n",
      "\n",
      "Epoch 00271: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 271/300\n",
      "1875/1875 [==============================] - 50s 26ms/step - loss: 0.2758 - accuracy: 0.9241\n",
      "\n",
      "Epoch 00272: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 272/300\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2754 - accuracy: 0.9245\n",
      "\n",
      "Epoch 00273: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 273/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.2749 - accuracy: 0.9244\n",
      "\n",
      "Epoch 00274: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 274/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.2749 - accuracy: 0.9249\n",
      "\n",
      "Epoch 00275: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 275/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.2742 - accuracy: 0.9250\n",
      "\n",
      "Epoch 00276: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 276/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.2742 - accuracy: 0.9244\n",
      "\n",
      "Epoch 00277: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 277/300\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.2739 - accuracy: 0.9244\n",
      "\n",
      "Epoch 00278: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 278/300\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.2736 - accuracy: 0.9246\n",
      "\n",
      "Epoch 00279: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 279/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.2730 - accuracy: 0.9249\n",
      "\n",
      "Epoch 00280: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 280/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.2731 - accuracy: 0.9249\n",
      "\n",
      "Epoch 00281: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 281/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.2724 - accuracy: 0.9253\n",
      "\n",
      "Epoch 00282: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 282/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.2722 - accuracy: 0.9249\n",
      "\n",
      "Epoch 00283: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 283/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.2721 - accuracy: 0.9251\n",
      "\n",
      "Epoch 00284: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 284/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.2716 - accuracy: 0.9250\n",
      "\n",
      "Epoch 00285: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 285/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.2715 - accuracy: 0.9251\n",
      "\n",
      "Epoch 00286: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 286/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.2712 - accuracy: 0.9254\n",
      "\n",
      "Epoch 00287: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 287/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.2706 - accuracy: 0.9255\n",
      "\n",
      "Epoch 00288: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 288/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.2704 - accuracy: 0.9253\n",
      "\n",
      "Epoch 00289: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 289/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.2700 - accuracy: 0.9255\n",
      "\n",
      "Epoch 00290: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 290/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.2698 - accuracy: 0.9251\n",
      "\n",
      "Epoch 00291: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 291/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.2695 - accuracy: 0.9256\n",
      "\n",
      "Epoch 00292: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 292/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.2691 - accuracy: 0.9257\n",
      "\n",
      "Epoch 00293: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 293/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.2687 - accuracy: 0.9259\n",
      "\n",
      "Epoch 00294: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 294/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.2687 - accuracy: 0.9260\n",
      "\n",
      "Epoch 00295: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 295/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.2680 - accuracy: 0.9255\n",
      "\n",
      "Epoch 00296: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 296/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.2679 - accuracy: 0.9258\n",
      "\n",
      "Epoch 00297: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 297/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.2675 - accuracy: 0.9260\n",
      "\n",
      "Epoch 00298: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 298/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.2673 - accuracy: 0.9257\n",
      "\n",
      "Epoch 00299: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 299/300\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.2670 - accuracy: 0.9257\n",
      "\n",
      "Epoch 00300: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 300/300\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.2666 - accuracy: 0.9259\n"
     ]
    }
   ],
   "source": [
    "x_train_2d = x_train[:, :, :, tf.newaxis]\n",
    "\n",
    "def step_decay(epoch):\n",
    "    x = 2e-5\n",
    "    if epoch >= 100 and epoch <= 200:\n",
    "        x = 2e-4\n",
    "    return x\n",
    "\n",
    "decay = LearningRateScheduler(step_decay, verbose=1)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "tn_model2.compile(optimizer=optimizer, \n",
    "                 loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                 metrics = [\"accuracy\"])\n",
    "history = tn_model2.fit(x_train_2d, y_train, batch_size=32, epochs=300, verbose=1, callbacks=[decay])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.1)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    " \n",
    "# ax1とax2を関連させる\n",
    "ax2 = ax1.twinx()\n",
    " \n",
    "# それぞれのaxesオブジェクトのlines属性にLine2Dオブジェクトを追加\n",
    "ax1.plot(history.epoch, history.history[\"loss\"],\n",
    "         color='r', label=\"Loss\")\n",
    "ax2.plot(history.epoch, history.history[\"accuracy\"], color='b',\n",
    "        label=\"Accuracy\")\n",
    " \n",
    "# 凡例\n",
    "# グラフの本体設定時に、ラベルを手動で設定する必要があるのは、barplotのみ。plotは自動で設定される＞\n",
    "#handler1, label1 = ax1.get_legend_handles_labels()\n",
    "#handler2, label2 = ax2.get_legend_handles_labels()\n",
    "# 凡例をまとめて出力する\n",
    "ax1.legend(handler1 + handler2, label1 + label2, loc=3, borderaxespad=0.)\n",
    "ax1.set_xlabel('Epoch', fontsize=14)\n",
    "ax1.set_ylabel('Loss', fontsize=14)\n",
    "ax2.set_ylabel('Accuracy', fontsize=14)\n",
    " \n",
    "ax1.set_ylim([0, 2.5])\n",
    "ax2.set_ylim([0, 1.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2d9a5f5350>,\n",
       " <matplotlib.lines.Line2D at 0x7f2d9a5f5210>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.epoch, history.history[\"accuracy\"], history.history[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
